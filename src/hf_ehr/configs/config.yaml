main:
  # Training seed.
  seed: 1
  # Where outputs will be saved; If folder exists, then run will be resumed from this directory's `../ckpts/last.ckpt` file
  path_to_output_dir: /share/pi/nigam/mwornow/hf_ehr/cache/runs/${now:%Y-%m-%d_%H-%M-%S}/

hydra:
  run:
    dir: "${main.path_to_output_dir}"

callbacks:
  early_stopping:
    # If we want to min/max the monitored quantity.
    metric_mode: min
    # Number of epochs with no improvement after which training will be stopped.
    patience: 3
  model_checkpointing:
    # Save the top K best models according to val/loss. If -1, then save all models
    save_top_k: 5
    # Save the most recent K models. If -1, then save all models
    save_most_recent_k: 1
    # Save model every N global steps; if NULL, save after every epoch; overwrites previous model
    most_recent_every_n_train_steps: 1_000
    # Save model every N global steps; persists
    every_n_train_steps: 50_000

data:
  dataset:
    # Path to FEMR extract
    path_to_femr_extract: /share/pi/nigam/data/som-rit-phi-starr-prod.starr_omop_cdm5_deid_2023_08_13_extract_v9_lite
  dataloader:
    # Batch size to be used.
    batch_size: 4
    # Number of data loader workers to use.
    n_workers: 10
    # Max number of codes to feed into model at once per patient.
    max_length: 1024
    # If TRUE, then truncate patient timelines at random locations, rather than always doing right-hand side truncation.
    is_truncation_random: True
  tokenizer:
    # Path to tokenizer
    path_to_tokenizer: /share/pi/nigam/mwornow/hf_ehr/cache/tokenizer_v9_lite/code_2_int.json


trainer:
  # Accumulated gradients runs K small batches of size `data.dataloader.batch_size` before doing a backwards pass.
  accumulate_grad_batches: 4
  # Value for gradient clipping
  gradient_clip_value: 3
  # Whether to clip the gradients based on their 'norm' or 'value'
  gradient_clip_algorithm: norm
  # List of devices to run on
  devices: 0, 1, 2, 3
  # Supports: dp, ddp, ddp2, fsdp, deepspeed
  distributed_backend: ddp
  # If true uses bf16 mixed precision (A100 only)
  is_use_bf16: False
  # If true uses fp16 mixed precision
  is_use_fp16: True
  # Limits training to a minimum number of epochs
  min_epochs: 1
  # Limits training to a max number number of epochs
  max_epochs: 1000
  # Limits training to `N` batches if `int`, or `N%` of batches if `float`
  limit_train_batches: null
  # Limits val to `N` batches if `int`, or `N%` of batches if `float`
  limit_val_batches: null
  # How often we check the validation set within an epoch
  val_check_interval: 0.1
  # OPTIMIZER
  optimizer:
    # Learning rate
    lr: 3e-4
  # SCHEDULER
  scheduler:
    # Number of local steps (i.e. non-global) from `initial_lr` -> `trainer.optimizer.lr`
    num_warmup_steps: 10_000
    # Number of local steps (i.e. non-global) from `trainer.optimizer.lr` -> `final_lr`
    num_decay_steps: 2_000_000
    initial_lr: 3e-6
    final_lr: 3e-5

logging:
  wandb:
    # If true, then turn ON wandb logging
    is_wandb: True
    # Run name
    name: null
  # If TRUE, then calculate + log grad norm over all params (slows down training)
  is_log_grad_norm: False
  # Log every N steps
  log_every_n_steps: 1