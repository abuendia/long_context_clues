{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sklearn.utils\n",
    "from typing import List, Optional\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import collections\n",
    "import warnings\n",
    "import pickle\n",
    "import femr.datasets\n",
    "import datetime\n",
    "from hf_ehr.notebooks.ehr_specific_properties import utils\n",
    "from hf_ehr.utils import load_tokenizer_from_path, load_model_from_path\n",
    "from starr_eda import calc_n_gram_count, calc_inter_event_times\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils.validation\")\n",
    "\n",
    "PATH_TO_DATABASE: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/femr/extract'\n",
    "PATH_TO_FEATURES_DIR: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/features_ehrshot'\n",
    "PATH_TO_RESULTS_DIR: str = '/share/pi/nigam/migufuen/ehrshot-benchmark/EHRSHOT_ASSETS/results_ehrshot'\n",
    "PATH_TO_TOKENIZED_TIMELINES_DIR: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/tokenized_timelines_ehrshot'\n",
    "PATH_TO_LABELS_DIR: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/benchmark_ehrshot'\n",
    "PATH_TO_SPLIT_CSV: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/splits_ehrshot/person_id_map.csv'\n",
    "femr_db = femr.datasets.PatientDatabase(PATH_TO_DATABASE)\n",
    "os.makedirs('../cache', exist_ok=True)\n",
    "\n",
    "# Output directory\n",
    "path_to_output_dir: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/ehrshot/stratify/'\n",
    "path_to_output_file: str = os.path.join(path_to_output_dir, f'metrics__{args.model}__per_patient__all_tasks.csv')\n",
    "os.makedirs(path_to_output_dir, exist_ok=True)\n",
    "\n",
    "IS_LOAD_FROM_CACHE: bool = False\n",
    "\n",
    "# Get list of tasks\n",
    "valid_tasks = os.listdir(PATH_TO_RESULTS_DIR)\n",
    "valid_tasks.remove('chexpert')\n",
    "\n",
    "LABELING_FUNCTION_2_PAPER_NAME = {\n",
    "    # Guo et al. 2023\n",
    "    \"guo_los\": \"Long LOS\",\n",
    "    \"guo_readmission\": \"30-day Readmission\",\n",
    "    \"guo_icu\": \"ICU Admission\",\n",
    "    # New diagnosis\n",
    "    \"new_pancan\": \"Pancreatic Cancer\",\n",
    "    \"new_celiac\": \"Celiac\",\n",
    "    \"new_lupus\": \"Lupus\",\n",
    "    \"new_acutemi\": \"Acute MI\",\n",
    "    \"new_hypertension\": \"Hypertension\",\n",
    "    \"new_hyperlipidemia\": \"Hyperlipidemia\",\n",
    "    # Instant lab values\n",
    "    \"lab_thrombocytopenia\": \"Thrombocytopenia\",\n",
    "    \"lab_hyperkalemia\": \"Hyperkalemia\",\n",
    "    \"lab_hypoglycemia\": \"Hypoglycemia\",\n",
    "    \"lab_hyponatremia\": \"Hyponatremia\",\n",
    "    \"lab_anemia\": \"Anemia\",\n",
    "    # Custom tasks\n",
    "    \"chexpert\": \"Chest X-ray Findings\",\n",
    "}\n",
    "\n",
    "# Filter results to only valid models\n",
    "valid_models = [ \n",
    "    'llama-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'llama-base-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'llama-base-2048--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'llama-base-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-2048--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-8192--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-16384--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-8192--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-16384--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last', \n",
    "    'clmbr',\n",
    "]\n",
    "print(\"Tasks:\", valid_tasks)\n",
    "print(\"# of valid models:\", len(valid_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # For each model...\n",
    "        for model in os.listdir(path_to_results_dir):\n",
    "            print(f'  Processing model: {model}')\n",
    "            path_to_output_file: str = os.path.join(path_to_output_dir, f'df_buckets__model={model}__task={LABELING_FUNCTION}.csv')\n",
    "            os.makedirs(os.path.dirname(path_to_output_dir), exist_ok=True)\n",
    "\n",
    "            # Load EHRSHOT results\n",
    "            ## Each model has its own results; Let's only examine results for k = -1\n",
    "            head: str = 'lr_lbfgs'\n",
    "            path_to_results_dir: str = os.path.join(PATH_TO_RESULTS_DIR, LABELING_FUNCTION, 'models')\n",
    "            path_to_results_file: str = os.path.join(path_to_results_dir, \n",
    "                                                        model, \n",
    "                                                        head, \n",
    "                                                        f'subtask={LABELING_FUNCTION}', \n",
    "                                                        'k=-1', # always use k=-1\n",
    "                                                        'preds.csv')\n",
    "            assert os.path.exists(path_to_results_file), f'Path to results file does not exist: {path_to_results_file}'\n",
    "            df_preds = pd.read_csv(path_to_results_file)\n",
    "\n",
    "            ## Map each result to proper patient id / label time\n",
    "            df_preds['pid_idx'] = train_pids_idx + val_pids_idx + test_pids_idx\n",
    "            df_preds['pid'] = patient_ids[train_pids_idx + val_pids_idx + test_pids_idx]\n",
    "            df_preds['label_time'] = label_times[train_pids_idx + val_pids_idx + test_pids_idx]\n",
    "            df_preds['label_value'] = label_values[train_pids_idx + val_pids_idx + test_pids_idx]\n",
    "            df_preds = df_preds[df_preds['pid_idx'].isin(test_pids_idx)]\n",
    "            \n",
    "            # Calculate quartiles based on each stratification metric\n",
    "            df_results = []\n",
    "            for strat, metrics in tqdm(strats.items(), desc=f'Stratifying {LABELING_FUNCTION}'):\n",
    "                # Load metric for each patient\n",
    "                strat_cols = metrics['strat_cols']\n",
    "                df_metrics = pd.read_parquet(os.path.join(path_to_output_dir, f'df__{LABELING_FUNCTION}__{strat}__metrics.parquet'))\n",
    "                \n",
    "                # For every metric, calculate quartiles\n",
    "                for strat_col in strat_cols:\n",
    "                    if strat_col not in df_metrics.columns:\n",
    "                        print(f'{strat_col} not in df_metrics columns. Skipping...')\n",
    "                        continue\n",
    "                    \n",
    "                    # If stratifying by inter-event times, need to pivot table since 'time' and 'metric' are separate columns\n",
    "                    if strat == 'inter_event_times':\n",
    "                        df_metrics = df_metrics.pivot_table(index=['pid', 'pid_idx', 'label_time', 'sub_task'], columns='metric', values='time').reset_index()\n",
    "                    assert df_metrics.shape[0] == df_preds.shape[0], f'Number of rows in df_metrics does not match number of rows in df_preds: {df_metrics.shape[0]} != {df_preds.shape[0]}'\n",
    "\n",
    "                    # Merge the predictions with the stratification metric\n",
    "                    df_merged = pd.merge(df_preds, df_metrics[['pid', 'label_time', strat_col]], on=['pid', 'label_time',])\n",
    "                    assert df_merged.shape[0] == df_preds.shape[0], f'Number of rows in merged DataFrame does not match number of rows in df_preds: {df_merged.shape[0]} != {df_preds.shape[0]}'\n",
    "                    \n",
    "                    # Create quartiles\n",
    "                    df_merged['metric_name'] = f'{strat_col}'\n",
    "                    df_merged['quartile'] = pd.qcut(df_merged[strat_col].rank(method='min'), 4, labels=False)\n",
    "                    df_merged = df_merged.rename(columns={strat_col: 'metric_value'})\n",
    "\n",
    "                    # Save results\n",
    "                    df_results.append(df_merged)\n",
    "\n",
    "            df_results = pd.concat(df_results, ignore_index=True)\n",
    "            df_results.to_csv(path_to_output_file, index=False)\n",
    "            print('    Saved to:', path_to_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# Read in the patient splits to get all test patient IDs\n",
    "print(\"Loading patient splits data...\")\n",
    "df_splits = pd.read_csv('/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/splits_ehrshot/person_id_map.csv')\n",
    "\n",
    "# Read in the metrics data\n",
    "print(\"Loading metrics data...\")\n",
    "dtypes = {\n",
    "    'sub_task': 'category',\n",
    "    'model': 'category',\n",
    "    'y': 'int8',\n",
    "    'pred_proba': 'float32',\n",
    "    'pid': 'int64',\n",
    "    'pid_idx': 'int32',\n",
    "    'metric_value': 'float32',\n",
    "    'brier_score': 'float32',\n",
    "    'metric_name': 'category',\n",
    "    'quartile': 'float32'\n",
    "}\n",
    "df = pd.read_csv('/share/pi/nigam/mwornow/ehrshot-benchmark/ehrshot/stratify/metrics__llama-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=1000000000-persist_chunk:last_embed:last__per_patient__all_tasks.csv',\n",
    "                 dtype=dtypes)\n",
    "\n",
    "# Filter the dataframe to include only test patients\n",
    "all_patients = df['pid'].unique()\n",
    "\n",
    "print(\"Metrics data loaded and filtered.\")\n",
    "\n",
    "# Patient-level resampling across all patient IDs in the test split\n",
    "print(\"Generating bootstrap weights...\")\n",
    "bootstrap_weights = []\n",
    "np.random.seed(342342)\n",
    "for i in range(1000):\n",
    "    patient_sample = np.random.choice(len(all_patients), len(all_patients), replace=True)\n",
    "    weights = np.zeros_like(all_patients, dtype=np.float32)\n",
    "    np.add.at(weights, patient_sample, 1)\n",
    "    bootstrap_weights.append(weights)\n",
    "print(\"Bootstrap weights generated.\")\n",
    "\n",
    "# Calculate Brier scores for each (model, task, metric_name) over bootstrapped resamples, stratified by quartile\n",
    "print(\"Calculating Brier scores...\")\n",
    "brier_scores = {}\n",
    "grouped = df.groupby(['model', 'sub_task', 'metric_name'])\n",
    "\n",
    "for (model, task, metric_name), df_preds in tqdm(grouped, total=len(grouped)):\n",
    "    # Labels / predictions / quartiles\n",
    "    y = df_preds['y'].values\n",
    "    pred_proba = df_preds['pred_proba'].values\n",
    "    quartiles = df_preds['quartile'].values\n",
    "    patient_ids = df_preds['pid'].values\n",
    "\n",
    "    # Map patient IDs to indices in the all_patients array\n",
    "    patient_id_indices = np.searchsorted(all_patients, patient_ids)\n",
    "\n",
    "    # Filter out any indices that are out of bounds (safety check)\n",
    "    valid_mask = (patient_id_indices >= 0) & (patient_id_indices < len(all_patients))\n",
    "    patient_id_indices = patient_id_indices[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "    pred_proba = pred_proba[valid_mask]\n",
    "    quartiles = quartiles[valid_mask]\n",
    "\n",
    "    # Initialize results storage\n",
    "    brier_scores[(model, task, metric_name)] = {quartile: [] for quartile in np.unique(quartiles)}\n",
    "\n",
    "    # Calculate Brier score for each bootstrap sample and each quartile\n",
    "    for weights in bootstrap_weights:\n",
    "        for quartile in np.unique(quartiles):\n",
    "            # Filter data for the current quartile\n",
    "            mask = quartiles == quartile\n",
    "            y_quartile = y[mask]\n",
    "            pred_proba_quartile = pred_proba[mask]\n",
    "            weights_quartile = weights[patient_id_indices[mask]]\n",
    "\n",
    "            # Calculate Brier score using the weighted samples\n",
    "            if len(y_quartile) > 0:\n",
    "                brier = brier_score_loss(y_quartile, pred_proba_quartile, sample_weight=weights_quartile)\n",
    "                brier_scores[(model, task, metric_name)][quartile].append(brier)\n",
    "\n",
    "# Display Brier score results\n",
    "print(\"Brier score calculation completed. Results:\")\n",
    "for (model, task, metric_name), quartile_results in brier_scores.items():\n",
    "    print(f'Results for model {model}, task {task}, metric {metric_name}:')\n",
    "    for quartile, scores in quartile_results.items():\n",
    "        mean_brier = np.mean(scores)\n",
    "        ci_lower = np.percentile(scores, 2.5)\n",
    "        ci_upper = np.percentile(scores, 97.5)\n",
    "        print(f'  Quartile {quartile}: Mean Brier = {mean_brier:.4f}, 95% CI = ({ci_lower:.4f}, {ci_upper:.4f})')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
