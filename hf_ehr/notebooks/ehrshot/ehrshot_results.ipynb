{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sklearn.utils\n",
    "from typing import List, Optional\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import collections\n",
    "import warnings\n",
    "import pickle\n",
    "import femr.datasets\n",
    "import datetime\n",
    "from hf_ehr.notebooks.ehr_specific_properties import utils\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils.validation\")\n",
    "\n",
    "PATH_TO_DATABASE: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/femr/extract'\n",
    "PATH_TO_FEATURES_DIR: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/features_ehrshot'\n",
    "PATH_TO_RESULTS_DIR: str = '/share/pi/nigam/migufuen/ehrshot-benchmark/EHRSHOT_ASSETS/results_ehrshot'\n",
    "PATH_TO_TOKENIZED_TIMELINES_DIR: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/tokenized_timelines_ehrshot'\n",
    "PATH_TO_LABELS_DIR: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/benchmark_ehrshot'\n",
    "PATH_TO_SPLIT_CSV: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/splits_ehrshot/person_id_map.csv'\n",
    "femr_db = femr.datasets.PatientDatabase(PATH_TO_DATABASE)\n",
    "os.makedirs('../cache', exist_ok=True)\n",
    "\n",
    "IS_LOAD_FROM_CACHE: bool = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "First, we need to load all of the patient-level predictions / labels for every model and task.\n",
    "This takes ~30 min.\n",
    "\n",
    "1. Load list of task names, model names\n",
    "2. Load patient-level predictions for each (model, task)\n",
    "3. Load patient-level ground truth labels for each (task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks: ['new_hypertension', 'guo_los', 'lab_hypoglycemia', 'new_lupus', 'lab_hyponatremia', 'new_pancan', 'lab_anemia', 'new_acutemi', 'guo_readmission', 'lab_thrombocytopenia', 'new_hyperlipidemia', 'new_celiac', 'lab_hyperkalemia', 'guo_icu']\n",
      "# of valid models: 17\n"
     ]
    }
   ],
   "source": [
    "# Get list of tasks\n",
    "valid_tasks = os.listdir(PATH_TO_RESULTS_DIR)\n",
    "valid_tasks.remove('chexpert')\n",
    "\n",
    "LABELING_FUNCTION_2_PAPER_NAME = {\n",
    "    # Guo et al. 2023\n",
    "    \"guo_los\": \"Long LOS\",\n",
    "    \"guo_readmission\": \"30-day Readmission\",\n",
    "    \"guo_icu\": \"ICU Admission\",\n",
    "    # New diagnosis\n",
    "    \"new_pancan\": \"Pancreatic Cancer\",\n",
    "    \"new_celiac\": \"Celiac\",\n",
    "    \"new_lupus\": \"Lupus\",\n",
    "    \"new_acutemi\": \"Acute MI\",\n",
    "    \"new_hypertension\": \"Hypertension\",\n",
    "    \"new_hyperlipidemia\": \"Hyperlipidemia\",\n",
    "    # Instant lab values\n",
    "    \"lab_thrombocytopenia\": \"Thrombocytopenia\",\n",
    "    \"lab_hyperkalemia\": \"Hyperkalemia\",\n",
    "    \"lab_hypoglycemia\": \"Hypoglycemia\",\n",
    "    \"lab_hyponatremia\": \"Hyponatremia\",\n",
    "    \"lab_anemia\": \"Anemia\",\n",
    "    # Custom tasks\n",
    "    \"chexpert\": \"Chest X-ray Findings\",\n",
    "}\n",
    "\n",
    "# Filter results to only valid models\n",
    "valid_models = [ \n",
    "    'llama-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'llama-base-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'llama-base-2048--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'llama-base-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-2048--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-8192--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-16384--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-8192--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-16384--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last', \n",
    "    'clmbr',\n",
    "]\n",
    "print(\"Tasks:\", valid_tasks)\n",
    "print(\"# of valid models:\", len(valid_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:12<00:00,  9.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of (model, task) preds: 238\n"
     ]
    }
   ],
   "source": [
    "# Load patient-level predictions for each (model, task)\n",
    "predictions = {} # [key] = (model, task), [value] = df_preds\n",
    "\n",
    "if IS_LOAD_FROM_CACHE and os.path.exists('../cache/predictions.pkl'):\n",
    "    print(\"Loading predictions from cache...\")\n",
    "    predictions = pickle.load(open('../cache/predictions.pkl', 'rb'))\n",
    "else:\n",
    "    for task in tqdm(valid_tasks):\n",
    "        for model in valid_models:\n",
    "            path = os.path.join(PATH_TO_RESULTS_DIR, task, 'models', model, 'lr_lbfgs', f'subtask={task}', 'k=-1', 'preds.csv')\n",
    "            if not os.path.exists(path):\n",
    "                print(\"Missing path for \", model, task)\n",
    "            df_preds = pd.read_csv(path)\n",
    "            assert df_preds['replicate'].nunique() == 1, f\"Multiple replicates for {model}, {task}\"\n",
    "            predictions[(model, task)] = df_preds\n",
    "    # Save results to .pkl file\n",
    "    with open('../cache/predictions.pkl', 'wb') as f:\n",
    "        pickle.dump(predictions, f)\n",
    "print(\"# of (model, task) preds:\", len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [00:15<03:27, 15.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [00:39<04:05, 20.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 3/14 [14:40<1:12:25, 395.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [15:03<41:23, 248.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 5/14 [24:28<54:23, 362.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6/14 [24:54<33:03, 247.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7/14 [33:07<38:17, 328.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [33:33<23:12, 232.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 9/14 [33:58<13:55, 167.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [41:57<17:34, 263.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 11/14 [42:15<09:25, 188.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 12/14 [42:39<04:36, 138.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 13/14 [51:29<04:17, 257.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for model: gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [51:53<00:00, 222.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load patient-level labels for each task\n",
    "# NOTE: Takes ~1 hr\n",
    "label_data = {} # [key] = task, [value] = {'times': label_times, 'values': label_values, 'patient_ids': patient_ids}\n",
    "\n",
    "if IS_LOAD_FROM_CACHE and os.path.exists('../cache/label_data.pkl'):\n",
    "    print(\"Loading label data from cache...\")\n",
    "    label_data = pickle.load(open('../cache/label_data.pkl', 'rb'))\n",
    "else:\n",
    "    for task in tqdm(valid_tasks):\n",
    "        # Load labeled patients for this task\n",
    "        LABELING_FUNCTION: str = task\n",
    "        PATH_TO_LABELED_PATIENTS: str =  os.path.join(PATH_TO_LABELS_DIR, LABELING_FUNCTION, 'labeled_patients.csv')\n",
    "        labeled_patients = femr.labelers.load_labeled_patients(PATH_TO_LABELED_PATIENTS)\n",
    "        \n",
    "        # Get features for patients\n",
    "        model: str = 'gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last'\n",
    "        patient_ids, label_values, label_times, feature_matrixes = utils.get_labels_and_features(labeled_patients, \n",
    "                                                                                            PATH_TO_FEATURES_DIR, \n",
    "                                                                                            PATH_TO_TOKENIZED_TIMELINES_DIR,\n",
    "                                                                                            models_to_keep=[model,])\n",
    "        train_pids_idx, val_pids_idx, test_pids_idx = utils.get_patient_splits_by_idx(PATH_TO_SPLIT_CSV, patient_ids)\n",
    "        label_times = label_times[train_pids_idx + val_pids_idx + test_pids_idx]\n",
    "        label_values = label_values[train_pids_idx + val_pids_idx + test_pids_idx]\n",
    "        patient_ids = patient_ids[train_pids_idx + val_pids_idx + test_pids_idx]\n",
    "        label_times = [ x.astype(datetime.datetime) for x in label_times ] # cast to Python datetime\n",
    "        label_data[task] = {'times': label_times, 'values': label_values, 'patient_ids': patient_ids}\n",
    "\n",
    "    # Save results to .pkl file\n",
    "    with open('../cache/label_data.pkl', 'wb') as f:\n",
    "        pickle.dump(label_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate CIs\n",
    "\n",
    "Calculate bootstrapped 95% CIs over the test set (one sample per patient).\n",
    "\n",
    "1. Generate 1000 bootstrapped resamples of patient IDs across all splits\n",
    "2. Loop through every (model, task) preds, limit to test set, loop through 1000 bootstrap weightings and calculate AUROC with patients reweighted accordingly (ignoring non-test patients)\n",
    "3. Calculate AUROC delta between each (model, task) and a **baseline** model (e.g. CLMBR, gpt-base-512, etc.)\n",
    "4. Calculate 95% CIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Bootstrapping of AUROC Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6275\n"
     ]
    }
   ],
   "source": [
    "# Get all patient IDs\n",
    "all_patients = np.sort(np.unique(np.concatenate([v['patient_ids'] for v in label_data.values()])))\n",
    "print(len(all_patients))\n",
    "\n",
    "# Patient-level resampling across all patient IDs in train/val/test\n",
    "# Later, we limit to just test patient IDs per task\n",
    "bootstrap_weights = []\n",
    "np.random.seed(342342)\n",
    "for i in range(1000):\n",
    "    patient_sample = np.random.choice(list(range(len(all_patients))), len(all_patients), replace=True)\n",
    "    weights = np.zeros_like(all_patients, dtype=np.float32)\n",
    "    np.add.at(weights, patient_sample, 1)\n",
    "    assert np.mean(weights) == 1\n",
    "    bootstrap_weights.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [37:50<00:00,  9.54s/it] \n"
     ]
    }
   ],
   "source": [
    "# Calculate AUROC scores for each (model, task) over bootstrapped resamples\n",
    "# NOTE: Takes ~1 hr\n",
    "auroc_scores = collections.defaultdict(list) # [key] = (model, task), [value] = auroc's across 1k resamples\n",
    "\n",
    "if IS_LOAD_FROM_CACHE and os.path.exists('../cache/auroc_scores.pkl'):\n",
    "    print(\"Loading auroc scores from cache...\")\n",
    "    auroc_scores = pickle.load(open('../cache/auroc_scores.pkl', 'rb'))\n",
    "else:\n",
    "    # Do bootstrap resampling for each (model, task)\n",
    "    for (model, task), df_preds in tqdm(predictions.items(), total=len(predictions)):\n",
    "        # Test split\n",
    "        df_preds['patient_ids'] = label_data[task]['patient_ids']\n",
    "        df_ = df_preds[df_preds['split'] == 'test']\n",
    "\n",
    "        # Labels / Preds\n",
    "        y = df_['y'].values\n",
    "        pred_proba = df_['pred_proba'].values\n",
    "\n",
    "        # patient_ids \n",
    "        patient_ids = df_['patient_ids'].values\n",
    "        patient_id_indices = np.searchsorted(all_patients, patient_ids)\n",
    "        assert np.all(patient_ids == all_patients[patient_id_indices])\n",
    "        \n",
    "        for weights in bootstrap_weights:\n",
    "            auroc = roc_auc_score(y, pred_proba, sample_weight=weights[patient_id_indices])\n",
    "            auroc_scores[(model, task)].append(auroc)\n",
    "\n",
    "    # Save results to .pkl file\n",
    "    with open('../cache/auroc_scores.pkl', 'wb') as f:\n",
    "        pickle.dump(dict(auroc_scores), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:00<00:00, 9060.53it/s]\n",
      "100%|██████████| 238/238 [00:00<00:00, 9001.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate delta in AUROC scores for each (model, task) v. baseline\n",
    "baseline_auroc_deltas = {} # [key] = baseline, [value] = auroc_deltas for all (model, task)\n",
    "for baseline in [\n",
    "    'clmbr',\n",
    "    'gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "]:\n",
    "    auroc_deltas = collections.defaultdict(list) # [key] = (model, task), [value] = diff in auroc v. CLMBR across 1k resamples\n",
    "    if IS_LOAD_FROM_CACHE and os.path.exists(f'../cache/auroc_deltas_{baseline}.pkl'):\n",
    "        print(f\"Loading auroc deltas for {baseline} from cache...\")\n",
    "        auroc_deltas = pickle.load(open(f'../cache/auroc_deltas_{baseline}.pkl', 'rb'))\n",
    "    else:\n",
    "        for (model, task), score in tqdm(auroc_scores.items(), total=len(auroc_scores)):\n",
    "            deltas = np.array(auroc_scores[(model, task)]) - np.array(auroc_scores[(baseline, task)])\n",
    "            auroc_deltas[(model, task)] = deltas.tolist()\n",
    "\n",
    "        # Save results to .pkl file\n",
    "        with open(f'../cache/auroc_deltas_{baseline}.pkl', 'wb') as f:\n",
    "            pickle.dump(dict(auroc_deltas), f)\n",
    "    baseline_auroc_deltas[baseline] = auroc_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting\n",
    "def clean_df_deltas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['model_name'] = df['model'].apply(lambda x: x.split('--')[0])\n",
    "    df['model_base'] = df['model'].apply(lambda x: x.split('-')[0])\n",
    "    df['ctx_length'] = df['model'].apply(lambda x: int(x.split('--')[0].split('-')[-1]) if x != 'clmbr' else 0).astype(int)\n",
    "    df['formatted_auroc_delta_mean'] = df['auroc_delta_mean'].apply(lambda x: f\"{x:.3f}\")\n",
    "    df['formatted_auroc_delta'] = df.apply(lambda x: f\"{x['auroc_delta_mean']:.3f} ({x['auroc_delta_ci_025']:.3f}, {x['auroc_delta_ci_975']:.3f})\", axis=1)\n",
    "    df['formatted_auroc_ci'] = df.apply(lambda x: f\"({x['auroc_delta_ci_025']:.3f}, {x['auroc_delta_ci_975']:.3f})\", axis=1)\n",
    "    df['is_auroc_win'] = (df['auroc_delta_ci_025'] > 0) & (df['auroc_delta_ci_975'] > 0)\n",
    "    df['is_stat_sig'] = (df['auroc_delta_ci_025'] > 0) | (df['auroc_delta_ci_975'] < 0)\n",
    "    df = df[['model', 'model_name', 'model_base', 'ctx_length'] + [col for col in df.columns if col not in ['model', 'model_name', 'model_base', 'ctx_length']]]\n",
    "    df = df.sort_values(['model_base', 'ctx_length', ] + ([ 'task' ] if 'task' in df.columns else []))\n",
    "    return df\n",
    "\n",
    "def format_df_deltas(df: pd.DataFrame, model_start: Optional[str] = None) -> pd.DataFrame:\n",
    "    if model_start is not None:\n",
    "        df = df[df['model'].str.startswith(model_start)]\n",
    "    df = df.drop(columns=['model', 'model_name', 'formatted_auroc_delta', 'auroc_delta_mean', 'auroc_delta_ci_025', 'auroc_delta_ci_500', 'auroc_delta_ci_975', 'is_auroc_win'], errors='ignore') \\\n",
    "        .rename(columns={   'model_base' : 'Model', \n",
    "                            'ctx_length' : 'Context Length', \n",
    "                            'formatted_auroc_delta_mean' : r'$\\Delta$ over baseline', \n",
    "                            'formatted_auroc_ci' : '95% CI', \n",
    "                            'is_stat_sig' : 'Statistically Significant'} | ({ 'task' : 'Task'} if 'task' in df.columns else {})) \\\n",
    "        .sort_values(['Model', 'Context Length',] + ([ 'Task' ] if 'task' in df.columns else []))\n",
    "    return df\n",
    "\n",
    "def format_df_deltas_for_latex(df: pd.DataFrame, model_start: Optional[str] = None) -> str:\n",
    "    latex = format_df_deltas(df, model_start).to_latex(index=False, escape=False)\n",
    "    for k, v in LABELING_FUNCTION_2_PAPER_NAME.items():\n",
    "        latex = latex.replace(k, v)\n",
    "    latex = latex.replace('False', '').replace('True', '\\checkmark')\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Level CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_base</th>\n",
       "      <th>ctx_length</th>\n",
       "      <th>win_mean</th>\n",
       "      <th>win_ci_025</th>\n",
       "      <th>win_ci_500</th>\n",
       "      <th>win_ci_975</th>\n",
       "      <th>auroc_delta_mean</th>\n",
       "      <th>auroc_delta_ci_025</th>\n",
       "      <th>auroc_delta_ci_500</th>\n",
       "      <th>auroc_delta_ci_975</th>\n",
       "      <th>formatted_auroc_delta_mean</th>\n",
       "      <th>formatted_auroc_delta</th>\n",
       "      <th>formatted_auroc_ci</th>\n",
       "      <th>is_auroc_win</th>\n",
       "      <th>is_stat_sig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000 (0.000, 0.000)</td>\n",
       "      <td>(0.000, 0.000)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2-base-512--clmbr_train-tokens-total_nonPAD...</td>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>512</td>\n",
       "      <td>0.196143</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.017038</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017 (0.005, 0.028)</td>\n",
       "      <td>(0.005, 0.028)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2-base-1024--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>-0.002871</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.021149</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008 (-0.003, 0.021)</td>\n",
       "      <td>(-0.003, 0.021)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt2-base-2048--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.024723</td>\n",
       "      <td>0.008462</td>\n",
       "      <td>0.025030</td>\n",
       "      <td>0.041175</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025 (0.008, 0.041)</td>\n",
       "      <td>(0.008, 0.041)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt2-base-4096--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.423929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>0.007289</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.032440</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019 (0.007, 0.032)</td>\n",
       "      <td>(0.007, 0.032)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hyena-large-1024--clmbr_train-tokens-total_non...</td>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>hyena</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.019351</td>\n",
       "      <td>0.006742</td>\n",
       "      <td>0.019035</td>\n",
       "      <td>0.033037</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019 (0.007, 0.033)</td>\n",
       "      <td>(0.007, 0.033)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hyena-large-4096--clmbr_train-tokens-total_non...</td>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>hyena</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.112429</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.021339</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.021063</td>\n",
       "      <td>0.035929</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021 (0.007, 0.036)</td>\n",
       "      <td>(0.007, 0.036)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hyena-large-8192--clmbr_train-tokens-total_non...</td>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>hyena</td>\n",
       "      <td>8192</td>\n",
       "      <td>-0.266714</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008174</td>\n",
       "      <td>-0.024471</td>\n",
       "      <td>-0.008385</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.008 (-0.024, 0.010)</td>\n",
       "      <td>(-0.024, 0.010)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hyena-large-16384--clmbr_train-tokens-total_no...</td>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>hyena</td>\n",
       "      <td>16384</td>\n",
       "      <td>-0.571929</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.042903</td>\n",
       "      <td>-0.059552</td>\n",
       "      <td>-0.042844</td>\n",
       "      <td>-0.025433</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.043 (-0.060, -0.025)</td>\n",
       "      <td>(-0.060, -0.025)</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>-0.054214</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.026618</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014 (0.001, 0.027)</td>\n",
       "      <td>(0.001, 0.027)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama-base-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>llama</td>\n",
       "      <td>1024</td>\n",
       "      <td>-0.142286</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.015063</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.027171</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015 (0.003, 0.027)</td>\n",
       "      <td>(0.003, 0.027)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-base-2048--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>llama</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.519214</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.025471</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.025616</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025 (0.013, 0.038)</td>\n",
       "      <td>(0.013, 0.038)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.464571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.010415</td>\n",
       "      <td>0.021987</td>\n",
       "      <td>0.034176</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022 (0.010, 0.034)</td>\n",
       "      <td>(0.010, 0.034)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.147571</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.009338</td>\n",
       "      <td>-0.005357</td>\n",
       "      <td>0.009645</td>\n",
       "      <td>0.023182</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009 (-0.005, 0.023)</td>\n",
       "      <td>(-0.005, 0.023)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>mamba</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.570214</td>\n",
       "      <td>0.283929</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.026839</td>\n",
       "      <td>0.015704</td>\n",
       "      <td>0.026950</td>\n",
       "      <td>0.037783</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027 (0.016, 0.038)</td>\n",
       "      <td>(0.016, 0.038)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>mamba</td>\n",
       "      <td>8192</td>\n",
       "      <td>0.555143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.025373</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.025231</td>\n",
       "      <td>0.036851</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025 (0.015, 0.037)</td>\n",
       "      <td>(0.015, 0.037)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.778357</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033288</td>\n",
       "      <td>0.021776</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.045925</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033 (0.022, 0.046)</td>\n",
       "      <td>(0.022, 0.046)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model         model_name  \\\n",
       "16                                              clmbr              clmbr   \n",
       "4   gpt2-base-512--clmbr_train-tokens-total_nonPAD...      gpt2-base-512   \n",
       "5   gpt2-base-1024--clmbr_train-tokens-total_nonPA...     gpt2-base-1024   \n",
       "6   gpt2-base-2048--clmbr_train-tokens-total_nonPA...     gpt2-base-2048   \n",
       "7   gpt2-base-4096--clmbr_train-tokens-total_nonPA...     gpt2-base-4096   \n",
       "8   hyena-large-1024--clmbr_train-tokens-total_non...   hyena-large-1024   \n",
       "9   hyena-large-4096--clmbr_train-tokens-total_non...   hyena-large-4096   \n",
       "10  hyena-large-8192--clmbr_train-tokens-total_non...   hyena-large-8192   \n",
       "11  hyena-large-16384--clmbr_train-tokens-total_no...  hyena-large-16384   \n",
       "0   llama-base-512--clmbr_train-tokens-total_nonPA...     llama-base-512   \n",
       "1   llama-base-1024--clmbr_train-tokens-total_nonP...    llama-base-1024   \n",
       "2   llama-base-2048--clmbr_train-tokens-total_nonP...    llama-base-2048   \n",
       "3   llama-base-4096--clmbr_train-tokens-total_nonP...    llama-base-4096   \n",
       "12  mamba-tiny-1024--clmbr_train-tokens-total_nonP...    mamba-tiny-1024   \n",
       "13  mamba-tiny-4096--clmbr_train-tokens-total_nonP...    mamba-tiny-4096   \n",
       "14  mamba-tiny-8192--clmbr_train-tokens-total_nonP...    mamba-tiny-8192   \n",
       "15  mamba-tiny-16384--clmbr_train-tokens-total_non...   mamba-tiny-16384   \n",
       "\n",
       "   model_base  ctx_length  win_mean  win_ci_025  win_ci_500  win_ci_975  \\\n",
       "16      clmbr           0  0.000000    0.000000    0.000000    0.000000   \n",
       "4        gpt2         512  0.196143   -0.142857    0.142857    0.571429   \n",
       "5        gpt2        1024  0.005714   -0.285714    0.000000    0.285714   \n",
       "6        gpt2        2048  0.216500   -0.142857    0.285714    0.571429   \n",
       "7        gpt2        4096  0.423929    0.000000    0.428571    0.714286   \n",
       "8       hyena        1024  0.131500   -0.142857    0.142857    0.428571   \n",
       "9       hyena        4096  0.112429   -0.142857    0.142857    0.428571   \n",
       "10      hyena        8192 -0.266714   -0.428571   -0.285714    0.000000   \n",
       "11      hyena       16384 -0.571929   -0.714286   -0.571429   -0.428571   \n",
       "0       llama         512 -0.054214   -0.428571    0.000000    0.285714   \n",
       "1       llama        1024 -0.142286   -0.428571   -0.142857    0.285714   \n",
       "2       llama        2048  0.519214    0.142857    0.571429    0.857143   \n",
       "3       llama        4096  0.464571    0.142857    0.428571    0.714286   \n",
       "12      mamba        1024  0.147571   -0.285714    0.142857    0.571429   \n",
       "13      mamba        4096  0.570214    0.283929    0.571429    0.857143   \n",
       "14      mamba        8192  0.555143    0.285714    0.571429    0.857143   \n",
       "15      mamba       16384  0.778357    0.428571    0.857143    1.000000   \n",
       "\n",
       "    auroc_delta_mean  auroc_delta_ci_025  auroc_delta_ci_500  \\\n",
       "16          0.000000            0.000000            0.000000   \n",
       "4           0.017038            0.005149            0.017045   \n",
       "5           0.008263           -0.002871            0.007939   \n",
       "6           0.024723            0.008462            0.025030   \n",
       "7           0.019307            0.007289            0.019119   \n",
       "8           0.019351            0.006742            0.019035   \n",
       "9           0.021339            0.007195            0.021063   \n",
       "10         -0.008174           -0.024471           -0.008385   \n",
       "11         -0.042903           -0.059552           -0.042844   \n",
       "0           0.013981            0.001354            0.013903   \n",
       "1           0.015063            0.002777            0.015146   \n",
       "2           0.025471            0.013091            0.025616   \n",
       "3           0.022074            0.010415            0.021987   \n",
       "12          0.009338           -0.005357            0.009645   \n",
       "13          0.026839            0.015704            0.026950   \n",
       "14          0.025373            0.014900            0.025231   \n",
       "15          0.033288            0.021776            0.033060   \n",
       "\n",
       "    auroc_delta_ci_975 formatted_auroc_delta_mean    formatted_auroc_delta  \\\n",
       "16            0.000000                      0.000     0.000 (0.000, 0.000)   \n",
       "4             0.028389                      0.017     0.017 (0.005, 0.028)   \n",
       "5             0.021149                      0.008    0.008 (-0.003, 0.021)   \n",
       "6             0.041175                      0.025     0.025 (0.008, 0.041)   \n",
       "7             0.032440                      0.019     0.019 (0.007, 0.032)   \n",
       "8             0.033037                      0.019     0.019 (0.007, 0.033)   \n",
       "9             0.035929                      0.021     0.021 (0.007, 0.036)   \n",
       "10            0.009969                     -0.008   -0.008 (-0.024, 0.010)   \n",
       "11           -0.025433                     -0.043  -0.043 (-0.060, -0.025)   \n",
       "0             0.026618                      0.014     0.014 (0.001, 0.027)   \n",
       "1             0.027171                      0.015     0.015 (0.003, 0.027)   \n",
       "2             0.037545                      0.025     0.025 (0.013, 0.038)   \n",
       "3             0.034176                      0.022     0.022 (0.010, 0.034)   \n",
       "12            0.023182                      0.009    0.009 (-0.005, 0.023)   \n",
       "13            0.037783                      0.027     0.027 (0.016, 0.038)   \n",
       "14            0.036851                      0.025     0.025 (0.015, 0.037)   \n",
       "15            0.045925                      0.033     0.033 (0.022, 0.046)   \n",
       "\n",
       "   formatted_auroc_ci  is_auroc_win  is_stat_sig  \n",
       "16     (0.000, 0.000)         False        False  \n",
       "4      (0.005, 0.028)          True         True  \n",
       "5     (-0.003, 0.021)         False        False  \n",
       "6      (0.008, 0.041)          True         True  \n",
       "7      (0.007, 0.032)          True         True  \n",
       "8      (0.007, 0.033)          True         True  \n",
       "9      (0.007, 0.036)          True         True  \n",
       "10    (-0.024, 0.010)         False        False  \n",
       "11   (-0.060, -0.025)         False         True  \n",
       "0      (0.001, 0.027)          True         True  \n",
       "1      (0.003, 0.027)          True         True  \n",
       "2      (0.013, 0.038)          True         True  \n",
       "3      (0.010, 0.034)          True         True  \n",
       "12    (-0.005, 0.023)         False        False  \n",
       "13     (0.016, 0.038)          True         True  \n",
       "14     (0.015, 0.037)          True         True  \n",
       "15     (0.022, 0.046)          True         True  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model-level delta CIs\n",
    "baseline: str = 'clmbr'\n",
    "\n",
    "df_model_deltas = []\n",
    "for model in valid_models:\n",
    "    # We care about the actual delta between model and baseline\n",
    "    aurocs = np.zeros(len(bootstrap_weights)) \n",
    "    for task in valid_tasks:\n",
    "        aurocs += baseline_auroc_deltas[baseline][(model, task)]\n",
    "    aurocs /= len(valid_tasks)\n",
    "    \n",
    "    # We only care about BINARY of whether model BEATS the baseline\n",
    "    win_aurocs = np.zeros(len(bootstrap_weights))\n",
    "    for task in valid_tasks:\n",
    "        win_aurocs += np.sign(baseline_auroc_deltas[baseline][(model, task)])\n",
    "    win_aurocs /= len(valid_tasks)\n",
    "\n",
    "    df_model_deltas.append({\n",
    "        'model' : model,\n",
    "        'win_mean' : np.mean(win_aurocs),\n",
    "        'win_ci_025' : np.percentile(win_aurocs, 2.5),\n",
    "        'win_ci_500' : np.percentile(win_aurocs, 50),\n",
    "        'win_ci_975' : np.percentile(win_aurocs, 97.5),\n",
    "        'auroc_delta_mean' : np.mean(aurocs),\n",
    "        'auroc_delta_ci_025' : np.percentile(aurocs, 2.5),\n",
    "        'auroc_delta_ci_500' : np.percentile(aurocs, 50),\n",
    "        'auroc_delta_ci_975' : np.percentile(aurocs, 97.5),\n",
    "    })\n",
    "df_model_deltas = pd.DataFrame(df_model_deltas)\n",
    "df_model_deltas = clean_df_deltas(df_model_deltas)\n",
    "df_model_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Context Length</th>\n",
       "      <th>win_mean</th>\n",
       "      <th>win_ci_025</th>\n",
       "      <th>win_ci_500</th>\n",
       "      <th>win_ci_975</th>\n",
       "      <th>$\\Delta$ over baseline</th>\n",
       "      <th>95% CI</th>\n",
       "      <th>Statistically Significant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>(0.000, 0.000)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>512</td>\n",
       "      <td>0.196143</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.017</td>\n",
       "      <td>(0.005, 0.028)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.008</td>\n",
       "      <td>(-0.003, 0.021)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.025</td>\n",
       "      <td>(0.008, 0.041)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.423929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.019</td>\n",
       "      <td>(0.007, 0.032)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hyena</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.019</td>\n",
       "      <td>(0.007, 0.033)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hyena</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.112429</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.021</td>\n",
       "      <td>(0.007, 0.036)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hyena</td>\n",
       "      <td>8192</td>\n",
       "      <td>-0.266714</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>(-0.024, 0.010)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hyena</td>\n",
       "      <td>16384</td>\n",
       "      <td>-0.571929</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>(-0.060, -0.025)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>-0.054214</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.014</td>\n",
       "      <td>(0.001, 0.027)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama</td>\n",
       "      <td>1024</td>\n",
       "      <td>-0.142286</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.015</td>\n",
       "      <td>(0.003, 0.027)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.519214</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.025</td>\n",
       "      <td>(0.013, 0.038)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.464571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.022</td>\n",
       "      <td>(0.010, 0.034)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.147571</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.009</td>\n",
       "      <td>(-0.005, 0.023)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.570214</td>\n",
       "      <td>0.283929</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.027</td>\n",
       "      <td>(0.016, 0.038)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba</td>\n",
       "      <td>8192</td>\n",
       "      <td>0.555143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.025</td>\n",
       "      <td>(0.015, 0.037)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.778357</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>(0.022, 0.046)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model  Context Length  win_mean  win_ci_025  win_ci_500  win_ci_975  \\\n",
       "16  clmbr               0  0.000000    0.000000    0.000000    0.000000   \n",
       "4    gpt2             512  0.196143   -0.142857    0.142857    0.571429   \n",
       "5    gpt2            1024  0.005714   -0.285714    0.000000    0.285714   \n",
       "6    gpt2            2048  0.216500   -0.142857    0.285714    0.571429   \n",
       "7    gpt2            4096  0.423929    0.000000    0.428571    0.714286   \n",
       "8   hyena            1024  0.131500   -0.142857    0.142857    0.428571   \n",
       "9   hyena            4096  0.112429   -0.142857    0.142857    0.428571   \n",
       "10  hyena            8192 -0.266714   -0.428571   -0.285714    0.000000   \n",
       "11  hyena           16384 -0.571929   -0.714286   -0.571429   -0.428571   \n",
       "0   llama             512 -0.054214   -0.428571    0.000000    0.285714   \n",
       "1   llama            1024 -0.142286   -0.428571   -0.142857    0.285714   \n",
       "2   llama            2048  0.519214    0.142857    0.571429    0.857143   \n",
       "3   llama            4096  0.464571    0.142857    0.428571    0.714286   \n",
       "12  mamba            1024  0.147571   -0.285714    0.142857    0.571429   \n",
       "13  mamba            4096  0.570214    0.283929    0.571429    0.857143   \n",
       "14  mamba            8192  0.555143    0.285714    0.571429    0.857143   \n",
       "15  mamba           16384  0.778357    0.428571    0.857143    1.000000   \n",
       "\n",
       "   $\\Delta$ over baseline            95% CI  Statistically Significant  \n",
       "16                  0.000    (0.000, 0.000)                      False  \n",
       "4                   0.017    (0.005, 0.028)                       True  \n",
       "5                   0.008   (-0.003, 0.021)                      False  \n",
       "6                   0.025    (0.008, 0.041)                       True  \n",
       "7                   0.019    (0.007, 0.032)                       True  \n",
       "8                   0.019    (0.007, 0.033)                       True  \n",
       "9                   0.021    (0.007, 0.036)                       True  \n",
       "10                 -0.008   (-0.024, 0.010)                      False  \n",
       "11                 -0.043  (-0.060, -0.025)                       True  \n",
       "0                   0.014    (0.001, 0.027)                       True  \n",
       "1                   0.015    (0.003, 0.027)                       True  \n",
       "2                   0.025    (0.013, 0.038)                       True  \n",
       "3                   0.022    (0.010, 0.034)                       True  \n",
       "12                  0.009   (-0.005, 0.023)                      False  \n",
       "13                  0.027    (0.016, 0.038)                       True  \n",
       "14                  0.025    (0.015, 0.037)                       True  \n",
       "15                  0.033    (0.022, 0.046)                       True  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_df_deltas(df_model_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-Level CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_base</th>\n",
       "      <th>ctx_length</th>\n",
       "      <th>task</th>\n",
       "      <th>auroc_delta_mean</th>\n",
       "      <th>auroc_delta_ci_025</th>\n",
       "      <th>auroc_delta_ci_500</th>\n",
       "      <th>auroc_delta_ci_975</th>\n",
       "      <th>formatted_auroc_delta_mean</th>\n",
       "      <th>formatted_auroc_delta</th>\n",
       "      <th>formatted_auroc_ci</th>\n",
       "      <th>is_auroc_win</th>\n",
       "      <th>is_stat_sig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>0</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000 (0.000, 0.000)</td>\n",
       "      <td>(0.000, 0.000)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>0</td>\n",
       "      <td>guo_los</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000 (0.000, 0.000)</td>\n",
       "      <td>(0.000, 0.000)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>0</td>\n",
       "      <td>guo_readmission</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000 (0.000, 0.000)</td>\n",
       "      <td>(0.000, 0.000)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>0</td>\n",
       "      <td>lab_anemia</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000 (0.000, 0.000)</td>\n",
       "      <td>(0.000, 0.000)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>0</td>\n",
       "      <td>lab_hyperkalemia</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000 (0.000, 0.000)</td>\n",
       "      <td>(0.000, 0.000)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_celiac</td>\n",
       "      <td>0.193925</td>\n",
       "      <td>0.108023</td>\n",
       "      <td>0.184678</td>\n",
       "      <td>0.333350</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194 (0.108, 0.333)</td>\n",
       "      <td>(0.108, 0.333)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_hyperlipidemia</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>-0.012559</td>\n",
       "      <td>0.022141</td>\n",
       "      <td>0.057557</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023 (-0.013, 0.058)</td>\n",
       "      <td>(-0.013, 0.058)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>-0.017625</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003 (-0.018, 0.023)</td>\n",
       "      <td>(-0.018, 0.023)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_lupus</td>\n",
       "      <td>0.036555</td>\n",
       "      <td>-0.056182</td>\n",
       "      <td>0.035704</td>\n",
       "      <td>0.132338</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037 (-0.056, 0.132)</td>\n",
       "      <td>(-0.056, 0.132)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_pancan</td>\n",
       "      <td>0.053438</td>\n",
       "      <td>0.023811</td>\n",
       "      <td>0.052413</td>\n",
       "      <td>0.087347</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053 (0.024, 0.087)</td>\n",
       "      <td>(0.024, 0.087)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 model        model_name  \\\n",
       "237                                              clmbr             clmbr   \n",
       "225                                              clmbr             clmbr   \n",
       "232                                              clmbr             clmbr   \n",
       "230                                              clmbr             clmbr   \n",
       "236                                              clmbr             clmbr   \n",
       "..                                                 ...               ...   \n",
       "221  mamba-tiny-16384--clmbr_train-tokens-total_non...  mamba-tiny-16384   \n",
       "220  mamba-tiny-16384--clmbr_train-tokens-total_non...  mamba-tiny-16384   \n",
       "210  mamba-tiny-16384--clmbr_train-tokens-total_non...  mamba-tiny-16384   \n",
       "213  mamba-tiny-16384--clmbr_train-tokens-total_non...  mamba-tiny-16384   \n",
       "215  mamba-tiny-16384--clmbr_train-tokens-total_non...  mamba-tiny-16384   \n",
       "\n",
       "    model_base  ctx_length                task  auroc_delta_mean  \\\n",
       "237      clmbr           0             guo_icu          0.000000   \n",
       "225      clmbr           0             guo_los          0.000000   \n",
       "232      clmbr           0     guo_readmission          0.000000   \n",
       "230      clmbr           0          lab_anemia          0.000000   \n",
       "236      clmbr           0    lab_hyperkalemia          0.000000   \n",
       "..         ...         ...                 ...               ...   \n",
       "221      mamba       16384          new_celiac          0.193925   \n",
       "220      mamba       16384  new_hyperlipidemia          0.022537   \n",
       "210      mamba       16384    new_hypertension          0.003078   \n",
       "213      mamba       16384           new_lupus          0.036555   \n",
       "215      mamba       16384          new_pancan          0.053438   \n",
       "\n",
       "     auroc_delta_ci_025  auroc_delta_ci_500  auroc_delta_ci_975  \\\n",
       "237            0.000000            0.000000            0.000000   \n",
       "225            0.000000            0.000000            0.000000   \n",
       "232            0.000000            0.000000            0.000000   \n",
       "230            0.000000            0.000000            0.000000   \n",
       "236            0.000000            0.000000            0.000000   \n",
       "..                  ...                 ...                 ...   \n",
       "221            0.108023            0.184678            0.333350   \n",
       "220           -0.012559            0.022141            0.057557   \n",
       "210           -0.017625            0.002916            0.022696   \n",
       "213           -0.056182            0.035704            0.132338   \n",
       "215            0.023811            0.052413            0.087347   \n",
       "\n",
       "    formatted_auroc_delta_mean  formatted_auroc_delta formatted_auroc_ci  \\\n",
       "237                      0.000   0.000 (0.000, 0.000)     (0.000, 0.000)   \n",
       "225                      0.000   0.000 (0.000, 0.000)     (0.000, 0.000)   \n",
       "232                      0.000   0.000 (0.000, 0.000)     (0.000, 0.000)   \n",
       "230                      0.000   0.000 (0.000, 0.000)     (0.000, 0.000)   \n",
       "236                      0.000   0.000 (0.000, 0.000)     (0.000, 0.000)   \n",
       "..                         ...                    ...                ...   \n",
       "221                      0.194   0.194 (0.108, 0.333)     (0.108, 0.333)   \n",
       "220                      0.023  0.023 (-0.013, 0.058)    (-0.013, 0.058)   \n",
       "210                      0.003  0.003 (-0.018, 0.023)    (-0.018, 0.023)   \n",
       "213                      0.037  0.037 (-0.056, 0.132)    (-0.056, 0.132)   \n",
       "215                      0.053   0.053 (0.024, 0.087)     (0.024, 0.087)   \n",
       "\n",
       "     is_auroc_win  is_stat_sig  \n",
       "237         False        False  \n",
       "225         False        False  \n",
       "232         False        False  \n",
       "230         False        False  \n",
       "236         False        False  \n",
       "..            ...          ...  \n",
       "221          True         True  \n",
       "220         False        False  \n",
       "210         False        False  \n",
       "213         False        False  \n",
       "215          True         True  \n",
       "\n",
       "[238 rows x 14 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate task-level delta CIs\n",
    "baseline: str = 'clmbr'\n",
    "\n",
    "df_task_deltas = []\n",
    "for model in valid_models:\n",
    "    for task in valid_tasks:\n",
    "        aurocs = baseline_auroc_deltas[baseline][(model, task)]\n",
    "        wins = np.sign(baseline_auroc_deltas[baseline][(model, task)])\n",
    "        df_task_deltas.append({\n",
    "            'model' : model,\n",
    "            'task' : task,\n",
    "            'auroc_delta_mean' : np.mean(aurocs),\n",
    "            'auroc_delta_ci_025' : np.percentile(aurocs, 2.5),\n",
    "            'auroc_delta_ci_500' : np.percentile(aurocs, 50),\n",
    "            'auroc_delta_ci_975' : np.percentile(aurocs, 97.5),\n",
    "        })\n",
    "df_task_deltas = pd.DataFrame(df_task_deltas).sort_values(['model', 'task'])\n",
    "df_task_deltas = clean_df_deltas(df_task_deltas)\n",
    "df_task_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Context Length</th>\n",
       "      <th>Task</th>\n",
       "      <th>$\\Delta$ over baseline</th>\n",
       "      <th>95% CI</th>\n",
       "      <th>Statistically Significant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>0.007</td>\n",
       "      <td>(-0.028, 0.040)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>guo_los</td>\n",
       "      <td>0.013</td>\n",
       "      <td>(-0.005, 0.029)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>guo_readmission</td>\n",
       "      <td>0.005</td>\n",
       "      <td>(-0.008, 0.017)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>lab_anemia</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(0.001, 0.003)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>lab_hyperkalemia</td>\n",
       "      <td>0.030</td>\n",
       "      <td>(0.019, 0.042)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>lab_hypoglycemia</td>\n",
       "      <td>0.006</td>\n",
       "      <td>(-0.006, 0.019)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>lab_hyponatremia</td>\n",
       "      <td>0.070</td>\n",
       "      <td>(0.061, 0.079)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>lab_thrombocytopenia</td>\n",
       "      <td>0.008</td>\n",
       "      <td>(0.004, 0.013)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_acutemi</td>\n",
       "      <td>0.016</td>\n",
       "      <td>(-0.005, 0.036)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_celiac</td>\n",
       "      <td>0.194</td>\n",
       "      <td>(0.108, 0.333)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_hyperlipidemia</td>\n",
       "      <td>0.023</td>\n",
       "      <td>(-0.013, 0.058)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>0.003</td>\n",
       "      <td>(-0.018, 0.023)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_lupus</td>\n",
       "      <td>0.037</td>\n",
       "      <td>(-0.056, 0.132)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>new_pancan</td>\n",
       "      <td>0.053</td>\n",
       "      <td>(0.024, 0.087)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Context Length                  Task $\\Delta$ over baseline  \\\n",
       "223  mamba           16384               guo_icu                  0.007   \n",
       "211  mamba           16384               guo_los                  0.013   \n",
       "218  mamba           16384       guo_readmission                  0.005   \n",
       "216  mamba           16384            lab_anemia                  0.002   \n",
       "222  mamba           16384      lab_hyperkalemia                  0.030   \n",
       "212  mamba           16384      lab_hypoglycemia                  0.006   \n",
       "214  mamba           16384      lab_hyponatremia                  0.070   \n",
       "219  mamba           16384  lab_thrombocytopenia                  0.008   \n",
       "217  mamba           16384           new_acutemi                  0.016   \n",
       "221  mamba           16384            new_celiac                  0.194   \n",
       "220  mamba           16384    new_hyperlipidemia                  0.023   \n",
       "210  mamba           16384      new_hypertension                  0.003   \n",
       "213  mamba           16384             new_lupus                  0.037   \n",
       "215  mamba           16384            new_pancan                  0.053   \n",
       "\n",
       "              95% CI  Statistically Significant  \n",
       "223  (-0.028, 0.040)                      False  \n",
       "211  (-0.005, 0.029)                      False  \n",
       "218  (-0.008, 0.017)                      False  \n",
       "216   (0.001, 0.003)                       True  \n",
       "222   (0.019, 0.042)                       True  \n",
       "212  (-0.006, 0.019)                      False  \n",
       "214   (0.061, 0.079)                       True  \n",
       "219   (0.004, 0.013)                       True  \n",
       "217  (-0.005, 0.036)                      False  \n",
       "221   (0.108, 0.333)                       True  \n",
       "220  (-0.013, 0.058)                      False  \n",
       "210  (-0.018, 0.023)                      False  \n",
       "213  (-0.056, 0.132)                      False  \n",
       "215   (0.024, 0.087)                       True  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_df_deltas(df_task_deltas, model_start='mamba-tiny-16384')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaTeX Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrllr}\n",
      "\\toprule\n",
      "Model & Context Length & win_mean & win_ci_025 & win_ci_500 & win_ci_975 & $\\Delta$ over baseline & 95% CI & Statistically Significant \\\\\n",
      "\\midrule\n",
      "clmbr & 0 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000 & (0.000, 0.000) &  \\\\\n",
      "gpt2 & 512 & 0.196143 & -0.142857 & 0.142857 & 0.571429 & 0.017 & (0.005, 0.028) & \\checkmark \\\\\n",
      "gpt2 & 1024 & 0.005714 & -0.285714 & 0.000000 & 0.285714 & 0.008 & (-0.003, 0.021) &  \\\\\n",
      "gpt2 & 2048 & 0.216500 & -0.142857 & 0.285714 & 0.571429 & 0.025 & (0.008, 0.041) & \\checkmark \\\\\n",
      "gpt2 & 4096 & 0.423929 & 0.000000 & 0.428571 & 0.714286 & 0.019 & (0.007, 0.032) & \\checkmark \\\\\n",
      "hyena & 1024 & 0.131500 & -0.142857 & 0.142857 & 0.428571 & 0.019 & (0.007, 0.033) & \\checkmark \\\\\n",
      "hyena & 4096 & 0.112429 & -0.142857 & 0.142857 & 0.428571 & 0.021 & (0.007, 0.036) & \\checkmark \\\\\n",
      "hyena & 8192 & -0.266714 & -0.428571 & -0.285714 & 0.000000 & -0.008 & (-0.024, 0.010) &  \\\\\n",
      "hyena & 16384 & -0.571929 & -0.714286 & -0.571429 & -0.428571 & -0.043 & (-0.060, -0.025) & \\checkmark \\\\\n",
      "llama & 512 & -0.054214 & -0.428571 & 0.000000 & 0.285714 & 0.014 & (0.001, 0.027) & \\checkmark \\\\\n",
      "llama & 1024 & -0.142286 & -0.428571 & -0.142857 & 0.285714 & 0.015 & (0.003, 0.027) & \\checkmark \\\\\n",
      "llama & 2048 & 0.519214 & 0.142857 & 0.571429 & 0.857143 & 0.025 & (0.013, 0.038) & \\checkmark \\\\\n",
      "llama & 4096 & 0.464571 & 0.142857 & 0.428571 & 0.714286 & 0.022 & (0.010, 0.034) & \\checkmark \\\\\n",
      "mamba & 1024 & 0.147571 & -0.285714 & 0.142857 & 0.571429 & 0.009 & (-0.005, 0.023) &  \\\\\n",
      "mamba & 4096 & 0.570214 & 0.283929 & 0.571429 & 0.857143 & 0.027 & (0.016, 0.038) & \\checkmark \\\\\n",
      "mamba & 8192 & 0.555143 & 0.285714 & 0.571429 & 0.857143 & 0.025 & (0.015, 0.037) & \\checkmark \\\\\n",
      "mamba & 16384 & 0.778357 & 0.428571 & 0.857143 & 1.000000 & 0.033 & (0.022, 0.046) & \\checkmark \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model-level delta CIs for LaTeX\n",
    "latex: str = format_df_deltas_for_latex(df_model_deltas)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrlllr}\n",
      "\\toprule\n",
      "Model & Context Length & Task & $\\Delta$ over baseline & 95% CI & Statistically Significant \\\\\n",
      "\\midrule\n",
      "clmbr & 0 & ICU Admission & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Long LOS & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & 30-day Readmission & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Anemia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Hyperkalemia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Hypoglycemia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Hyponatremia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Thrombocytopenia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Acute MI & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Celiac & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Hyperlipidemia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Hypertension & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Lupus & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Pancreatic Cancer & 0.000 & (0.000, 0.000) &  \\\\\n",
      "gpt2 & 512 & ICU Admission & 0.022 & (-0.005, 0.050) &  \\\\\n",
      "gpt2 & 512 & Long LOS & -0.002 & (-0.017, 0.012) &  \\\\\n",
      "gpt2 & 512 & 30-day Readmission & -0.002 & (-0.013, 0.009) &  \\\\\n",
      "gpt2 & 512 & Anemia & -0.003 & (-0.004, -0.002) & \\checkmark \\\\\n",
      "gpt2 & 512 & Hyperkalemia & 0.011 & (0.001, 0.021) & \\checkmark \\\\\n",
      "gpt2 & 512 & Hypoglycemia & -0.001 & (-0.014, 0.012) &  \\\\\n",
      "gpt2 & 512 & Hyponatremia & 0.037 & (0.028, 0.046) & \\checkmark \\\\\n",
      "gpt2 & 512 & Thrombocytopenia & 0.020 & (0.015, 0.025) & \\checkmark \\\\\n",
      "gpt2 & 512 & Acute MI & 0.001 & (-0.022, 0.027) &  \\\\\n",
      "gpt2 & 512 & Celiac & 0.181 & (0.063, 0.295) & \\checkmark \\\\\n",
      "gpt2 & 512 & Hyperlipidemia & -0.004 & (-0.047, 0.043) &  \\\\\n",
      "gpt2 & 512 & Hypertension & -0.003 & (-0.021, 0.014) &  \\\\\n",
      "gpt2 & 512 & Lupus & -0.031 & (-0.110, 0.050) &  \\\\\n",
      "gpt2 & 512 & Pancreatic Cancer & 0.014 & (-0.028, 0.054) &  \\\\\n",
      "gpt2 & 1024 & ICU Admission & -0.021 & (-0.052, 0.009) &  \\\\\n",
      "gpt2 & 1024 & Long LOS & -0.014 & (-0.032, 0.004) &  \\\\\n",
      "gpt2 & 1024 & 30-day Readmission & 0.004 & (-0.009, 0.015) &  \\\\\n",
      "gpt2 & 1024 & Anemia & -0.011 & (-0.012, -0.009) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Hyperkalemia & 0.022 & (0.011, 0.033) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Hypoglycemia & -0.009 & (-0.022, 0.004) &  \\\\\n",
      "gpt2 & 1024 & Hyponatremia & 0.037 & (0.028, 0.046) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Thrombocytopenia & 0.013 & (0.009, 0.019) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Acute MI & -0.003 & (-0.027, 0.021) &  \\\\\n",
      "gpt2 & 1024 & Celiac & 0.125 & (0.007, 0.274) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Hyperlipidemia & -0.008 & (-0.053, 0.036) &  \\\\\n",
      "gpt2 & 1024 & Hypertension & -0.026 & (-0.049, -0.005) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Lupus & -0.016 & (-0.090, 0.062) &  \\\\\n",
      "gpt2 & 1024 & Pancreatic Cancer & 0.022 & (-0.009, 0.050) &  \\\\\n",
      "gpt2 & 2048 & ICU Admission & -0.010 & (-0.040, 0.021) &  \\\\\n",
      "gpt2 & 2048 & Long LOS & -0.008 & (-0.022, 0.006) &  \\\\\n",
      "gpt2 & 2048 & 30-day Readmission & 0.002 & (-0.011, 0.014) &  \\\\\n",
      "gpt2 & 2048 & Anemia & -0.004 & (-0.005, -0.003) & \\checkmark \\\\\n",
      "gpt2 & 2048 & Hyperkalemia & 0.007 & (-0.003, 0.017) &  \\\\\n",
      "gpt2 & 2048 & Hypoglycemia & 0.001 & (-0.013, 0.013) &  \\\\\n",
      "gpt2 & 2048 & Hyponatremia & 0.023 & (0.015, 0.029) & \\checkmark \\\\\n",
      "gpt2 & 2048 & Thrombocytopenia & 0.021 & (0.016, 0.027) & \\checkmark \\\\\n",
      "gpt2 & 2048 & Acute MI & -0.003 & (-0.030, 0.024) &  \\\\\n",
      "gpt2 & 2048 & Celiac & 0.227 & (0.037, 0.433) & \\checkmark \\\\\n",
      "gpt2 & 2048 & Hyperlipidemia & 0.005 & (-0.014, 0.025) &  \\\\\n",
      "gpt2 & 2048 & Hypertension & -0.002 & (-0.021, 0.017) &  \\\\\n",
      "gpt2 & 2048 & Lupus & 0.085 & (0.005, 0.165) & \\checkmark \\\\\n",
      "gpt2 & 2048 & Pancreatic Cancer & 0.004 & (-0.032, 0.037) &  \\\\\n",
      "gpt2 & 4096 & ICU Admission & 0.011 & (-0.021, 0.044) &  \\\\\n",
      "gpt2 & 4096 & Long LOS & -0.001 & (-0.014, 0.014) &  \\\\\n",
      "gpt2 & 4096 & 30-day Readmission & 0.004 & (-0.009, 0.015) &  \\\\\n",
      "gpt2 & 4096 & Anemia & -0.005 & (-0.006, -0.004) & \\checkmark \\\\\n",
      "gpt2 & 4096 & Hyperkalemia & 0.011 & (0.001, 0.021) & \\checkmark \\\\\n",
      "gpt2 & 4096 & Hypoglycemia & 0.003 & (-0.011, 0.015) &  \\\\\n",
      "gpt2 & 4096 & Hyponatremia & 0.046 & (0.036, 0.055) & \\checkmark \\\\\n",
      "gpt2 & 4096 & Thrombocytopenia & 0.014 & (0.009, 0.018) & \\checkmark \\\\\n",
      "gpt2 & 4096 & Acute MI & 0.006 & (-0.022, 0.033) &  \\\\\n",
      "gpt2 & 4096 & Celiac & 0.149 & (0.041, 0.278) & \\checkmark \\\\\n",
      "gpt2 & 4096 & Hyperlipidemia & 0.012 & (-0.018, 0.043) &  \\\\\n",
      "gpt2 & 4096 & Hypertension & 0.004 & (-0.015, 0.024) &  \\\\\n",
      "gpt2 & 4096 & Lupus & -0.008 & (-0.095, 0.088) &  \\\\\n",
      "gpt2 & 4096 & Pancreatic Cancer & 0.027 & (-0.008, 0.062) &  \\\\\n",
      "hyena & 1024 & ICU Admission & -0.026 & (-0.064, 0.013) &  \\\\\n",
      "hyena & 1024 & Long LOS & -0.006 & (-0.020, 0.011) &  \\\\\n",
      "hyena & 1024 & 30-day Readmission & -0.001 & (-0.012, 0.010) &  \\\\\n",
      "hyena & 1024 & Anemia & -0.002 & (-0.003, -0.001) & \\checkmark \\\\\n",
      "hyena & 1024 & Hyperkalemia & 0.026 & (0.015, 0.036) & \\checkmark \\\\\n",
      "hyena & 1024 & Hypoglycemia & -0.004 & (-0.015, 0.008) &  \\\\\n",
      "hyena & 1024 & Hyponatremia & 0.045 & (0.036, 0.055) & \\checkmark \\\\\n",
      "hyena & 1024 & Thrombocytopenia & 0.019 & (0.014, 0.024) & \\checkmark \\\\\n",
      "hyena & 1024 & Acute MI & 0.011 & (-0.015, 0.038) &  \\\\\n",
      "hyena & 1024 & Celiac & 0.224 & (0.095, 0.367) & \\checkmark \\\\\n",
      "hyena & 1024 & Hyperlipidemia & 0.018 & (-0.000, 0.037) &  \\\\\n",
      "hyena & 1024 & Hypertension & -0.026 & (-0.053, -0.003) & \\checkmark \\\\\n",
      "hyena & 1024 & Lupus & -0.026 & (-0.116, 0.055) &  \\\\\n",
      "hyena & 1024 & Pancreatic Cancer & 0.019 & (-0.022, 0.060) &  \\\\\n",
      "hyena & 4096 & ICU Admission & -0.026 & (-0.058, 0.004) &  \\\\\n",
      "hyena & 4096 & Long LOS & -0.012 & (-0.030, 0.006) &  \\\\\n",
      "hyena & 4096 & 30-day Readmission & 0.002 & (-0.012, 0.013) &  \\\\\n",
      "hyena & 4096 & Anemia & -0.005 & (-0.006, -0.004) & \\checkmark \\\\\n",
      "hyena & 4096 & Hyperkalemia & 0.022 & (0.013, 0.033) & \\checkmark \\\\\n",
      "hyena & 4096 & Hypoglycemia & -0.013 & (-0.027, 0.001) &  \\\\\n",
      "hyena & 4096 & Hyponatremia & 0.066 & (0.056, 0.078) & \\checkmark \\\\\n",
      "hyena & 4096 & Thrombocytopenia & 0.018 & (0.013, 0.023) & \\checkmark \\\\\n",
      "hyena & 4096 & Acute MI & 0.013 & (-0.013, 0.040) &  \\\\\n",
      "hyena & 4096 & Celiac & 0.216 & (0.077, 0.370) & \\checkmark \\\\\n",
      "hyena & 4096 & Hyperlipidemia & 0.023 & (-0.012, 0.057) &  \\\\\n",
      "hyena & 4096 & Hypertension & -0.023 & (-0.050, 0.002) &  \\\\\n",
      "hyena & 4096 & Lupus & -0.019 & (-0.110, 0.056) &  \\\\\n",
      "hyena & 4096 & Pancreatic Cancer & 0.038 & (-0.011, 0.092) &  \\\\\n",
      "hyena & 8192 & ICU Admission & -0.069 & (-0.106, -0.032) & \\checkmark \\\\\n",
      "hyena & 8192 & Long LOS & -0.023 & (-0.041, -0.004) & \\checkmark \\\\\n",
      "hyena & 8192 & 30-day Readmission & -0.017 & (-0.033, -0.002) & \\checkmark \\\\\n",
      "hyena & 8192 & Anemia & -0.016 & (-0.018, -0.014) & \\checkmark \\\\\n",
      "hyena & 8192 & Hyperkalemia & 0.010 & (0.000, 0.022) & \\checkmark \\\\\n",
      "hyena & 8192 & Hypoglycemia & -0.041 & (-0.056, -0.025) & \\checkmark \\\\\n",
      "hyena & 8192 & Hyponatremia & 0.049 & (0.039, 0.059) & \\checkmark \\\\\n",
      "hyena & 8192 & Thrombocytopenia & 0.005 & (-0.001, 0.010) &  \\\\\n",
      "hyena & 8192 & Acute MI & -0.009 & (-0.038, 0.022) &  \\\\\n",
      "hyena & 8192 & Celiac & 0.154 & (-0.013, 0.352) &  \\\\\n",
      "hyena & 8192 & Hyperlipidemia & 0.014 & (-0.026, 0.052) &  \\\\\n",
      "hyena & 8192 & Hypertension & -0.066 & (-0.108, -0.030) & \\checkmark \\\\\n",
      "hyena & 8192 & Lupus & -0.073 & (-0.189, 0.025) &  \\\\\n",
      "hyena & 8192 & Pancreatic Cancer & -0.033 & (-0.088, 0.018) &  \\\\\n",
      "hyena & 16384 & ICU Admission & -0.110 & (-0.147, -0.075) & \\checkmark \\\\\n",
      "hyena & 16384 & Long LOS & -0.048 & (-0.068, -0.029) & \\checkmark \\\\\n",
      "hyena & 16384 & 30-day Readmission & -0.048 & (-0.067, -0.026) & \\checkmark \\\\\n",
      "hyena & 16384 & Anemia & -0.047 & (-0.051, -0.043) & \\checkmark \\\\\n",
      "hyena & 16384 & Hyperkalemia & -0.038 & (-0.054, -0.023) & \\checkmark \\\\\n",
      "hyena & 16384 & Hypoglycemia & -0.093 & (-0.109, -0.075) & \\checkmark \\\\\n",
      "hyena & 16384 & Hyponatremia & 0.010 & (-0.002, 0.021) &  \\\\\n",
      "hyena & 16384 & Thrombocytopenia & 0.003 & (-0.005, 0.011) &  \\\\\n",
      "hyena & 16384 & Acute MI & -0.100 & (-0.145, -0.053) & \\checkmark \\\\\n",
      "hyena & 16384 & Celiac & 0.176 & (0.029, 0.318) & \\checkmark \\\\\n",
      "hyena & 16384 & Hyperlipidemia & -0.016 & (-0.069, 0.034) &  \\\\\n",
      "hyena & 16384 & Hypertension & -0.071 & (-0.125, -0.023) & \\checkmark \\\\\n",
      "hyena & 16384 & Lupus & -0.145 & (-0.268, -0.017) & \\checkmark \\\\\n",
      "hyena & 16384 & Pancreatic Cancer & -0.073 & (-0.148, 0.006) &  \\\\\n",
      "llama & 512 & ICU Admission & -0.018 & (-0.052, 0.015) &  \\\\\n",
      "llama & 512 & Long LOS & 0.002 & (-0.014, 0.017) &  \\\\\n",
      "llama & 512 & 30-day Readmission & 0.012 & (0.000, 0.024) & \\checkmark \\\\\n",
      "llama & 512 & Anemia & -0.004 & (-0.005, -0.003) & \\checkmark \\\\\n",
      "llama & 512 & Hyperkalemia & 0.012 & (0.004, 0.020) & \\checkmark \\\\\n",
      "llama & 512 & Hypoglycemia & -0.011 & (-0.022, 0.001) &  \\\\\n",
      "llama & 512 & Hyponatremia & -0.010 & (-0.016, -0.004) & \\checkmark \\\\\n",
      "llama & 512 & Thrombocytopenia & -0.001 & (-0.006, 0.004) &  \\\\\n",
      "llama & 512 & Acute MI & 0.015 & (-0.006, 0.037) &  \\\\\n",
      "llama & 512 & Celiac & 0.227 & (0.111, 0.356) & \\checkmark \\\\\n",
      "llama & 512 & Hyperlipidemia & 0.001 & (-0.018, 0.020) &  \\\\\n",
      "llama & 512 & Hypertension & -0.035 & (-0.057, -0.012) & \\checkmark \\\\\n",
      "llama & 512 & Lupus & 0.005 & (-0.084, 0.095) &  \\\\\n",
      "llama & 512 & Pancreatic Cancer & 0.001 & (-0.044, 0.046) &  \\\\\n",
      "llama & 1024 & ICU Admission & -0.005 & (-0.042, 0.032) &  \\\\\n",
      "llama & 1024 & Long LOS & -0.013 & (-0.034, 0.005) &  \\\\\n",
      "llama & 1024 & 30-day Readmission & 0.010 & (-0.002, 0.024) &  \\\\\n",
      "llama & 1024 & Anemia & -0.004 & (-0.005, -0.003) & \\checkmark \\\\\n",
      "llama & 1024 & Hyperkalemia & 0.010 & (0.002, 0.019) & \\checkmark \\\\\n",
      "llama & 1024 & Hypoglycemia & -0.003 & (-0.014, 0.008) &  \\\\\n",
      "llama & 1024 & Hyponatremia & -0.004 & (-0.010, 0.001) &  \\\\\n",
      "llama & 1024 & Thrombocytopenia & -0.005 & (-0.009, -0.000) & \\checkmark \\\\\n",
      "llama & 1024 & Acute MI & 0.007 & (-0.014, 0.029) &  \\\\\n",
      "llama & 1024 & Celiac & 0.250 & (0.149, 0.359) & \\checkmark \\\\\n",
      "llama & 1024 & Hyperlipidemia & 0.003 & (-0.016, 0.021) &  \\\\\n",
      "llama & 1024 & Hypertension & -0.014 & (-0.033, 0.003) &  \\\\\n",
      "llama & 1024 & Lupus & -0.014 & (-0.102, 0.079) &  \\\\\n",
      "llama & 1024 & Pancreatic Cancer & -0.007 & (-0.053, 0.037) &  \\\\\n",
      "llama & 2048 & ICU Admission & 0.005 & (-0.023, 0.033) &  \\\\\n",
      "llama & 2048 & Long LOS & 0.014 & (-0.003, 0.029) &  \\\\\n",
      "llama & 2048 & 30-day Readmission & 0.010 & (-0.003, 0.023) &  \\\\\n",
      "llama & 2048 & Anemia & -0.002 & (-0.003, -0.001) & \\checkmark \\\\\n",
      "llama & 2048 & Hyperkalemia & 0.015 & (0.005, 0.025) & \\checkmark \\\\\n",
      "llama & 2048 & Hypoglycemia & 0.011 & (-0.002, 0.023) &  \\\\\n",
      "llama & 2048 & Hyponatremia & 0.013 & (0.005, 0.020) & \\checkmark \\\\\n",
      "llama & 2048 & Thrombocytopenia & -0.000 & (-0.006, 0.004) &  \\\\\n",
      "llama & 2048 & Acute MI & 0.022 & (-0.001, 0.044) &  \\\\\n",
      "llama & 2048 & Celiac & 0.212 & (0.083, 0.343) & \\checkmark \\\\\n",
      "llama & 2048 & Hyperlipidemia & 0.021 & (-0.005, 0.049) &  \\\\\n",
      "llama & 2048 & Hypertension & -0.003 & (-0.025, 0.018) &  \\\\\n",
      "llama & 2048 & Lupus & 0.031 & (-0.049, 0.119) &  \\\\\n",
      "llama & 2048 & Pancreatic Cancer & 0.007 & (-0.042, 0.053) &  \\\\\n",
      "llama & 4096 & ICU Admission & -0.003 & (-0.026, 0.021) &  \\\\\n",
      "llama & 4096 & Long LOS & -0.004 & (-0.018, 0.010) &  \\\\\n",
      "llama & 4096 & 30-day Readmission & 0.013 & (0.002, 0.026) & \\checkmark \\\\\n",
      "llama & 4096 & Anemia & 0.001 & (0.000, 0.002) & \\checkmark \\\\\n",
      "llama & 4096 & Hyperkalemia & 0.024 & (0.016, 0.033) & \\checkmark \\\\\n",
      "llama & 4096 & Hypoglycemia & 0.012 & (-0.000, 0.022) &  \\\\\n",
      "llama & 4096 & Hyponatremia & 0.036 & (0.028, 0.046) & \\checkmark \\\\\n",
      "llama & 4096 & Thrombocytopenia & 0.000 & (-0.004, 0.005) &  \\\\\n",
      "llama & 4096 & Acute MI & 0.015 & (-0.008, 0.038) &  \\\\\n",
      "llama & 4096 & Celiac & 0.226 & (0.097, 0.365) & \\checkmark \\\\\n",
      "llama & 4096 & Hyperlipidemia & 0.016 & (-0.002, 0.036) &  \\\\\n",
      "llama & 4096 & Hypertension & 0.004 & (-0.013, 0.021) &  \\\\\n",
      "llama & 4096 & Lupus & -0.023 & (-0.097, 0.049) &  \\\\\n",
      "llama & 4096 & Pancreatic Cancer & -0.008 & (-0.056, 0.033) &  \\\\\n",
      "mamba & 1024 & ICU Admission & -0.009 & (-0.039, 0.019) &  \\\\\n",
      "mamba & 1024 & Long LOS & -0.003 & (-0.018, 0.010) &  \\\\\n",
      "mamba & 1024 & 30-day Readmission & 0.001 & (-0.010, 0.013) &  \\\\\n",
      "mamba & 1024 & Anemia & 0.000 & (-0.001, 0.001) &  \\\\\n",
      "mamba & 1024 & Hyperkalemia & 0.003 & (-0.006, 0.013) &  \\\\\n",
      "mamba & 1024 & Hypoglycemia & 0.001 & (-0.011, 0.013) &  \\\\\n",
      "mamba & 1024 & Hyponatremia & 0.014 & (0.007, 0.022) & \\checkmark \\\\\n",
      "mamba & 1024 & Thrombocytopenia & -0.005 & (-0.010, -0.001) & \\checkmark \\\\\n",
      "mamba & 1024 & Acute MI & 0.017 & (-0.007, 0.040) &  \\\\\n",
      "mamba & 1024 & Celiac & 0.102 & (-0.076, 0.262) &  \\\\\n",
      "mamba & 1024 & Hyperlipidemia & 0.020 & (-0.010, 0.050) &  \\\\\n",
      "mamba & 1024 & Hypertension & -0.011 & (-0.034, 0.011) &  \\\\\n",
      "mamba & 1024 & Lupus & -0.030 & (-0.115, 0.052) &  \\\\\n",
      "mamba & 1024 & Pancreatic Cancer & 0.032 & (-0.008, 0.071) &  \\\\\n",
      "mamba & 4096 & ICU Admission & 0.004 & (-0.024, 0.029) &  \\\\\n",
      "mamba & 4096 & Long LOS & 0.005 & (-0.010, 0.021) &  \\\\\n",
      "mamba & 4096 & 30-day Readmission & 0.006 & (-0.006, 0.017) &  \\\\\n",
      "mamba & 4096 & Anemia & 0.002 & (0.001, 0.003) & \\checkmark \\\\\n",
      "mamba & 4096 & Hyperkalemia & 0.024 & (0.014, 0.034) & \\checkmark \\\\\n",
      "mamba & 4096 & Hypoglycemia & 0.001 & (-0.012, 0.013) &  \\\\\n",
      "mamba & 4096 & Hyponatremia & 0.066 & (0.057, 0.075) & \\checkmark \\\\\n",
      "mamba & 4096 & Thrombocytopenia & 0.007 & (0.002, 0.011) & \\checkmark \\\\\n",
      "mamba & 4096 & Acute MI & 0.014 & (-0.009, 0.036) &  \\\\\n",
      "mamba & 4096 & Celiac & 0.198 & (0.115, 0.288) & \\checkmark \\\\\n",
      "mamba & 4096 & Hyperlipidemia & 0.015 & (-0.034, 0.057) &  \\\\\n",
      "mamba & 4096 & Hypertension & -0.010 & (-0.033, 0.010) &  \\\\\n",
      "mamba & 4096 & Lupus & -0.003 & (-0.091, 0.086) &  \\\\\n",
      "mamba & 4096 & Pancreatic Cancer & 0.049 & (0.017, 0.081) & \\checkmark \\\\\n",
      "mamba & 8192 & ICU Admission & -0.007 & (-0.033, 0.018) &  \\\\\n",
      "mamba & 8192 & Long LOS & 0.009 & (-0.006, 0.024) &  \\\\\n",
      "mamba & 8192 & 30-day Readmission & 0.003 & (-0.010, 0.016) &  \\\\\n",
      "mamba & 8192 & Anemia & 0.001 & (0.000, 0.002) & \\checkmark \\\\\n",
      "mamba & 8192 & Hyperkalemia & 0.018 & (0.008, 0.029) & \\checkmark \\\\\n",
      "mamba & 8192 & Hypoglycemia & -0.002 & (-0.014, 0.010) &  \\\\\n",
      "mamba & 8192 & Hyponatremia & 0.063 & (0.053, 0.072) & \\checkmark \\\\\n",
      "mamba & 8192 & Thrombocytopenia & 0.004 & (-0.001, 0.008) &  \\\\\n",
      "mamba & 8192 & Acute MI & 0.014 & (-0.008, 0.036) &  \\\\\n",
      "mamba & 8192 & Celiac & 0.173 & (0.083, 0.312) & \\checkmark \\\\\n",
      "mamba & 8192 & Hyperlipidemia & 0.030 & (-0.011, 0.068) &  \\\\\n",
      "mamba & 8192 & Hypertension & -0.016 & (-0.036, 0.003) &  \\\\\n",
      "mamba & 8192 & Lupus & 0.038 & (-0.029, 0.113) &  \\\\\n",
      "mamba & 8192 & Pancreatic Cancer & 0.027 & (-0.010, 0.062) &  \\\\\n",
      "mamba & 16384 & ICU Admission & 0.007 & (-0.028, 0.040) &  \\\\\n",
      "mamba & 16384 & Long LOS & 0.013 & (-0.005, 0.029) &  \\\\\n",
      "mamba & 16384 & 30-day Readmission & 0.005 & (-0.008, 0.017) &  \\\\\n",
      "mamba & 16384 & Anemia & 0.002 & (0.001, 0.003) & \\checkmark \\\\\n",
      "mamba & 16384 & Hyperkalemia & 0.030 & (0.019, 0.042) & \\checkmark \\\\\n",
      "mamba & 16384 & Hypoglycemia & 0.006 & (-0.006, 0.019) &  \\\\\n",
      "mamba & 16384 & Hyponatremia & 0.070 & (0.061, 0.079) & \\checkmark \\\\\n",
      "mamba & 16384 & Thrombocytopenia & 0.008 & (0.004, 0.013) & \\checkmark \\\\\n",
      "mamba & 16384 & Acute MI & 0.016 & (-0.005, 0.036) &  \\\\\n",
      "mamba & 16384 & Celiac & 0.194 & (0.108, 0.333) & \\checkmark \\\\\n",
      "mamba & 16384 & Hyperlipidemia & 0.023 & (-0.013, 0.058) &  \\\\\n",
      "mamba & 16384 & Hypertension & 0.003 & (-0.018, 0.023) &  \\\\\n",
      "mamba & 16384 & Lupus & 0.037 & (-0.056, 0.132) &  \\\\\n",
      "mamba & 16384 & Pancreatic Cancer & 0.053 & (0.024, 0.087) & \\checkmark \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task-level delta CIs for LaTeX\n",
    "latex: str = format_df_deltas_for_latex(df_task_deltas)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Win Rates of Models\n",
    "\n",
    "Count how many times a model beats CLMBR / best baseline. No bootstrapping or CIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ctx_orig = pd.read_csv('../cache/results_nonchexpert.csv')\n",
    "df_chexpert_orig = pd.read_csv('../cache/results_chexpert.csv')\n",
    "\n",
    "def preprocess_df(df, is_keep_max_tokens=False, drop_tasks=[]):\n",
    "    assert df.drop_duplicates(subset=['model', 'task_name']).shape == df.shape, \"Duplicate rows found\"\n",
    "    df['Baseline_AUROC'] = df[['Logistic_AUROC', 'GBM_AUROC', 'Random_Forest_AUROC']].max(axis=1)\n",
    "    df['delta_clmbr'] = df['value_mean'] - df['CLMBR_AUROC']\n",
    "    df['delta_baseline'] = df['value_mean'] - df['Baseline_AUROC']\n",
    "    df['is_max_tokens'] = df['model'].apply(lambda x : x.endswith('max'))\n",
    "    df['model_name'] = df['model'].apply(lambda x : x.replace('-max', '').replace(\"-\" + x.replace('-max', '').split(\"-\")[-1], \"\").strip())\n",
    "    df['ctx_length'] = df['model'].apply(lambda x : int(x.replace('-max', '').split(\"-\")[-1]))\n",
    "    \n",
    "    \n",
    "    # Best GPT\n",
    "    for task in df['task_name'].unique():\n",
    "        best_ = df[(df['task_name'] == task) & (df['model'].str.contains('gpt2-base'))]['value_mean'].max()\n",
    "        df.loc[df['task_name'] == task, 'BestGPT2_AUROC'] = best_\n",
    "    for task in df['task_name'].unique():\n",
    "        best_ = df[(df['task_name'] == task) & (df['model'].str.contains('mamba-tiny'))]['value_mean'].max()\n",
    "        df.loc[df['task_name'] == task, 'BestMamba_AUROC'] = best_\n",
    "    for task in df['task_name'].unique():\n",
    "        best_ = df[(df['task_name'] == task) & (df['model'].str.contains('llama-base'))]['value_mean'].max()\n",
    "        df.loc[df['task_name'] == task, 'BestLlama_AUROC'] = best_\n",
    "    for task in df['task_name'].unique():\n",
    "        best_ = df[(df['task_name'] == task) & (df['model'].str.contains('hyena-large'))]['value_mean'].max()\n",
    "        df.loc[df['task_name'] == task, 'BestHyena_AUROC'] = best_\n",
    "    df['delta_bestGPT2'] = df['value_mean'] - df['BestGPT2_AUROC']\n",
    "    df['delta_bestMamba'] = df['value_mean'] - df['BestMamba_AUROC']\n",
    "    df['delta_bestLlama'] = df['value_mean'] - df['BestLlama_AUROC']\n",
    "    df['delta_bestHyena'] = df['value_mean'] - df['BestHyena_AUROC']\n",
    "    \n",
    "    # Count win rates v. CLMBR / Baseline / Models\n",
    "    df['is_beat_clmbr'] = df['delta_clmbr'] > 0\n",
    "    df['is_beat_baseline'] = df['delta_baseline'] > 0\n",
    "    df['is_beat_bestGPT2'] = df['delta_bestGPT2'] > 0\n",
    "    df['is_beat_bestMamba'] = df['delta_bestMamba'] > 0\n",
    "    df['is_beat_bestLlama'] = df['delta_bestLlama'] > 0\n",
    "    df['is_beat_bestHyena'] = df['delta_bestHyena'] > 0\n",
    "    \n",
    "    # Filtering\n",
    "    ## Only keep 2B models\n",
    "    if not is_keep_max_tokens:\n",
    "        df = df[~df['is_max_tokens']]\n",
    "    ## Drop tasks (usually Lupus and Celiac b/c so low n)\n",
    "    if len(drop_tasks) > 0:\n",
    "        df = df[~df['task_name'].isin(drop_tasks)]\n",
    "    \n",
    "    # Sanity check baseline / CLMBR numbers\n",
    "    for task in df['task_name'].unique():\n",
    "        df_ = df[df['task_name'] == task]\n",
    "        assert all(x == df_['Logistic_AUROC'].tolist()[0] for x in df_['Logistic_AUROC'].tolist()), \"Logistic_AUROC is not uniform\"\n",
    "        assert all(x == df_['GBM_AUROC'].tolist()[0] for x in df_['GBM_AUROC'].tolist()), \"GBM_AUROC is not uniform\"\n",
    "        assert all(x == df_['Random_Forest_AUROC'].tolist()[0] for x in df_['Random_Forest_AUROC'].tolist()), \"Random_Forest_AUROC is not uniform\"\n",
    "        assert all(x == df_['Baseline_AUROC'].tolist()[0] for x in df_['Baseline_AUROC'].tolist()), \"Baseline Model AUROC is not uniform\"\n",
    "        assert all(x == df_['CLMBR_AUROC'].tolist()[0] for x in df_['CLMBR_AUROC'].tolist()), \"CLMBR AUROC is not uniform\"\n",
    "    return df\n",
    "\n",
    "df_ctx = preprocess_df(df_ctx_orig, drop_tasks=['Lupus', 'Celiac'])\n",
    "df_chexpert = preprocess_df(df_chexpert_orig, drop_tasks=['Lupus', 'Celiac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Anemia', 'Hypoglycemia', 'Hyponatremia'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: We're missing gpt2-base-1024 for some tasks...\n",
    "set(df_ctx['task_name'].unique())- set(df_ctx[df_ctx['model'] == 'gpt2-base-1024']['task_name'].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+---------------+------------------+\n",
      "| model_name   |   ctx_length |   delta_clmbr |   delta_baseline |\n",
      "+==============+==============+===============+==================+\n",
      "| gpt2-base    |          512 |       -0.0016 |           0.0334 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| gpt2-base    |         1024 |        0.004  |           0.039  |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| gpt2-base    |         2048 |        0.0016 |           0.0366 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| gpt2-base    |         4096 |        0.0038 |           0.0388 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| hyena-large  |         1024 |       -0.0007 |           0.0343 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| hyena-large  |         4096 |        0.0018 |           0.0368 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| hyena-large  |         8192 |       -0.0163 |           0.0187 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| hyena-large  |        16384 |       -0.0476 |          -0.0126 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| llama-base   |          512 |        0.0074 |           0.0424 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| llama-base   |         1024 |        0.007  |           0.042  |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| llama-base   |         2048 |        0.0103 |           0.0453 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| llama-base   |         4096 |        0.0134 |           0.0484 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| mamba-tiny   |         1024 |        0.0013 |           0.0363 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| mamba-tiny   |         4096 |        0.0058 |           0.0408 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| mamba-tiny   |         8192 |        0.0032 |           0.0382 |\n",
      "+--------------+--------------+---------------+------------------+\n",
      "| mamba-tiny   |        16384 |        0.0047 |           0.0397 |\n",
      "+--------------+--------------+---------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "df_ctx['delta_clmbr']\n",
    "print(df_ctx[df_ctx['task_name'] == '30-Day Readmission'][['model_name', 'ctx_length', 'delta_clmbr', 'delta_baseline']].round({'delta_clmbr': 4, 'delta_baseline': 4}).sort_values(['model_name', 'ctx_length'], ascending=[True, True]).to_markdown(tablefmt=\"grid\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Chexpert\n",
    "os.makedirs('../cache/tables_ehrshot/chexpert/', exist_ok=True)\n",
    "for task in df_ctx['task_name'].unique():\n",
    "    df_task_ = df_ctx[df_ctx['task_name'] == task][['model_name', 'ctx_length', 'delta_clmbr', 'delta_baseline']].round({'delta_clmbr': 4, 'delta_baseline': 4})\n",
    "    df_delta_ordered = df_task_.sort_values(['delta_clmbr'], ascending=False)\n",
    "    df_model_ordered = df_task_.sort_values(['model_name', 'ctx_length'], ascending=[True, False])\n",
    "    with open(f'../cache/tables_ehrshot/{task}.md', 'w') as fd:\n",
    "        fd.write(f\"## {task}\\n\\n\")\n",
    "        fd.write(\"### Delta Ordered\\n\")\n",
    "        fd.write(df_delta_ordered.to_markdown(tablefmt=\"grid\", index=False))\n",
    "        fd.write(\"\\n\\n### Model Ordered\\n\")\n",
    "        fd.write(df_model_ordered.to_markdown(tablefmt=\"grid\", index=False))\n",
    "# Chexpert\n",
    "for task in df_chexpert['task_name'].unique():\n",
    "    df_task_ = df_chexpert[df_chexpert['task_name'] == task][['model_name', 'ctx_length', 'delta_clmbr', 'delta_baseline']].round({'delta_clmbr': 4, 'delta_baseline': 4})\n",
    "    df_delta_ordered = df_task_.sort_values(['delta_clmbr'], ascending=False)\n",
    "    df_model_ordered = df_task_.sort_values(['model_name', 'ctx_length'], ascending=[True, False])\n",
    "    with open(f'../cache/tables_ehrshot/chexpert/{task}.md', 'w') as fd:\n",
    "        fd.write(f\"## {task}\\n\\n\")\n",
    "        fd.write(\"### Delta Ordered\\n\")\n",
    "        fd.write(df_delta_ordered.to_markdown(tablefmt=\"grid\", index=False))\n",
    "        fd.write(\"\\n\\n### Model Ordered\\n\")\n",
    "        fd.write(df_model_ordered.to_markdown(tablefmt=\"grid\", index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Counts v. Baseline\n",
    "\n",
    "Count how many times a model beats CLMBR / best baseline. No bootstrapping or CIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_win_rates(df):\n",
    "    # Count # of times a model beats CLMBR\n",
    "    df_clmbr_beats = df.groupby('model')['is_beat_clmbr'].sum().reset_index().sort_values('is_beat_clmbr', ascending=False)\n",
    "    df_mean_clmbr_delta = df.groupby('model')['delta_clmbr'].mean().reset_index().sort_values('delta_clmbr', ascending=False)\n",
    "\n",
    "    # Count # of times a model beats best baseline (max(LR, RF, XGB))\n",
    "    df_baseline_beats = df.groupby('model')['is_beat_baseline'].sum().reset_index().sort_values('is_beat_baseline', ascending=False)\n",
    "    df_mean_baseline_delta = df.groupby('model')['delta_baseline'].mean().reset_index().sort_values('delta_baseline', ascending=False)\n",
    "\n",
    "    # Count # of times a model beats best GPT2-base\n",
    "    df_gpt2_beats = df.groupby('model')['is_beat_bestGPT2'].sum().reset_index().sort_values('is_beat_bestGPT2', ascending=False)\n",
    "    df_mean_gpt2_delta = df.groupby('model')['delta_bestGPT2'].mean().reset_index().sort_values('delta_bestGPT2', ascending=False)\n",
    "\n",
    "    # Count # of times a model beats best Llama-base\n",
    "    df_llama_beats = df.groupby('model')['is_beat_bestLlama'].sum().reset_index().sort_values('is_beat_bestLlama', ascending=False)\n",
    "    df_mean_llama_delta = df.groupby('model')['delta_bestLlama'].mean().reset_index().sort_values('delta_bestLlama', ascending=False)\n",
    "    return {\n",
    "        'clmbr_beats' : df_clmbr_beats,\n",
    "        'mean_clmbr_delta' : df_mean_clmbr_delta,\n",
    "        'baseline_beats' : df_baseline_beats,\n",
    "        'mean_baseline_delta' : df_mean_baseline_delta,\n",
    "        'gpt2_beats' : df_gpt2_beats,\n",
    "        'mean_gpt2_delta' : df_mean_gpt2_delta,\n",
    "        'llama_beats' : df_llama_beats,\n",
    "        'mean_llama_delta' : df_mean_llama_delta,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Chexpert tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ctx_win_rates = count_win_rates(df_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Win rate v. CLMBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tasks that model achieves higher AUROC than CLMBR (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>is_beat_clmbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  is_beat_clmbr\n",
       "13   mamba-tiny-16384             12\n",
       "14    mamba-tiny-4096             11\n",
       "2      gpt2-base-4096             10\n",
       "9     llama-base-2048              9\n",
       "15    mamba-tiny-8192              9\n",
       "10    llama-base-4096              8\n",
       "12    mamba-tiny-1024              8\n",
       "6    hyena-large-4096              7\n",
       "1      gpt2-base-2048              6\n",
       "3       gpt2-base-512              6\n",
       "4    hyena-large-1024              6\n",
       "0      gpt2-base-1024              4\n",
       "8     llama-base-1024              4\n",
       "7    hyena-large-8192              4\n",
       "11     llama-base-512              3\n",
       "5   hyena-large-16384              2"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# of tasks that model achieves higher AUROC than CLMBR (higher is better)\")\n",
    "df_ctx_win_rates['clmbr_beats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUROC diff between model and CLMBR (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>delta_clmbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>0.018845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>0.014324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>0.011398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>0.009947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>0.008483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>0.007948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>0.007604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>0.006575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>0.005338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>0.004107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>0.002011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>-0.002368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>-0.002938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>-0.011253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>-0.017298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>-0.053359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  delta_clmbr\n",
       "13   mamba-tiny-16384     0.018845\n",
       "14    mamba-tiny-4096     0.014324\n",
       "15    mamba-tiny-8192     0.011398\n",
       "2      gpt2-base-4096     0.009947\n",
       "9     llama-base-2048     0.008483\n",
       "10    llama-base-4096     0.007948\n",
       "6    hyena-large-4096     0.007604\n",
       "3       gpt2-base-512     0.006575\n",
       "4    hyena-large-1024     0.005338\n",
       "12    mamba-tiny-1024     0.004107\n",
       "1      gpt2-base-2048     0.002011\n",
       "0      gpt2-base-1024    -0.002368\n",
       "8     llama-base-1024    -0.002938\n",
       "11     llama-base-512    -0.011253\n",
       "7    hyena-large-8192    -0.017298\n",
       "5   hyena-large-16384    -0.053359"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean AUROC diff between model and CLMBR (higher is better)\")\n",
    "df_ctx_win_rates['mean_clmbr_delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Win rate v. GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>delta_bestGPT2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30-Day Readmission</td>\n",
       "      <td>0.000720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Acute MI</td>\n",
       "      <td>0.010173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Anemia</td>\n",
       "      <td>0.005698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Hyperkalemia</td>\n",
       "      <td>0.007539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Hyperlipidemia</td>\n",
       "      <td>0.010968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hypertension</td>\n",
       "      <td>-0.001608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Hypoglycemia</td>\n",
       "      <td>0.004088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Hyponatremia</td>\n",
       "      <td>0.024350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>ICU Prediction</td>\n",
       "      <td>-0.014959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Long LOS</td>\n",
       "      <td>0.013017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Pancreatic Cancer</td>\n",
       "      <td>0.027042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Thrombocytopenia</td>\n",
       "      <td>-0.013062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              task_name  delta_bestGPT2\n",
       "5    30-Day Readmission        0.000720\n",
       "19             Acute MI        0.010173\n",
       "32               Anemia        0.005698\n",
       "63         Hyperkalemia        0.007539\n",
       "81       Hyperlipidemia        0.010968\n",
       "97         Hypertension       -0.001608\n",
       "113        Hypoglycemia        0.004088\n",
       "126        Hyponatremia        0.024350\n",
       "143      ICU Prediction       -0.014959\n",
       "158            Long LOS        0.013017\n",
       "189   Pancreatic Cancer        0.027042\n",
       "211    Thrombocytopenia       -0.013062"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ctx[df_ctx['model'] == 'mamba-tiny-16384'][['task_name', 'delta_bestGPT2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tasks that model achieves higher AUROC than best GPT2-base (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>is_beat_bestGPT2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  is_beat_bestGPT2\n",
       "13   mamba-tiny-16384                 9\n",
       "14    mamba-tiny-4096                 8\n",
       "10    llama-base-4096                 6\n",
       "9     llama-base-2048                 6\n",
       "15    mamba-tiny-8192                 6\n",
       "6    hyena-large-4096                 4\n",
       "12    mamba-tiny-1024                 4\n",
       "4    hyena-large-1024                 4\n",
       "11     llama-base-512                 2\n",
       "7    hyena-large-8192                 2\n",
       "8     llama-base-1024                 2\n",
       "5   hyena-large-16384                 0\n",
       "0      gpt2-base-1024                 0\n",
       "1      gpt2-base-2048                 0\n",
       "2      gpt2-base-4096                 0\n",
       "3       gpt2-base-512                 0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# of tasks that model achieves higher AUROC than best GPT2-base (higher is better)\")\n",
    "df_ctx_win_rates['gpt2_beats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUROC diff between model and best GPT2 (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>delta_bestGPT2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>0.006164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>0.001642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>-0.001284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>-0.002734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>-0.004198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>-0.004733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>-0.005077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>-0.006106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>-0.007343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>-0.008574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>-0.010670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>-0.014289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>-0.015620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>-0.023935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>-0.029980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>-0.066041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  delta_bestGPT2\n",
       "13   mamba-tiny-16384        0.006164\n",
       "14    mamba-tiny-4096        0.001642\n",
       "15    mamba-tiny-8192       -0.001284\n",
       "2      gpt2-base-4096       -0.002734\n",
       "9     llama-base-2048       -0.004198\n",
       "10    llama-base-4096       -0.004733\n",
       "6    hyena-large-4096       -0.005077\n",
       "3       gpt2-base-512       -0.006106\n",
       "4    hyena-large-1024       -0.007343\n",
       "12    mamba-tiny-1024       -0.008574\n",
       "1      gpt2-base-2048       -0.010670\n",
       "0      gpt2-base-1024       -0.014289\n",
       "8     llama-base-1024       -0.015620\n",
       "11     llama-base-512       -0.023935\n",
       "7    hyena-large-8192       -0.029980\n",
       "5   hyena-large-16384       -0.066041"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean AUROC diff between model and best GPT2 (higher is better)\")\n",
    "df_ctx_win_rates['mean_gpt2_delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Win rate v. Best Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>delta_bestLlama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30-Day Readmission</td>\n",
       "      <td>-0.008647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Acute MI</td>\n",
       "      <td>-0.007046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Anemia</td>\n",
       "      <td>0.001165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Hyperkalemia</td>\n",
       "      <td>0.005238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Hyperlipidemia</td>\n",
       "      <td>0.001584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hypertension</td>\n",
       "      <td>-0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Hypoglycemia</td>\n",
       "      <td>-0.005177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Hyponatremia</td>\n",
       "      <td>0.033643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>ICU Prediction</td>\n",
       "      <td>0.001561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Long LOS</td>\n",
       "      <td>-0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Pancreatic Cancer</td>\n",
       "      <td>0.047193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Thrombocytopenia</td>\n",
       "      <td>0.008391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              task_name  delta_bestLlama\n",
       "5    30-Day Readmission        -0.008647\n",
       "19             Acute MI        -0.007046\n",
       "32               Anemia         0.001165\n",
       "63         Hyperkalemia         0.005238\n",
       "81       Hyperlipidemia         0.001584\n",
       "97         Hypertension        -0.000847\n",
       "113        Hypoglycemia        -0.005177\n",
       "126        Hyponatremia         0.033643\n",
       "143      ICU Prediction         0.001561\n",
       "158            Long LOS        -0.001396\n",
       "189   Pancreatic Cancer         0.047193\n",
       "211    Thrombocytopenia         0.008391"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ctx[df_ctx['model'] == 'mamba-tiny-16384'][['task_name', 'delta_bestLlama']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tasks that model achieves higher AUROC than best Llama-base (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>is_beat_bestLlama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  is_beat_bestLlama\n",
       "13   mamba-tiny-16384                  7\n",
       "2      gpt2-base-4096                  5\n",
       "6    hyena-large-4096                  4\n",
       "3       gpt2-base-512                  4\n",
       "14    mamba-tiny-4096                  4\n",
       "15    mamba-tiny-8192                  4\n",
       "4    hyena-large-1024                  4\n",
       "7    hyena-large-8192                  2\n",
       "0      gpt2-base-1024                  2\n",
       "1      gpt2-base-2048                  1\n",
       "5   hyena-large-16384                  1\n",
       "12    mamba-tiny-1024                  1\n",
       "11     llama-base-512                  0\n",
       "10    llama-base-4096                  0\n",
       "9     llama-base-2048                  0\n",
       "8     llama-base-1024                  0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# of tasks that model achieves higher AUROC than best Llama-base (higher is better)\")\n",
    "df_ctx_win_rates['llama_beats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUROC diff between model and best Llama (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>delta_bestLlama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>0.006305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>0.001784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>-0.001142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>-0.002593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>-0.004056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>-0.004592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>-0.004936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>-0.005965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>-0.007202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>-0.008433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>-0.010528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>-0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>-0.015478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>-0.023793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>-0.029838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>-0.065899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  delta_bestLlama\n",
       "13   mamba-tiny-16384         0.006305\n",
       "14    mamba-tiny-4096         0.001784\n",
       "15    mamba-tiny-8192        -0.001142\n",
       "2      gpt2-base-4096        -0.002593\n",
       "9     llama-base-2048        -0.004056\n",
       "10    llama-base-4096        -0.004592\n",
       "6    hyena-large-4096        -0.004936\n",
       "3       gpt2-base-512        -0.005965\n",
       "4    hyena-large-1024        -0.007202\n",
       "12    mamba-tiny-1024        -0.008433\n",
       "1      gpt2-base-2048        -0.010528\n",
       "0      gpt2-base-1024        -0.013600\n",
       "8     llama-base-1024        -0.015478\n",
       "11     llama-base-512        -0.023793\n",
       "7    hyena-large-8192        -0.029838\n",
       "5   hyena-large-16384        -0.065899"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean AUROC diff between model and best Llama (higher is better)\")\n",
    "df_ctx_win_rates['mean_llama_delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Win rate v. Best Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tasks that model achieves higher AUROC than best baseline (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>is_beat_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  is_beat_baseline\n",
       "6    hyena-large-4096                10\n",
       "15    mamba-tiny-8192                10\n",
       "13   mamba-tiny-16384                10\n",
       "14    mamba-tiny-4096                10\n",
       "12    mamba-tiny-1024                10\n",
       "10    llama-base-4096                10\n",
       "9     llama-base-2048                10\n",
       "8     llama-base-1024                10\n",
       "2      gpt2-base-4096                 9\n",
       "3       gpt2-base-512                 9\n",
       "1      gpt2-base-2048                 9\n",
       "4    hyena-large-1024                 9\n",
       "11     llama-base-512                 9\n",
       "7    hyena-large-8192                 7\n",
       "0      gpt2-base-1024                 6\n",
       "5   hyena-large-16384                 5"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# of tasks that model achieves higher AUROC than best baseline (higher is better)\")\n",
    "df_ctx_win_rates['baseline_beats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUROC diff between model and best baseline (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>delta_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>0.063262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>0.058740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>0.055814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>0.054364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>0.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>0.052365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>0.052021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>0.050992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>0.049755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>0.048524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>0.046428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>0.041478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>0.033163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>0.027118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>0.010632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>-0.008943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  delta_baseline\n",
       "13   mamba-tiny-16384        0.063262\n",
       "14    mamba-tiny-4096        0.058740\n",
       "15    mamba-tiny-8192        0.055814\n",
       "2      gpt2-base-4096        0.054364\n",
       "9     llama-base-2048        0.052900\n",
       "10    llama-base-4096        0.052365\n",
       "6    hyena-large-4096        0.052021\n",
       "3       gpt2-base-512        0.050992\n",
       "4    hyena-large-1024        0.049755\n",
       "12    mamba-tiny-1024        0.048524\n",
       "1      gpt2-base-2048        0.046428\n",
       "8     llama-base-1024        0.041478\n",
       "11     llama-base-512        0.033163\n",
       "7    hyena-large-8192        0.027118\n",
       "0      gpt2-base-1024        0.010632\n",
       "5   hyena-large-16384       -0.008943"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean AUROC diff between model and best baseline (higher is better)\")\n",
    "df_ctx_win_rates['mean_baseline_delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model win rate v. best other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>model</th>\n",
       "      <th>value_mean</th>\n",
       "      <th>lower_bound</th>\n",
       "      <th>upper_bound</th>\n",
       "      <th>Logistic_AUROC</th>\n",
       "      <th>GBM_AUROC</th>\n",
       "      <th>Random_Forest_AUROC</th>\n",
       "      <th>CLMBR_AUROC</th>\n",
       "      <th>Baseline_AUROC</th>\n",
       "      <th>...</th>\n",
       "      <th>delta_bestGPT2</th>\n",
       "      <th>delta_bestMamba</th>\n",
       "      <th>delta_bestLlama</th>\n",
       "      <th>delta_bestHyena</th>\n",
       "      <th>is_beat_clmbr</th>\n",
       "      <th>is_beat_baseline</th>\n",
       "      <th>is_beat_bestGPT2</th>\n",
       "      <th>is_beat_bestMamba</th>\n",
       "      <th>is_beat_bestLlama</th>\n",
       "      <th>is_beat_bestHyena</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30-Day Readmission</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>0.823392</td>\n",
       "      <td>0.788580</td>\n",
       "      <td>0.852653</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>0.007630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30-Day Readmission</td>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>0.820290</td>\n",
       "      <td>0.787215</td>\n",
       "      <td>0.849638</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>-0.003102</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30-Day Readmission</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>0.817426</td>\n",
       "      <td>0.782813</td>\n",
       "      <td>0.846699</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003402</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>-0.005966</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30-Day Readmission</td>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>0.816984</td>\n",
       "      <td>0.781272</td>\n",
       "      <td>0.848693</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>-0.006408</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30-Day Readmission</td>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>0.815761</td>\n",
       "      <td>0.779548</td>\n",
       "      <td>0.847507</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.007630</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Thrombocytopenia</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>0.851733</td>\n",
       "      <td>0.834264</td>\n",
       "      <td>0.867966</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021453</td>\n",
       "      <td>-0.008391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.018525</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Thrombocytopenia</td>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>0.851180</td>\n",
       "      <td>0.833910</td>\n",
       "      <td>0.867854</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022006</td>\n",
       "      <td>-0.008943</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>-0.019078</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Thrombocytopenia</td>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>0.848140</td>\n",
       "      <td>0.831247</td>\n",
       "      <td>0.864922</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025046</td>\n",
       "      <td>-0.011984</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>-0.022118</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Thrombocytopenia</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>0.846590</td>\n",
       "      <td>0.829281</td>\n",
       "      <td>0.863533</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026595</td>\n",
       "      <td>-0.013533</td>\n",
       "      <td>-0.005142</td>\n",
       "      <td>-0.023668</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Thrombocytopenia</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>0.846332</td>\n",
       "      <td>0.828436</td>\n",
       "      <td>0.862971</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026854</td>\n",
       "      <td>-0.013791</td>\n",
       "      <td>-0.005400</td>\n",
       "      <td>-0.023926</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              task_name            model  value_mean  lower_bound  \\\n",
       "0    30-Day Readmission  llama-base-4096    0.823392     0.788580   \n",
       "1    30-Day Readmission  llama-base-2048    0.820290     0.787215   \n",
       "2    30-Day Readmission   llama-base-512    0.817426     0.782813   \n",
       "3    30-Day Readmission  llama-base-1024    0.816984     0.781272   \n",
       "4    30-Day Readmission  mamba-tiny-4096    0.815761     0.779548   \n",
       "..                  ...              ...         ...          ...   \n",
       "216    Thrombocytopenia  llama-base-4096    0.851733     0.834264   \n",
       "217    Thrombocytopenia  llama-base-2048    0.851180     0.833910   \n",
       "218    Thrombocytopenia  llama-base-1024    0.848140     0.831247   \n",
       "219    Thrombocytopenia   llama-base-512    0.846590     0.829281   \n",
       "220    Thrombocytopenia  mamba-tiny-1024    0.846332     0.828436   \n",
       "\n",
       "     upper_bound  Logistic_AUROC  GBM_AUROC  Random_Forest_AUROC  CLMBR_AUROC  \\\n",
       "0       0.852653           0.751      0.741                0.775        0.810   \n",
       "1       0.849638           0.751      0.741                0.775        0.810   \n",
       "2       0.846699           0.751      0.741                0.775        0.810   \n",
       "3       0.848693           0.751      0.741                0.775        0.810   \n",
       "4       0.847507           0.751      0.741                0.775        0.810   \n",
       "..           ...             ...        ...                  ...          ...   \n",
       "216     0.867966           0.753      0.815                0.811        0.852   \n",
       "217     0.867854           0.753      0.815                0.811        0.852   \n",
       "218     0.864922           0.753      0.815                0.811        0.852   \n",
       "219     0.863533           0.753      0.815                0.811        0.852   \n",
       "220     0.862971           0.753      0.815                0.811        0.852   \n",
       "\n",
       "     Baseline_AUROC  ...  delta_bestGPT2  delta_bestMamba  delta_bestLlama  \\\n",
       "0             0.775  ...        0.009367         0.007630         0.000000   \n",
       "1             0.775  ...        0.006265         0.004528        -0.003102   \n",
       "2             0.775  ...        0.003402         0.001665        -0.005966   \n",
       "3             0.775  ...        0.002959         0.001222        -0.006408   \n",
       "4             0.775  ...        0.001737         0.000000        -0.007630   \n",
       "..              ...  ...             ...              ...              ...   \n",
       "216           0.815  ...       -0.021453        -0.008391         0.000000   \n",
       "217           0.815  ...       -0.022006        -0.008943        -0.000552   \n",
       "218           0.815  ...       -0.025046        -0.011984        -0.003593   \n",
       "219           0.815  ...       -0.026595        -0.013533        -0.005142   \n",
       "220           0.815  ...       -0.026854        -0.013791        -0.005400   \n",
       "\n",
       "    delta_bestHyena  is_beat_clmbr  is_beat_baseline  is_beat_bestGPT2  \\\n",
       "0          0.009046           True              True              True   \n",
       "1          0.005944           True              True              True   \n",
       "2          0.003081           True              True              True   \n",
       "3          0.002638           True              True              True   \n",
       "4          0.001416           True              True              True   \n",
       "..              ...            ...               ...               ...   \n",
       "216       -0.018525          False              True             False   \n",
       "217       -0.019078          False              True             False   \n",
       "218       -0.022118          False              True             False   \n",
       "219       -0.023668          False              True             False   \n",
       "220       -0.023926          False              True             False   \n",
       "\n",
       "     is_beat_bestMamba  is_beat_bestLlama  is_beat_bestHyena  \n",
       "0                 True              False               True  \n",
       "1                 True              False               True  \n",
       "2                 True              False               True  \n",
       "3                 True              False               True  \n",
       "4                False              False               True  \n",
       "..                 ...                ...                ...  \n",
       "216              False              False              False  \n",
       "217              False              False              False  \n",
       "218              False              False              False  \n",
       "219              False              False              False  \n",
       "220              False              False              False  \n",
       "\n",
       "[189 rows x 29 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>model_name</th>\n",
       "      <th>task_name</th>\n",
       "      <th>gpt2-base</th>\n",
       "      <th>hyena-large</th>\n",
       "      <th>llama-base</th>\n",
       "      <th>mamba-tiny</th>\n",
       "      <th>gpt2-base_rank</th>\n",
       "      <th>hyena-large_rank</th>\n",
       "      <th>llama-base_rank</th>\n",
       "      <th>mamba-tiny_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30-Day Readmission</td>\n",
       "      <td>0.814025</td>\n",
       "      <td>0.811818</td>\n",
       "      <td>0.823392</td>\n",
       "      <td>0.815761</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acute MI</td>\n",
       "      <td>0.735222</td>\n",
       "      <td>0.742005</td>\n",
       "      <td>0.752441</td>\n",
       "      <td>0.746761</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anemia</td>\n",
       "      <td>0.959098</td>\n",
       "      <td>0.959973</td>\n",
       "      <td>0.963632</td>\n",
       "      <td>0.964796</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyperkalemia</td>\n",
       "      <td>0.814182</td>\n",
       "      <td>0.817473</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.821721</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hyperlipidemia</td>\n",
       "      <td>0.678603</td>\n",
       "      <td>0.689662</td>\n",
       "      <td>0.687986</td>\n",
       "      <td>0.697883</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hypertension</td>\n",
       "      <td>0.721966</td>\n",
       "      <td>0.694439</td>\n",
       "      <td>0.721205</td>\n",
       "      <td>0.720358</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hypoglycemia</td>\n",
       "      <td>0.796039</td>\n",
       "      <td>0.790018</td>\n",
       "      <td>0.805305</td>\n",
       "      <td>0.800128</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hyponatremia</td>\n",
       "      <td>0.805744</td>\n",
       "      <td>0.826314</td>\n",
       "      <td>0.796450</td>\n",
       "      <td>0.830094</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ICU Prediction</td>\n",
       "      <td>0.868563</td>\n",
       "      <td>0.822390</td>\n",
       "      <td>0.852043</td>\n",
       "      <td>0.853604</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Long LOS</td>\n",
       "      <td>0.813097</td>\n",
       "      <td>0.808156</td>\n",
       "      <td>0.827510</td>\n",
       "      <td>0.826115</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pancreatic Cancer</td>\n",
       "      <td>0.839451</td>\n",
       "      <td>0.850665</td>\n",
       "      <td>0.819300</td>\n",
       "      <td>0.866492</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Thrombocytopenia</td>\n",
       "      <td>0.873186</td>\n",
       "      <td>0.870258</td>\n",
       "      <td>0.851733</td>\n",
       "      <td>0.860123</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model_name           task_name  gpt2-base  hyena-large  llama-base  \\\n",
       "0           30-Day Readmission   0.814025     0.811818    0.823392   \n",
       "1                     Acute MI   0.735222     0.742005    0.752441   \n",
       "2                       Anemia   0.959098     0.959973    0.963632   \n",
       "3                 Hyperkalemia   0.814182     0.817473    0.816482   \n",
       "4               Hyperlipidemia   0.678603     0.689662    0.687986   \n",
       "5                 Hypertension   0.721966     0.694439    0.721205   \n",
       "6                 Hypoglycemia   0.796039     0.790018    0.805305   \n",
       "7                 Hyponatremia   0.805744     0.826314    0.796450   \n",
       "8               ICU Prediction   0.868563     0.822390    0.852043   \n",
       "9                     Long LOS   0.813097     0.808156    0.827510   \n",
       "10           Pancreatic Cancer   0.839451     0.850665    0.819300   \n",
       "11            Thrombocytopenia   0.873186     0.870258    0.851733   \n",
       "\n",
       "model_name  mamba-tiny  gpt2-base_rank  hyena-large_rank  llama-base_rank  \\\n",
       "0             0.815761               3                 4                1   \n",
       "1             0.746761               4                 3                1   \n",
       "2             0.964796               4                 3                2   \n",
       "3             0.821721               4                 2                3   \n",
       "4             0.697883               4                 2                3   \n",
       "5             0.720358               1                 4                2   \n",
       "6             0.800128               3                 4                1   \n",
       "7             0.830094               3                 2                4   \n",
       "8             0.853604               1                 4                3   \n",
       "9             0.826115               3                 4                1   \n",
       "10            0.866492               3                 2                4   \n",
       "11            0.860123               1                 2                4   \n",
       "\n",
       "model_name  mamba-tiny_rank  \n",
       "0                         2  \n",
       "1                         2  \n",
       "2                         1  \n",
       "3                         1  \n",
       "4                         1  \n",
       "5                         3  \n",
       "6                         2  \n",
       "7                         1  \n",
       "8                         2  \n",
       "9                         2  \n",
       "10                        1  \n",
       "11                        3  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_models = ['gpt2-base', 'hyena-large', 'llama-base', 'mamba-tiny']\n",
    "df_base_model_max = df_ctx.groupby(['model_name', 'task_name']).agg({ 'value_mean': 'max' }).reset_index().pivot(index='model_name', columns='task_name', values='value_mean').T.reset_index()\n",
    "for model in base_models:\n",
    "    df_base_model_max[f'{model}_rank'] = df_base_model_max[base_models].rank(axis=1, method='min', ascending=False)[model].astype(int)\n",
    "df_base_model_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rank of gpt2-base: 2.8333333333333335\n",
      "Mean rank of hyena-large: 3.0\n",
      "Mean rank of llama-base: 2.4166666666666665\n",
      "Mean rank of mamba-tiny: 1.75\n"
     ]
    }
   ],
   "source": [
    "for model in base_models:\n",
    "    print(f\"Mean rank of {model}:\", df_base_model_max[model + '_rank'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chexpert tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chexpert_win_rates = count_win_rates(df_chexpert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Win rate v. CLMBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tasks that model achieves higher AUROC than CLMBR (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>is_beat_clmbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  is_beat_clmbr\n",
       "1      gpt2-base-2048             13\n",
       "3       gpt2-base-512             13\n",
       "6    hyena-large-4096             13\n",
       "11     llama-base-512             13\n",
       "13   mamba-tiny-16384             13\n",
       "14    mamba-tiny-4096             13\n",
       "2      gpt2-base-4096             12\n",
       "0      gpt2-base-1024             12\n",
       "12    mamba-tiny-1024             12\n",
       "15    mamba-tiny-8192             12\n",
       "10    llama-base-4096             12\n",
       "9     llama-base-2048             11\n",
       "4    hyena-large-1024             10\n",
       "8     llama-base-1024             10\n",
       "7    hyena-large-8192              6\n",
       "5   hyena-large-16384              1"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# of tasks that model achieves higher AUROC than CLMBR (higher is better)\")\n",
    "df_chexpert_win_rates['clmbr_beats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUROC diff between model and CLMBR (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>delta_clmbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>0.017374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>0.015817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>0.015726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>0.015642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>0.013601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>0.013119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>0.012177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>0.011759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>0.010881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>0.010164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>0.010097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>0.007888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>0.005387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>0.004295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>0.003060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>-0.036969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  delta_clmbr\n",
       "1      gpt2-base-2048     0.017374\n",
       "14    mamba-tiny-4096     0.015817\n",
       "15    mamba-tiny-8192     0.015726\n",
       "13   mamba-tiny-16384     0.015642\n",
       "3       gpt2-base-512     0.013601\n",
       "2      gpt2-base-4096     0.013119\n",
       "12    mamba-tiny-1024     0.012177\n",
       "9     llama-base-2048     0.011759\n",
       "10    llama-base-4096     0.010881\n",
       "6    hyena-large-4096     0.010164\n",
       "0      gpt2-base-1024     0.010097\n",
       "11     llama-base-512     0.007888\n",
       "8     llama-base-1024     0.005387\n",
       "7    hyena-large-8192     0.004295\n",
       "4    hyena-large-1024     0.003060\n",
       "5   hyena-large-16384    -0.036969"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean AUROC diff between model and CLMBR (higher is better)\")\n",
    "df_chexpert_win_rates['mean_clmbr_delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Win rate v. GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>delta_bestGPT2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Atelectasis</td>\n",
       "      <td>-0.005638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0.001889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Consolidation</td>\n",
       "      <td>-0.001827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Edema</td>\n",
       "      <td>0.006886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Enlarged Cardiomediastinum</td>\n",
       "      <td>-0.028122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Fracture</td>\n",
       "      <td>-0.069331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Lung Lesion</td>\n",
       "      <td>-0.001337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Lung Opacity</td>\n",
       "      <td>0.002343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>No Finding</td>\n",
       "      <td>-0.006456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Pleural Effusion</td>\n",
       "      <td>0.001405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Pleural Other</td>\n",
       "      <td>-0.044215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Pneumonia</td>\n",
       "      <td>0.022530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>0.013662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Support Devices</td>\n",
       "      <td>0.003930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      task_name  delta_bestGPT2\n",
       "6                   Atelectasis       -0.005638\n",
       "16                 Cardiomegaly        0.001889\n",
       "34                Consolidation       -0.001827\n",
       "48                        Edema        0.006886\n",
       "73   Enlarged Cardiomediastinum       -0.028122\n",
       "93                     Fracture       -0.069331\n",
       "99                  Lung Lesion       -0.001337\n",
       "115                Lung Opacity        0.002343\n",
       "135                  No Finding       -0.006456\n",
       "147            Pleural Effusion        0.001405\n",
       "167               Pleural Other       -0.044215\n",
       "176                   Pneumonia        0.022530\n",
       "193                Pneumothorax        0.013662\n",
       "209             Support Devices        0.003930"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chexpert[df_chexpert['model'] == 'mamba-tiny-16384'][['task_name', 'delta_bestGPT2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tasks that model achieves higher AUROC than best GPT2-base (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>is_beat_bestGPT2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  is_beat_bestGPT2\n",
       "14    mamba-tiny-4096                 8\n",
       "13   mamba-tiny-16384                 7\n",
       "15    mamba-tiny-8192                 7\n",
       "6    hyena-large-4096                 6\n",
       "4    hyena-large-1024                 4\n",
       "12    mamba-tiny-1024                 3\n",
       "7    hyena-large-8192                 2\n",
       "10    llama-base-4096                 2\n",
       "9     llama-base-2048                 2\n",
       "8     llama-base-1024                 1\n",
       "5   hyena-large-16384                 0\n",
       "3       gpt2-base-512                 0\n",
       "0      gpt2-base-1024                 0\n",
       "1      gpt2-base-2048                 0\n",
       "2      gpt2-base-4096                 0\n",
       "11     llama-base-512                 0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# of tasks that model achieves higher AUROC than best GPT2-base (higher is better)\")\n",
    "df_chexpert_win_rates['gpt2_beats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUROC diff between model and best GPT2 (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>delta_bestGPT2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>-0.005717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>-0.007273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>-0.007364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>-0.007449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>-0.009489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>-0.009972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>-0.010913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>-0.011332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>-0.012209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>-0.012926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>-0.012994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>-0.015202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>-0.017704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>-0.018796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>-0.020030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>-0.060060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  delta_bestGPT2\n",
       "1      gpt2-base-2048       -0.005717\n",
       "14    mamba-tiny-4096       -0.007273\n",
       "15    mamba-tiny-8192       -0.007364\n",
       "13   mamba-tiny-16384       -0.007449\n",
       "3       gpt2-base-512       -0.009489\n",
       "2      gpt2-base-4096       -0.009972\n",
       "12    mamba-tiny-1024       -0.010913\n",
       "9     llama-base-2048       -0.011332\n",
       "10    llama-base-4096       -0.012209\n",
       "6    hyena-large-4096       -0.012926\n",
       "0      gpt2-base-1024       -0.012994\n",
       "11     llama-base-512       -0.015202\n",
       "8     llama-base-1024       -0.017704\n",
       "7    hyena-large-8192       -0.018796\n",
       "4    hyena-large-1024       -0.020030\n",
       "5   hyena-large-16384       -0.060060"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean AUROC diff between model and best GPT2 (higher is better)\")\n",
    "df_chexpert_win_rates['mean_gpt2_delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Win rate v. Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tasks that model achieves higher AUROC than best llama-base (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>is_beat_bestLlama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  is_beat_bestLlama\n",
       "15    mamba-tiny-8192                 10\n",
       "14    mamba-tiny-4096                  8\n",
       "13   mamba-tiny-16384                  7\n",
       "12    mamba-tiny-1024                  7\n",
       "1      gpt2-base-2048                  6\n",
       "2      gpt2-base-4096                  5\n",
       "0      gpt2-base-1024                  3\n",
       "6    hyena-large-4096                  3\n",
       "4    hyena-large-1024                  2\n",
       "7    hyena-large-8192                  2\n",
       "3       gpt2-base-512                  2\n",
       "5   hyena-large-16384                  1\n",
       "11     llama-base-512                  0\n",
       "10    llama-base-4096                  0\n",
       "9     llama-base-2048                  0\n",
       "8     llama-base-1024                  0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# of tasks that model achieves higher AUROC than best llama-base (higher is better)\")\n",
    "df_chexpert_win_rates['llama_beats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUROC diff between model and best Llama (higher is better)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>delta_bestLlama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-2048</td>\n",
       "      <td>0.000882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>-0.000675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>-0.000766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>-0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-512</td>\n",
       "      <td>-0.002890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-4096</td>\n",
       "      <td>-0.003373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>-0.004315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>-0.004733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>-0.005610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-4096</td>\n",
       "      <td>-0.006328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-base-1024</td>\n",
       "      <td>-0.006395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>-0.008604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>-0.011105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hyena-large-8192</td>\n",
       "      <td>-0.012197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-1024</td>\n",
       "      <td>-0.013432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-16384</td>\n",
       "      <td>-0.053461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  delta_bestLlama\n",
       "1      gpt2-base-2048         0.000882\n",
       "14    mamba-tiny-4096        -0.000675\n",
       "15    mamba-tiny-8192        -0.000766\n",
       "13   mamba-tiny-16384        -0.000850\n",
       "3       gpt2-base-512        -0.002890\n",
       "2      gpt2-base-4096        -0.003373\n",
       "12    mamba-tiny-1024        -0.004315\n",
       "9     llama-base-2048        -0.004733\n",
       "10    llama-base-4096        -0.005610\n",
       "6    hyena-large-4096        -0.006328\n",
       "0      gpt2-base-1024        -0.006395\n",
       "11     llama-base-512        -0.008604\n",
       "8     llama-base-1024        -0.011105\n",
       "7    hyena-large-8192        -0.012197\n",
       "4    hyena-large-1024        -0.013432\n",
       "5   hyena-large-16384        -0.053461"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean AUROC diff between model and best Llama (higher is better)\")\n",
    "df_chexpert_win_rates['mean_llama_delta']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
