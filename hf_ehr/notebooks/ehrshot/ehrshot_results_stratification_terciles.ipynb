{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sklearn.utils\n",
    "from typing import List, Optional, Dict\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "import collections\n",
    "import warnings\n",
    "import pickle\n",
    "import femr.datasets\n",
    "import datetime\n",
    "from hf_ehr.config import EHRSHOT_LABELING_FUNCTION_2_PAPER_NAME\n",
    "from hf_ehr.notebooks.ehr_specific_properties.utils import (\n",
    "    get_labels_and_features, \n",
    "    get_patient_splits_by_idx\n",
    ")\n",
    "import scipy.stats\n",
    "from femr.labelers import load_labeled_patients, LabeledPatients\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils.validation\")\n",
    "\n",
    "PATH_TO_DATABASE: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/femr/extract'\n",
    "PATH_TO_FEATURES_DIR: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/features_ehrshot'\n",
    "PATH_TO_RESULTS_DIR: str = '/share/pi/nigam/migufuen/ehrshot-benchmark/EHRSHOT_ASSETS/results_ehrshot'\n",
    "PATH_TO_TOKENIZED_TIMELINES_DIR: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/tokenized_timelines_ehrshot'\n",
    "PATH_TO_LABELS_DIR: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/benchmark_ehrshot'\n",
    "PATH_TO_SPLIT_CSV: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/EHRSHOT_ASSETS/splits_ehrshot/person_id_map.csv'\n",
    "femr_db = femr.datasets.PatientDatabase(PATH_TO_DATABASE)\n",
    "os.makedirs('../cache', exist_ok=True)\n",
    "\n",
    "IS_LOAD_FROM_CACHE: bool = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "First, we need to load all of the patient-level predictions / labels for every model and task.\n",
    "This takes ~30 min.\n",
    "\n",
    "1. Load list of task names, model names\n",
    "2. Load patient-level predictions for each (model, task)\n",
    "3. Load patient-level ground truth labels for each (task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks: ['new_hypertension', 'guo_los', 'lab_hypoglycemia', 'new_lupus', 'lab_hyponatremia', 'new_pancan', 'lab_anemia', 'new_acutemi', 'guo_readmission', 'lab_thrombocytopenia', 'new_hyperlipidemia', 'new_celiac', 'lab_hyperkalemia', 'guo_icu']\n",
      "# of valid models: 17\n"
     ]
    }
   ],
   "source": [
    "# Get list of tasks\n",
    "valid_tasks = os.listdir(PATH_TO_RESULTS_DIR)\n",
    "valid_tasks.remove('chexpert')\n",
    "\n",
    "# Filter results to only valid models\n",
    "valid_models = [ \n",
    "    'llama-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'llama-base-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'llama-base-2048--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'llama-base-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-2048--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'gpt2-base-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-8192--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'hyena-large-16384--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-1024--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-4096--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-8192--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last',\n",
    "    'mamba-tiny-16384--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last', \n",
    "    'clmbr',\n",
    "]\n",
    "print(\"Tasks:\", valid_tasks)\n",
    "print(\"# of valid models:\", len(valid_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from cache...\n",
      "# of (model, task) preds: 238\n"
     ]
    }
   ],
   "source": [
    "# Load patient-level predictions for each (model, task)\n",
    "predictions = {} # [key] = (model, task), [value] = df_preds\n",
    "\n",
    "if IS_LOAD_FROM_CACHE and os.path.exists('../cache/predictions.pkl'):\n",
    "    print(\"Loading predictions from cache...\")\n",
    "    predictions = pickle.load(open('../cache/predictions.pkl', 'rb'))\n",
    "else:\n",
    "    for task in tqdm(valid_tasks):\n",
    "        for model in valid_models:\n",
    "            path = os.path.join(PATH_TO_RESULTS_DIR, task, 'models', model, 'lr_lbfgs', f'subtask={task}', 'k=-1', 'preds.csv')\n",
    "            if not os.path.exists(path):\n",
    "                print(\"Missing path for \", model, task)\n",
    "            df_preds = pd.read_csv(path)\n",
    "            assert df_preds['replicate'].nunique() == 1, f\"Multiple replicates for {model}, {task}\"\n",
    "            predictions[(model, task)] = df_preds\n",
    "    # Save results to .pkl file\n",
    "    with open('../cache/predictions.pkl', 'wb') as f:\n",
    "        pickle.dump(predictions, f)\n",
    "print(\"# of (model, task) preds:\", len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading label data from cache...\n"
     ]
    }
   ],
   "source": [
    "# Load patient-level labels for each task\n",
    "# NOTE: Takes ~1 hr\n",
    "label_data = {} # [key] = task, [value] = {'times': label_times, 'values': label_values, 'patient_ids': patient_ids}\n",
    "\n",
    "if IS_LOAD_FROM_CACHE and os.path.exists('../cache/label_data.pkl'):\n",
    "    print(\"Loading label data from cache...\")\n",
    "    label_data = pickle.load(open('../cache/label_data.pkl', 'rb'))\n",
    "else:\n",
    "    for task in tqdm(valid_tasks):\n",
    "        # Load labeled patients for this task\n",
    "        LABELING_FUNCTION: str = task\n",
    "        PATH_TO_LABELED_PATIENTS: str =  os.path.join(PATH_TO_LABELS_DIR, LABELING_FUNCTION, 'labeled_patients.csv')\n",
    "        labeled_patients = femr.labelers.load_labeled_patients(PATH_TO_LABELED_PATIENTS)\n",
    "        \n",
    "        # Get features for patients\n",
    "        model: str = 'gpt2-base-512--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last'\n",
    "        patient_ids, label_values, label_times, feature_matrixes = get_labels_and_features(labeled_patients, \n",
    "                                                                                            PATH_TO_FEATURES_DIR, \n",
    "                                                                                            PATH_TO_TOKENIZED_TIMELINES_DIR,\n",
    "                                                                                            models_to_keep=[model,])\n",
    "        train_pids_idx, val_pids_idx, test_pids_idx = get_patient_splits_by_idx(PATH_TO_SPLIT_CSV, patient_ids)\n",
    "        label_times = label_times[train_pids_idx + val_pids_idx + test_pids_idx]\n",
    "        label_values = label_values[train_pids_idx + val_pids_idx + test_pids_idx]\n",
    "        patient_ids = patient_ids[train_pids_idx + val_pids_idx + test_pids_idx]\n",
    "        label_times = [ x.astype(datetime.datetime) for x in label_times ] # cast to Python datetime\n",
    "        label_data[task] = {'times': label_times, 'values': label_values, 'patient_ids': patient_ids}\n",
    "\n",
    "    # Save results to .pkl file\n",
    "    with open('../cache/label_data.pkl', 'wb') as f:\n",
    "        pickle.dump(label_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate CIs\n",
    "\n",
    "Calculate bootstrapped 95% CIs over the test set (one sample per patient).\n",
    "\n",
    "1. Generate 1000 bootstrapped resamples of patient IDs across all splits\n",
    "2. Loop through every (model, task) preds, limit to test set, loop through 1000 bootstrap weightings, loop through each stratification metric, recalculate **terciles** of patients based on their metrics, and calculate Brier score for each **tercile**\n",
    "3. Calculate 95% CIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Bootstrapping of Brier Scores across Terciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "strats = {\n",
    "    'inter_event_times': [ 'std' ],\n",
    "    'n_gram_count': ['rr_1'], \n",
    "    'timeline_lengths': ['n_events'],\n",
    "}\n",
    "path_to_ehrshot_metrics_dir: str = '/share/pi/nigam/mwornow/ehrshot-benchmark/ehrshot/stratify/'\n",
    "\n",
    "def weighted_quantile(values, quantiles, sample_weight=None, values_sorted=False):\n",
    "    values = np.array(values)\n",
    "    quantiles = np.array(quantiles)\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones(len(values))\n",
    "    sample_weight = np.array(sample_weight)\n",
    "    assert np.all(quantiles >= 0) and np.all(quantiles <= 1), \\\n",
    "        'quantiles should be in [0, 1]'\n",
    "\n",
    "    if not values_sorted:\n",
    "        sorter = np.argsort(values)\n",
    "        values = values[sorter]\n",
    "        sample_weight = sample_weight[sorter]\n",
    "\n",
    "    weighted_quantiles = np.cumsum(sample_weight) - 0.5 * sample_weight\n",
    "    weighted_quantiles /= np.sum(sample_weight)\n",
    "    return np.interp(quantiles, weighted_quantiles, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique patients: 6275\n"
     ]
    }
   ],
   "source": [
    "# Get all patient IDs\n",
    "all_patients = np.sort(np.unique(np.concatenate([v['patient_ids'] for v in label_data.values()])))\n",
    "print(\"# of unique patients:\", len(all_patients))\n",
    "\n",
    "# Patient-level resampling across all patient IDs in train/val/test\n",
    "# Later, we limit to just test patient IDs per task\n",
    "bootstrap_weights = []\n",
    "np.random.seed(342342)\n",
    "for i in range(1000):\n",
    "    patient_sample = np.random.choice(list(range(len(all_patients))), len(all_patients), replace=True)\n",
    "    weights = np.zeros_like(all_patients, dtype=np.float32)\n",
    "    np.add.at(weights, patient_sample, 1)\n",
    "    assert np.mean(weights) == 1\n",
    "    bootstrap_weights.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting\n",
    "def model_to_base(model: str) -> str:\n",
    "    return model.split('-')[0]\n",
    "def model_to_ctx(model: str) -> str:\n",
    "    return int(model.split('--')[0].split('-')[-1]) if model != 'clmbr' else 0\n",
    "def model_to_name(model: str) -> str:\n",
    "    return model.split('--')[0]\n",
    "\n",
    "def clean_df_briers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['model_name'] = df['model'].apply(model_to_name)\n",
    "    df['model_base'] = df['model'].apply(model_to_base)\n",
    "    df['ctx_length'] = df['model'].apply(model_to_ctx).astype(int)\n",
    "    if 'brier_true' in df.columns:\n",
    "        df['formatted_brier_ci_mean'] = df['brier_true'].apply(lambda x: f\"{x:.4f}\")\n",
    "        df['formatted_brier'] = df.apply(lambda x: f\"{x['brier_true']:.4f} ({x['brier_ci_025']:.4f}, {x['brier_ci_975']:.4f})\", axis=1)\n",
    "        df['formatted_brier_ci'] = df.apply(lambda x: f\"({x['brier_ci_025']:.4f}, {x['brier_ci_975']:.4f})\", axis=1)\n",
    "        df['is_brier_win'] = (df['brier_ci_025'] > 0) & (df['brier_ci_975'] > 0)\n",
    "        df['is_brier_stat_sig'] = (df['brier_ci_025'] > 0) | (df['brier_ci_975'] < 0)\n",
    "    if 'win_rate_true' in df.columns:\n",
    "        df['formatted_win_rate_ci_mean'] = df['win_rate_true'].apply(lambda x: f\"{x:.4f}\")\n",
    "        df['formatted_win_rate'] = df.apply(lambda x: f\"{x['win_rate_true']:.4f} ({x['win_rate_ci_025']:.4f}, {x['win_rate_ci_975']:.4f})\", axis=1)\n",
    "        df['formatted_win_rate'] = df.apply(lambda x: f\"{x['win_rate_true']:.4f} ({x['win_rate_ci_025']:.4f}, {x['win_rate_ci_975']:.4f})\", axis=1)\n",
    "        df['is_win_rate_win'] = (df['win_rate_ci_025'] > 0) & (df['win_rate_ci_975'] > 0)\n",
    "        df['is_win_rate_stat_sig'] = (df['win_rate_ci_025'] > 0) | (df['win_rate_ci_975'] < 0)\n",
    "    df = df[['model', 'model_name', 'model_base', 'ctx_length'] + [col for col in df.columns if col not in ['model', 'model_name', 'model_base', 'ctx_length']]]\n",
    "    df = df.sort_values(['model_base', 'ctx_length', ] + ([ 'task' ] if 'task' in df.columns else []))\n",
    "    return df\n",
    "\n",
    "def format_df_briers(df: pd.DataFrame, model_start: Optional[str] = None) -> pd.DataFrame:\n",
    "    if model_start is not None:\n",
    "        df = df[df['model'].str.startswith(model_start)]\n",
    "    df = df.drop(columns=['model', 'model_name', 'formatted_brier', 'brier_ci_mean', 'brier_ci_025', 'brier_ci_500', 'brier_ci_975', 'is_brier_win'], errors='ignore') \\\n",
    "        .rename(columns={   'model_base' : 'Model', \n",
    "                            'ctx_length' : 'Context Length', \n",
    "                            'formatted_brier_ci_mean' : r'$\\Delta$ over baseline', \n",
    "                            'formatted_brier_ci' : '95% CI', \n",
    "                            'formatted_win_rate_ci_mean' : r'Win Rate over baseline', \n",
    "                            'formatted_win_rate_ci' : 'Win Rate 95% CI', \n",
    "                            'is_brier_stat_sig' : 'Statistically Significant',\n",
    "                            'is_win_rate_stat_sig' : 'Win Rate Statistically Significant',\n",
    "                            'task' : 'Task'\n",
    "                }, errors='ignore') \\\n",
    "        .sort_values(['Model', 'Context Length',] + ([ 'Task' ] if 'task' in df.columns else []))\n",
    "    return df\n",
    "\n",
    "def format_df_briers_for_latex(df: pd.DataFrame, model_start: Optional[str] = None) -> str:\n",
    "    latex = format_df_briers(df, model_start).to_latex(index=False, escape=False)\n",
    "    for k, v in EHRSHOT_LABELING_FUNCTION_2_PAPER_NAME.items():\n",
    "        latex = latex.replace(k, v)\n",
    "    latex = latex.replace('False', '').replace('True', '\\checkmark')\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_scores_per_tercile = collections.defaultdict(list) # [key] = (model, task, strat, strat_col, tercile), [value] = brier's across 1k resamples for this tercile\n",
    "brier_scores_metadata_per_tercile = collections.defaultdict(list) # [key] = (model, task, strat, strat_col, tercile), [value] = df_terciles\n",
    "true_brier_scores_per_tercile = collections.defaultdict(int) # [key] = (model, task, strat, strat_col, tercile), [value] = True brier score for this model using original raw (non-bootstrapped) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Brier scores for each (model, task, tercile) across 1k resamples\n",
    "if IS_LOAD_FROM_CACHE and os.path.exists('../cache/brier_scores_per_tercile.pkl') and os.path.exists('../cache/true_brier_scores_per_tercile.pkl') and os.path.exists('../cache/brier_scores_metadata_per_tercile.pkl'):\n",
    "    print(\"Loading brier_scores_per_tercile from cache...\")\n",
    "    brier_scores_per_tercile = pickle.load(open('../cache/brier_scores_per_tercile.pkl', 'rb'))\n",
    "    print(\"Loading true_brier_scores_per_tercile from cache...\")\n",
    "    true_brier_scores_per_tercile = pickle.load(open('../cache/true_brier_scores_per_tercile.pkl', 'rb'))\n",
    "    print(\"Loading brier_scores_metadata_per_tercile from cache...\")\n",
    "    brier_scores_metadata_per_tercile = pickle.load(open('../cache/brier_scores_metadata_per_tercile.pkl', 'rb'))\n",
    "else:\n",
    "    # Add metrics to each (model, task) predictions\n",
    "    for (model, task), df_preds in tqdm(predictions.items(), total=len(predictions)):\n",
    "        # Add (patient ID, label time) to df_preds to align with terciles\n",
    "        df_preds['pid'] = label_data[task]['patient_ids']\n",
    "        df_preds['label_time'] = label_data[task]['times']\n",
    "\n",
    "        # Test split\n",
    "        df_preds = df_preds[df_preds['split'] == 'test']\n",
    "\n",
    "        # Merge patient IDs\n",
    "        for strat, strat_cols in strats.items():\n",
    "            df_metrics = pd.read_parquet(os.path.join(path_to_ehrshot_metrics_dir, f'df__{task}__{strat}__metrics.parquet'))\n",
    "\n",
    "            # If stratifying by inter-event times, need to pivot table since 'time' and 'metric' are separate columns\n",
    "            if strat == 'inter_event_times':\n",
    "                df_metrics = df_metrics.pivot_table(index=['pid', 'pid_idx', 'label_time', 'sub_task'], columns='metric', values='time').reset_index()\n",
    "\n",
    "            # Merge metrics with predictions\n",
    "            df_ = pd.merge(df_preds, df_metrics, on=['pid', 'label_time'])\n",
    "            if df_.shape[0] != df_preds.shape[0]:\n",
    "                print(f'{model} | {task} | {strat} | Number of rows in df does not match number of rows in df_preds: {df_.shape[0]} != {df_preds.shape[0]}')\n",
    "                \n",
    "            for strat_col in strat_cols:\n",
    "                if strat_col not in df_metrics.columns:\n",
    "                    raise ValueError(f'col={strat_col} not in df_metrics columns for strat={strat}.')\n",
    "\n",
    "                # Metric values\n",
    "                metric_values = df_[strat_col]\n",
    "\n",
    "                # Get test patient IDs\n",
    "                patient_ids = df_['pid'].values\n",
    "                patient_id_indices = np.searchsorted(all_patients, patient_ids)\n",
    "                assert np.all(patient_ids == all_patients[patient_id_indices])\n",
    "                \n",
    "                # Calculate \"true\" Brier scores on non-bootstrapped dataset\n",
    "                df_['tercile'] = pd.qcut(df_[strat_col].fillna(0).rank(method='min'), 3, labels=False)\n",
    "                assert set(df_['tercile'].unique()) == {0, 1, 2}, f'terciles not 0, 1, 2: {set(df_[\"tercile\"].unique())}'\n",
    "                for tercile in range(3):\n",
    "                    # Limit to this tercile\n",
    "                    df_tercile = df_[df_['tercile'] == tercile]\n",
    "                    # Labels / Preds\n",
    "                    y = df_tercile['y'].values.astype(int)\n",
    "                    pred_proba = df_tercile['pred_proba'].values\n",
    "                    # Calculate Brier score\n",
    "                    brier = brier_score_loss(y, pred_proba)\n",
    "                    true_brier_scores_per_tercile[(model, task, strat, strat_col, tercile)] = brier\n",
    "\n",
    "                # Do bootstraps\n",
    "                for weights in bootstrap_weights:\n",
    "                    weights = weights[patient_id_indices]\n",
    "                    assert weights.shape[0] == df_.shape[0] and weights.shape[0] == metric_values.shape[0], f\"Error - weights shape: {weights.shape[0]}, df shape: {df_.shape[0]}, metric_values shape: {metric_values.shape[0]}\"\n",
    "\n",
    "                    # Calculate terciles\n",
    "                    quantile_cutoffs = weighted_quantile(metric_values, [0.33, .67, 1], weights)\n",
    "                    df_['tercile'] = np.searchsorted(quantile_cutoffs, metric_values)\n",
    "                    df_['weight'] = weights\n",
    "                    assert set(df_['tercile'].unique()) == {0, 1, 2}, f'terciles not 0, 1, 2: {set(df_metrics[\"tercile\"].unique())}'\n",
    "\n",
    "                    # Calculate Brier scores\n",
    "                    for tercile in range(3):\n",
    "                        # Limit to this tercile\n",
    "                        df_tercile = df_[df_['tercile'] == tercile]\n",
    "                        # Labels / Preds\n",
    "                        y = df_tercile['y'].values.astype(int)\n",
    "                        pred_proba = df_tercile['pred_proba'].values\n",
    "                        sample_weight = df_tercile['weight'].values\n",
    "                        # Calculate Brier score\n",
    "                        brier = brier_score_loss(y, pred_proba, sample_weight=sample_weight)\n",
    "                        brier_scores_per_tercile[(model, task, strat, strat_col, tercile)].append(brier)\n",
    "                        brier_scores_metadata_per_tercile[(model, task, strat, strat_col, tercile)].append({\n",
    "                            'brier' : ((y - pred_proba) ** 2).astype(np.float32),\n",
    "                            'sample_weight' : sample_weight.astype(np.int8),\n",
    "                        })\n",
    "\n",
    "    # Save results to .pkl file\n",
    "    with open('../cache/brier_scores_per_tercile.pkl', 'wb') as f:\n",
    "        pickle.dump(brier_scores_per_tercile, f)\n",
    "    with open('../cache/true_brier_scores_per_tercile.pkl', 'wb') as f:\n",
    "        pickle.dump(true_brier_scores_per_tercile, f)\n",
    "    with open('../cache/brier_scores_metadata_per_tercile.pkl', 'wb') as f:\n",
    "        pickle.dump(brier_scores_metadata_per_tercile, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-Level CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>tercile</th>\n",
       "      <th>strat</th>\n",
       "      <th>strat_col</th>\n",
       "      <th>brier_true</th>\n",
       "      <th>brier_ci_mean</th>\n",
       "      <th>brier_ci_025</th>\n",
       "      <th>brier_ci_500</th>\n",
       "      <th>brier_ci_975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>0</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0.031065</td>\n",
       "      <td>0.031765</td>\n",
       "      <td>0.020401</td>\n",
       "      <td>0.031514</td>\n",
       "      <td>0.045442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>0</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>0.036430</td>\n",
       "      <td>0.037051</td>\n",
       "      <td>0.024866</td>\n",
       "      <td>0.036749</td>\n",
       "      <td>0.050059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>0</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>0.039529</td>\n",
       "      <td>0.039948</td>\n",
       "      <td>0.028213</td>\n",
       "      <td>0.040080</td>\n",
       "      <td>0.053327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>1</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0.029109</td>\n",
       "      <td>0.028930</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.028980</td>\n",
       "      <td>0.039850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>1</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.031551</td>\n",
       "      <td>0.020631</td>\n",
       "      <td>0.031499</td>\n",
       "      <td>0.043136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_pancan</td>\n",
       "      <td>1</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>0.034799</td>\n",
       "      <td>0.022949</td>\n",
       "      <td>0.034370</td>\n",
       "      <td>0.048056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_pancan</td>\n",
       "      <td>1</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>0.027915</td>\n",
       "      <td>0.027906</td>\n",
       "      <td>0.018306</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.040117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_pancan</td>\n",
       "      <td>2</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0.026984</td>\n",
       "      <td>0.027351</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.026637</td>\n",
       "      <td>0.040013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_pancan</td>\n",
       "      <td>2</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>0.015716</td>\n",
       "      <td>0.008298</td>\n",
       "      <td>0.015370</td>\n",
       "      <td>0.025473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_pancan</td>\n",
       "      <td>2</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>0.023173</td>\n",
       "      <td>0.022848</td>\n",
       "      <td>0.012525</td>\n",
       "      <td>0.022412</td>\n",
       "      <td>0.035149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2142 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  model        task  tercile  \\\n",
       "2133                                              clmbr     guo_icu        0   \n",
       "2136                                              clmbr     guo_icu        0   \n",
       "2139                                              clmbr     guo_icu        0   \n",
       "2134                                              clmbr     guo_icu        1   \n",
       "2137                                              clmbr     guo_icu        1   \n",
       "...                                                 ...         ...      ...   \n",
       "895   mamba-tiny-8192--clmbr_train-tokens-total_nonP...  new_pancan        1   \n",
       "898   mamba-tiny-8192--clmbr_train-tokens-total_nonP...  new_pancan        1   \n",
       "893   mamba-tiny-8192--clmbr_train-tokens-total_nonP...  new_pancan        2   \n",
       "896   mamba-tiny-8192--clmbr_train-tokens-total_nonP...  new_pancan        2   \n",
       "899   mamba-tiny-8192--clmbr_train-tokens-total_nonP...  new_pancan        2   \n",
       "\n",
       "                  strat strat_col  brier_true  brier_ci_mean  brier_ci_025  \\\n",
       "2133  inter_event_times       std    0.031065       0.031765      0.020401   \n",
       "2136       n_gram_count      rr_1    0.036430       0.037051      0.024866   \n",
       "2139   timeline_lengths  n_events    0.039529       0.039948      0.028213   \n",
       "2134  inter_event_times       std    0.029109       0.028930      0.018663   \n",
       "2137       n_gram_count      rr_1    0.033230       0.031551      0.020631   \n",
       "...                 ...       ...         ...            ...           ...   \n",
       "895        n_gram_count      rr_1    0.035790       0.034799      0.022949   \n",
       "898    timeline_lengths  n_events    0.027915       0.027906      0.018306   \n",
       "893   inter_event_times       std    0.026984       0.027351      0.017607   \n",
       "896        n_gram_count      rr_1    0.015759       0.015716      0.008298   \n",
       "899    timeline_lengths  n_events    0.023173       0.022848      0.012525   \n",
       "\n",
       "      brier_ci_500  brier_ci_975  \n",
       "2133      0.031514      0.045442  \n",
       "2136      0.036749      0.050059  \n",
       "2139      0.040080      0.053327  \n",
       "2134      0.028980      0.039850  \n",
       "2137      0.031499      0.043136  \n",
       "...            ...           ...  \n",
       "895       0.034370      0.048056  \n",
       "898       0.027600      0.040117  \n",
       "893       0.026637      0.040013  \n",
       "896       0.015370      0.025473  \n",
       "899       0.022412      0.035149  \n",
       "\n",
       "[2142 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate task-level Brier CIs\n",
    "df_brier_cis = []\n",
    "for key in brier_scores_per_tercile.keys():\n",
    "    model, task, strat, strat_col, tercile = key\n",
    "    scores = brier_scores_per_tercile[key]\n",
    "    df_brier_cis.append({\n",
    "        'model' : model,\n",
    "        'task' : task,\n",
    "        'tercile' : tercile,\n",
    "        'strat' : strat,\n",
    "        'strat_col' : strat_col,\n",
    "        'brier_true' : true_brier_scores_per_tercile[key],\n",
    "        'brier_ci_mean' : np.mean(scores),\n",
    "        'brier_ci_025' : np.percentile(scores, 2.5),\n",
    "        'brier_ci_500' : np.percentile(scores, 50),\n",
    "        'brier_ci_975' : np.percentile(scores, 97.5),\n",
    "    })\n",
    "df_brier_cis = pd.DataFrame(df_brier_cis).sort_values(['model', 'task', 'tercile'])\n",
    "df_brier_cis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-level CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1258, 2195, 100568, 2243, 67028, 2220, 58155, 2127, 2189, 56338, 1317, 2222, 63653, 2037]\n"
     ]
    }
   ],
   "source": [
    "# Get test PIDs for each task\n",
    "task_2_test_pids = {}\n",
    "for (model, task), df_preds in predictions.items():\n",
    "    if task in task_2_test_pids:\n",
    "        continue\n",
    "    assert label_data[task]['patient_ids'].shape[0] == len(label_data[task]['times'])\n",
    "    df_preds['pid'] = label_data[task]['patient_ids']\n",
    "    df_preds['label_time'] = label_data[task]['times']\n",
    "    test_pids = df_preds[df_preds['split'] == 'test']\n",
    "    task_2_test_pids[task] = test_pids['pid']\n",
    "    if len(task_2_test_pids[task]) == len(valid_tasks):\n",
    "        break\n",
    "print([ len(task_2_test_pids[task]) for task in valid_tasks ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>strat</th>\n",
       "      <th>strat_col</th>\n",
       "      <th>tercile</th>\n",
       "      <th>brier_true</th>\n",
       "      <th>brier_ci_mean</th>\n",
       "      <th>brier_ci_025</th>\n",
       "      <th>brier_ci_500</th>\n",
       "      <th>brier_ci_975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>all</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0</td>\n",
       "      <td>0.069993</td>\n",
       "      <td>0.070055</td>\n",
       "      <td>0.063221</td>\n",
       "      <td>0.070332</td>\n",
       "      <td>0.076010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>all</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067175</td>\n",
       "      <td>0.067434</td>\n",
       "      <td>0.063413</td>\n",
       "      <td>0.067454</td>\n",
       "      <td>0.071231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>all</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072197</td>\n",
       "      <td>0.072107</td>\n",
       "      <td>0.068240</td>\n",
       "      <td>0.072184</td>\n",
       "      <td>0.076160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>all</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>1</td>\n",
       "      <td>0.072023</td>\n",
       "      <td>0.072004</td>\n",
       "      <td>0.067915</td>\n",
       "      <td>0.071931</td>\n",
       "      <td>0.076225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>all</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.073277</td>\n",
       "      <td>0.073251</td>\n",
       "      <td>0.068984</td>\n",
       "      <td>0.073308</td>\n",
       "      <td>0.077590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>all</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.069956</td>\n",
       "      <td>0.069877</td>\n",
       "      <td>0.065870</td>\n",
       "      <td>0.069895</td>\n",
       "      <td>0.074072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>all</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>1</td>\n",
       "      <td>0.069587</td>\n",
       "      <td>0.069711</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>0.069711</td>\n",
       "      <td>0.074168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>all</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>2</td>\n",
       "      <td>0.072616</td>\n",
       "      <td>0.073025</td>\n",
       "      <td>0.068764</td>\n",
       "      <td>0.073041</td>\n",
       "      <td>0.077507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>all</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.073496</td>\n",
       "      <td>0.073672</td>\n",
       "      <td>0.067804</td>\n",
       "      <td>0.073706</td>\n",
       "      <td>0.079436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>all</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>2</td>\n",
       "      <td>0.070719</td>\n",
       "      <td>0.070925</td>\n",
       "      <td>0.064547</td>\n",
       "      <td>0.071054</td>\n",
       "      <td>0.076406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 model task  \\\n",
       "48                                               clmbr  all   \n",
       "99                                               clmbr  all   \n",
       "150                                              clmbr  all   \n",
       "49                                               clmbr  all   \n",
       "100                                              clmbr  all   \n",
       "..                                                 ...  ...   \n",
       "94   mamba-tiny-8192--clmbr_train-tokens-total_nonP...  all   \n",
       "145  mamba-tiny-8192--clmbr_train-tokens-total_nonP...  all   \n",
       "44   mamba-tiny-8192--clmbr_train-tokens-total_nonP...  all   \n",
       "95   mamba-tiny-8192--clmbr_train-tokens-total_nonP...  all   \n",
       "146  mamba-tiny-8192--clmbr_train-tokens-total_nonP...  all   \n",
       "\n",
       "                 strat strat_col  tercile  brier_true  brier_ci_mean  \\\n",
       "48   inter_event_times       std        0    0.069993       0.070055   \n",
       "99        n_gram_count      rr_1        0    0.067175       0.067434   \n",
       "150   timeline_lengths  n_events        0    0.072197       0.072107   \n",
       "49   inter_event_times       std        1    0.072023       0.072004   \n",
       "100       n_gram_count      rr_1        1    0.073277       0.073251   \n",
       "..                 ...       ...      ...         ...            ...   \n",
       "94        n_gram_count      rr_1        1    0.069956       0.069877   \n",
       "145   timeline_lengths  n_events        1    0.069587       0.069711   \n",
       "44   inter_event_times       std        2    0.072616       0.073025   \n",
       "95        n_gram_count      rr_1        2    0.073496       0.073672   \n",
       "146   timeline_lengths  n_events        2    0.070719       0.070925   \n",
       "\n",
       "     brier_ci_025  brier_ci_500  brier_ci_975  \n",
       "48       0.063221      0.070332      0.076010  \n",
       "99       0.063413      0.067454      0.071231  \n",
       "150      0.068240      0.072184      0.076160  \n",
       "49       0.067915      0.071931      0.076225  \n",
       "100      0.068984      0.073308      0.077590  \n",
       "..            ...           ...           ...  \n",
       "94       0.065870      0.069895      0.074072  \n",
       "145      0.065651      0.069711      0.074168  \n",
       "44       0.068764      0.073041      0.077507  \n",
       "95       0.067804      0.073706      0.079436  \n",
       "146      0.064547      0.071054      0.076406  \n",
       "\n",
       "[153 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reweight each bootstrap by the number of labels in the task\n",
    "n_labels_per_task = []\n",
    "for weights in bootstrap_weights:\n",
    "    weights_per_task = [\n",
    "        weights[np.searchsorted(all_patients, task_2_test_pids[task])] \n",
    "        for task in valid_tasks\n",
    "    ]\n",
    "    n_labels_per_task.append([w.sum() for w in weights_per_task])\n",
    "n_labels_per_task = np.array(n_labels_per_task).T\n",
    "assert n_labels_per_task.shape == (14, 1000)\n",
    "\n",
    "# Calculate model-level Brier CIs\n",
    "df_brier_model_cis = []\n",
    "for strat, strat_cols in strats.items():\n",
    "    for strat_col in strat_cols:\n",
    "        for model in valid_models:\n",
    "            for tercile in range(3):\n",
    "                ci_scores = [] # scores for this tercile, averaged across all tasks\n",
    "                true_scores = [] # true scores for this tercile, averaged across all tasks\n",
    "                for task in valid_tasks:\n",
    "                    ci_scores.append(brier_scores_per_tercile[(model, task, strat, strat_col, tercile)]) # each is 1 x 1000\n",
    "                    true_scores.append(true_brier_scores_per_tercile[(model, task, strat, strat_col, tercile)]) # each is 1 x 1\n",
    "                # Raw scores\n",
    "                scores = np.vstack(ci_scores) # 14 x 1000\n",
    "                assert scores.shape == (14, 1000)\n",
    "                # Macro-average across tasks\n",
    "                scores = np.mean(scores, axis=0)\n",
    "                # Micro-average across tasks\n",
    "                # scores = np.average(scores, axis=0, weights=n_labels_per_task)\n",
    "                # Win rates\n",
    "                # t-test\n",
    "                df_brier_model_cis.append({\n",
    "                    'model' : model,\n",
    "                    'task' : 'all',\n",
    "                    'strat' : strat,\n",
    "                    'strat_col' : strat_col,\n",
    "                    'tercile' : tercile,\n",
    "                    'brier_true' : np.mean(true_scores),\n",
    "                    'brier_ci_mean' : np.mean(scores),\n",
    "                    'brier_ci_025' : np.percentile(scores, 2.5),\n",
    "                    'brier_ci_500' : np.percentile(scores, 50),\n",
    "                    'brier_ci_975' : np.percentile(scores, 97.5),\n",
    "                })\n",
    "df_brier_model_cis = pd.DataFrame(df_brier_model_cis).sort_values(['model', 'tercile'])\n",
    "df_brier_model_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tercile</th>\n",
       "      <th>model</th>\n",
       "      <th>model_base</th>\n",
       "      <th>ctx_length</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2-base-512--clmbr_train-tokens-total_nonPAD...</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0664 (0.0603, 0.0720)</td>\n",
       "      <td>0.0700 (0.0661, 0.0742)</td>\n",
       "      <td>0.0726 (0.0688, 0.0771)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-4096--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.0665 (0.0604, 0.0723)</td>\n",
       "      <td>0.0692 (0.0655, 0.0734)</td>\n",
       "      <td>0.0752 (0.0713, 0.0801)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-1024--clmbr_train-tokens-total_non...</td>\n",
       "      <td>hyena</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0682 (0.0616, 0.0737)</td>\n",
       "      <td>0.0683 (0.0646, 0.0729)</td>\n",
       "      <td>0.0742 (0.0699, 0.0790)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-16384--clmbr_train-tokens-total_no...</td>\n",
       "      <td>hyena</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.0715 (0.0654, 0.0770)</td>\n",
       "      <td>0.0763 (0.0723, 0.0807)</td>\n",
       "      <td>0.0842 (0.0802, 0.0890)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0710 (0.0646, 0.0762)</td>\n",
       "      <td>0.0706 (0.0668, 0.0753)</td>\n",
       "      <td>0.0747 (0.0706, 0.0793)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.0678 (0.0616, 0.0736)</td>\n",
       "      <td>0.0691 (0.0654, 0.0732)</td>\n",
       "      <td>0.0734 (0.0695, 0.0782)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0708 (0.0649, 0.0765)</td>\n",
       "      <td>0.0717 (0.0679, 0.0759)</td>\n",
       "      <td>0.0761 (0.0719, 0.0808)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.0651 (0.0597, 0.0703)</td>\n",
       "      <td>0.0676 (0.0637, 0.0716)</td>\n",
       "      <td>0.0714 (0.0677, 0.0761)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tercile                                              model model_base  \\\n",
       "4        gpt2-base-512--clmbr_train-tokens-total_nonPAD...       gpt2   \n",
       "3        gpt2-base-4096--clmbr_train-tokens-total_nonPA...       gpt2   \n",
       "5        hyena-large-1024--clmbr_train-tokens-total_non...      hyena   \n",
       "6        hyena-large-16384--clmbr_train-tokens-total_no...      hyena   \n",
       "12       llama-base-512--clmbr_train-tokens-total_nonPA...      llama   \n",
       "11       llama-base-4096--clmbr_train-tokens-total_nonP...      llama   \n",
       "13       mamba-tiny-1024--clmbr_train-tokens-total_nonP...      mamba   \n",
       "14       mamba-tiny-16384--clmbr_train-tokens-total_non...      mamba   \n",
       "\n",
       "tercile  ctx_length                        0                        1  \\\n",
       "4               512  0.0664 (0.0603, 0.0720)  0.0700 (0.0661, 0.0742)   \n",
       "3              4096  0.0665 (0.0604, 0.0723)  0.0692 (0.0655, 0.0734)   \n",
       "5              1024  0.0682 (0.0616, 0.0737)  0.0683 (0.0646, 0.0729)   \n",
       "6             16384  0.0715 (0.0654, 0.0770)  0.0763 (0.0723, 0.0807)   \n",
       "12              512  0.0710 (0.0646, 0.0762)  0.0706 (0.0668, 0.0753)   \n",
       "11             4096  0.0678 (0.0616, 0.0736)  0.0691 (0.0654, 0.0732)   \n",
       "13             1024  0.0708 (0.0649, 0.0765)  0.0717 (0.0679, 0.0759)   \n",
       "14            16384  0.0651 (0.0597, 0.0703)  0.0676 (0.0637, 0.0716)   \n",
       "\n",
       "tercile                        2  \n",
       "4        0.0726 (0.0688, 0.0771)  \n",
       "3        0.0752 (0.0713, 0.0801)  \n",
       "5        0.0742 (0.0699, 0.0790)  \n",
       "6        0.0842 (0.0802, 0.0890)  \n",
       "12       0.0747 (0.0706, 0.0793)  \n",
       "11       0.0734 (0.0695, 0.0782)  \n",
       "13       0.0761 (0.0719, 0.0808)  \n",
       "14       0.0714 (0.0677, 0.0761)  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_irregularity = clean_df_briers(df_brier_model_cis[df_brier_model_cis['strat_col'] == 'std'])\n",
    "# Take the 'tercile' column and give each unique value its own column. Use the `formatted_brier` column as the values.\n",
    "df_irregularity = df_irregularity.pivot(index=['model', 'model_base', 'ctx_length'], columns='tercile', values='formatted_brier').reset_index().sort_values(['model_base', 'ctx_length'])\n",
    "\n",
    "# Limit to first / last for clarity\n",
    "df_model_latex = df_irregularity[\n",
    "    (\n",
    "        (df_irregularity['model_base'].isin(['mamba', 'hyena']) & df_irregularity['ctx_length'].isin([1024, 16384]))\n",
    "        | (df_irregularity['model_base'].isin(['gpt2', 'llama']) & df_irregularity['ctx_length'].isin([512, 4096]))\n",
    "    )\n",
    "]\n",
    "df_model_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tercile</th>\n",
       "      <th>model</th>\n",
       "      <th>model_base</th>\n",
       "      <th>ctx_length</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2-base-512--clmbr_train-tokens-total_nonPAD...</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0641 (0.0606, 0.0679)</td>\n",
       "      <td>0.0702 (0.0662, 0.0742)</td>\n",
       "      <td>0.0746 (0.0685, 0.0802)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-base-4096--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.0665 (0.0628, 0.0707)</td>\n",
       "      <td>0.0698 (0.0656, 0.0738)</td>\n",
       "      <td>0.0745 (0.0690, 0.0804)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyena-large-1024--clmbr_train-tokens-total_non...</td>\n",
       "      <td>hyena</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0652 (0.0616, 0.0691)</td>\n",
       "      <td>0.0703 (0.0661, 0.0744)</td>\n",
       "      <td>0.0753 (0.0695, 0.0809)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyena-large-16384--clmbr_train-tokens-total_no...</td>\n",
       "      <td>hyena</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.0746 (0.0708, 0.0786)</td>\n",
       "      <td>0.0771 (0.0730, 0.0812)</td>\n",
       "      <td>0.0803 (0.0747, 0.0864)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0661 (0.0626, 0.0700)</td>\n",
       "      <td>0.0733 (0.0691, 0.0774)</td>\n",
       "      <td>0.0770 (0.0712, 0.0826)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.0646 (0.0611, 0.0687)</td>\n",
       "      <td>0.0707 (0.0667, 0.0747)</td>\n",
       "      <td>0.0750 (0.0695, 0.0807)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0675 (0.0637, 0.0713)</td>\n",
       "      <td>0.0743 (0.0702, 0.0785)</td>\n",
       "      <td>0.0768 (0.0710, 0.0828)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.0626 (0.0592, 0.0666)</td>\n",
       "      <td>0.0685 (0.0643, 0.0724)</td>\n",
       "      <td>0.0729 (0.0672, 0.0785)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tercile                                              model model_base  \\\n",
       "4        gpt2-base-512--clmbr_train-tokens-total_nonPAD...       gpt2   \n",
       "3        gpt2-base-4096--clmbr_train-tokens-total_nonPA...       gpt2   \n",
       "5        hyena-large-1024--clmbr_train-tokens-total_non...      hyena   \n",
       "6        hyena-large-16384--clmbr_train-tokens-total_no...      hyena   \n",
       "12       llama-base-512--clmbr_train-tokens-total_nonPA...      llama   \n",
       "11       llama-base-4096--clmbr_train-tokens-total_nonP...      llama   \n",
       "13       mamba-tiny-1024--clmbr_train-tokens-total_nonP...      mamba   \n",
       "14       mamba-tiny-16384--clmbr_train-tokens-total_non...      mamba   \n",
       "\n",
       "tercile  ctx_length                        0                        1  \\\n",
       "4               512  0.0641 (0.0606, 0.0679)  0.0702 (0.0662, 0.0742)   \n",
       "3              4096  0.0665 (0.0628, 0.0707)  0.0698 (0.0656, 0.0738)   \n",
       "5              1024  0.0652 (0.0616, 0.0691)  0.0703 (0.0661, 0.0744)   \n",
       "6             16384  0.0746 (0.0708, 0.0786)  0.0771 (0.0730, 0.0812)   \n",
       "12              512  0.0661 (0.0626, 0.0700)  0.0733 (0.0691, 0.0774)   \n",
       "11             4096  0.0646 (0.0611, 0.0687)  0.0707 (0.0667, 0.0747)   \n",
       "13             1024  0.0675 (0.0637, 0.0713)  0.0743 (0.0702, 0.0785)   \n",
       "14            16384  0.0626 (0.0592, 0.0666)  0.0685 (0.0643, 0.0724)   \n",
       "\n",
       "tercile                        2  \n",
       "4        0.0746 (0.0685, 0.0802)  \n",
       "3        0.0745 (0.0690, 0.0804)  \n",
       "5        0.0753 (0.0695, 0.0809)  \n",
       "6        0.0803 (0.0747, 0.0864)  \n",
       "12       0.0770 (0.0712, 0.0826)  \n",
       "11       0.0750 (0.0695, 0.0807)  \n",
       "13       0.0768 (0.0710, 0.0828)  \n",
       "14       0.0729 (0.0672, 0.0785)  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repetition = clean_df_briers(df_brier_model_cis[df_brier_model_cis['strat_col'] == 'rr_1'])\n",
    "# Take the 'tercile' column and give each unique value its own column. Use the `formatted_brier` column as the values.\n",
    "df_repetition = df_repetition.pivot(index=['model', 'model_base', 'ctx_length'], columns='tercile', values='formatted_brier').reset_index().sort_values(['model_base', 'ctx_length'])\n",
    "\n",
    "# Limit to first / last for clarity\n",
    "df_model_latex = df_repetition[\n",
    "    (\n",
    "        (df_repetition['model_base'].isin(['mamba', 'hyena']) & df_repetition['ctx_length'].isin([1024, 16384]))\n",
    "        | (df_repetition['model_base'].isin(['gpt2', 'llama']) & df_repetition['ctx_length'].isin([512, 4096]))\n",
    "    )\n",
    "]\n",
    "df_model_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaTeX Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                model        model_name  \\\n",
      "64                                              clmbr             clmbr   \n",
      "65                                              clmbr             clmbr   \n",
      "66                                              clmbr             clmbr   \n",
      "67                                              clmbr             clmbr   \n",
      "16  gpt2-base-512--clmbr_train-tokens-total_nonPAD...     gpt2-base-512   \n",
      "..                                                ...               ...   \n",
      "59  mamba-tiny-8192--clmbr_train-tokens-total_nonP...   mamba-tiny-8192   \n",
      "60  mamba-tiny-16384--clmbr_train-tokens-total_non...  mamba-tiny-16384   \n",
      "61  mamba-tiny-16384--clmbr_train-tokens-total_non...  mamba-tiny-16384   \n",
      "62  mamba-tiny-16384--clmbr_train-tokens-total_non...  mamba-tiny-16384   \n",
      "63  mamba-tiny-16384--clmbr_train-tokens-total_non...  mamba-tiny-16384   \n",
      "\n",
      "   model_base  ctx_length  quartile  brier_mean  brier_ci_025  brier_ci_500  \\\n",
      "64      clmbr           0         0    0.069885      0.065220      0.069992   \n",
      "65      clmbr           0         1    0.073058      0.068459      0.072974   \n",
      "66      clmbr           0         2    0.075797      0.070718      0.075892   \n",
      "67      clmbr           0         3    0.073625      0.065431      0.073656   \n",
      "16       gpt2         512         0    0.064534      0.060300      0.064567   \n",
      "..        ...         ...       ...         ...           ...           ...   \n",
      "59      mamba        8192         3    0.069554      0.062388      0.069526   \n",
      "60      mamba       16384         0    0.063436      0.059242      0.063479   \n",
      "61      mamba       16384         1    0.068812      0.064273      0.068811   \n",
      "62      mamba       16384         2    0.071175      0.066498      0.071253   \n",
      "63      mamba       16384         3    0.069152      0.062770      0.069099   \n",
      "\n",
      "    brier_ci_975 formatted_brier_mean       formatted_brier  \\\n",
      "64      0.074372                0.070  0.070 (0.065, 0.074)   \n",
      "65      0.077792                0.073  0.073 (0.068, 0.078)   \n",
      "66      0.080744                0.076  0.076 (0.071, 0.081)   \n",
      "67      0.081331                0.074  0.074 (0.065, 0.081)   \n",
      "16      0.069122                0.065  0.065 (0.060, 0.069)   \n",
      "..           ...                  ...                   ...   \n",
      "59      0.076128                0.070  0.070 (0.062, 0.076)   \n",
      "60      0.067957                0.063  0.063 (0.059, 0.068)   \n",
      "61      0.073500                0.069  0.069 (0.064, 0.073)   \n",
      "62      0.075757                0.071  0.071 (0.066, 0.076)   \n",
      "63      0.075583                0.069  0.069 (0.063, 0.076)   \n",
      "\n",
      "   formatted_brier_ci  is_brier_win  is_stat_sig  \n",
      "64     (0.065, 0.074)          True         True  \n",
      "65     (0.068, 0.078)          True         True  \n",
      "66     (0.071, 0.081)          True         True  \n",
      "67     (0.065, 0.081)          True         True  \n",
      "16     (0.060, 0.069)          True         True  \n",
      "..                ...           ...          ...  \n",
      "59     (0.062, 0.076)          True         True  \n",
      "60     (0.059, 0.068)          True         True  \n",
      "61     (0.064, 0.073)          True         True  \n",
      "62     (0.066, 0.076)          True         True  \n",
      "63     (0.063, 0.076)          True         True  \n",
      "\n",
      "[68 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# Model-level delta CIs for LaTeX\n",
    "latex: str = clean_df_briers(df_brier_model_cis)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrlllr}\n",
      "\\toprule\n",
      "Model & Context Length & Task & $\\Delta$ over baseline & 95% CI & Statistically Significant \\\\\n",
      "\\midrule\n",
      "clmbr & 0 & ICU Admission & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Long LOS & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & 30-day Readmission & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Anemia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Hyperkalemia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Hypoglycemia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Hyponatremia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Thrombocytopenia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Acute MI & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Celiac & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Hyperlipidemia & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Hypertension & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Lupus & 0.000 & (0.000, 0.000) &  \\\\\n",
      "clmbr & 0 & Pancreatic Cancer & 0.000 & (0.000, 0.000) &  \\\\\n",
      "gpt2 & 512 & ICU Admission & 0.022 & (-0.005, 0.050) &  \\\\\n",
      "gpt2 & 512 & Long LOS & -0.002 & (-0.017, 0.012) &  \\\\\n",
      "gpt2 & 512 & 30-day Readmission & -0.002 & (-0.013, 0.009) &  \\\\\n",
      "gpt2 & 512 & Anemia & -0.003 & (-0.004, -0.002) & \\checkmark \\\\\n",
      "gpt2 & 512 & Hyperkalemia & 0.011 & (0.001, 0.021) & \\checkmark \\\\\n",
      "gpt2 & 512 & Hypoglycemia & -0.001 & (-0.014, 0.012) &  \\\\\n",
      "gpt2 & 512 & Hyponatremia & 0.037 & (0.028, 0.046) & \\checkmark \\\\\n",
      "gpt2 & 512 & Thrombocytopenia & 0.020 & (0.015, 0.025) & \\checkmark \\\\\n",
      "gpt2 & 512 & Acute MI & 0.001 & (-0.022, 0.027) &  \\\\\n",
      "gpt2 & 512 & Celiac & 0.181 & (0.063, 0.295) & \\checkmark \\\\\n",
      "gpt2 & 512 & Hyperlipidemia & -0.004 & (-0.047, 0.043) &  \\\\\n",
      "gpt2 & 512 & Hypertension & -0.003 & (-0.021, 0.014) &  \\\\\n",
      "gpt2 & 512 & Lupus & -0.031 & (-0.110, 0.050) &  \\\\\n",
      "gpt2 & 512 & Pancreatic Cancer & 0.014 & (-0.028, 0.054) &  \\\\\n",
      "gpt2 & 1024 & ICU Admission & -0.021 & (-0.052, 0.009) &  \\\\\n",
      "gpt2 & 1024 & Long LOS & -0.014 & (-0.032, 0.004) &  \\\\\n",
      "gpt2 & 1024 & 30-day Readmission & 0.004 & (-0.009, 0.015) &  \\\\\n",
      "gpt2 & 1024 & Anemia & -0.011 & (-0.012, -0.009) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Hyperkalemia & 0.022 & (0.011, 0.033) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Hypoglycemia & -0.009 & (-0.022, 0.004) &  \\\\\n",
      "gpt2 & 1024 & Hyponatremia & 0.037 & (0.028, 0.046) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Thrombocytopenia & 0.013 & (0.009, 0.019) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Acute MI & -0.003 & (-0.027, 0.021) &  \\\\\n",
      "gpt2 & 1024 & Celiac & 0.125 & (0.007, 0.274) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Hyperlipidemia & -0.008 & (-0.053, 0.036) &  \\\\\n",
      "gpt2 & 1024 & Hypertension & -0.026 & (-0.049, -0.005) & \\checkmark \\\\\n",
      "gpt2 & 1024 & Lupus & -0.016 & (-0.090, 0.062) &  \\\\\n",
      "gpt2 & 1024 & Pancreatic Cancer & 0.022 & (-0.009, 0.050) &  \\\\\n",
      "gpt2 & 2048 & ICU Admission & -0.010 & (-0.040, 0.021) &  \\\\\n",
      "gpt2 & 2048 & Long LOS & -0.008 & (-0.022, 0.006) &  \\\\\n",
      "gpt2 & 2048 & 30-day Readmission & 0.002 & (-0.011, 0.014) &  \\\\\n",
      "gpt2 & 2048 & Anemia & -0.004 & (-0.005, -0.003) & \\checkmark \\\\\n",
      "gpt2 & 2048 & Hyperkalemia & 0.007 & (-0.003, 0.017) &  \\\\\n",
      "gpt2 & 2048 & Hypoglycemia & 0.001 & (-0.013, 0.013) &  \\\\\n",
      "gpt2 & 2048 & Hyponatremia & 0.023 & (0.015, 0.029) & \\checkmark \\\\\n",
      "gpt2 & 2048 & Thrombocytopenia & 0.021 & (0.016, 0.027) & \\checkmark \\\\\n",
      "gpt2 & 2048 & Acute MI & -0.003 & (-0.030, 0.024) &  \\\\\n",
      "gpt2 & 2048 & Celiac & 0.227 & (0.037, 0.433) & \\checkmark \\\\\n",
      "gpt2 & 2048 & Hyperlipidemia & 0.005 & (-0.014, 0.025) &  \\\\\n",
      "gpt2 & 2048 & Hypertension & -0.002 & (-0.021, 0.017) &  \\\\\n",
      "gpt2 & 2048 & Lupus & 0.085 & (0.005, 0.165) & \\checkmark \\\\\n",
      "gpt2 & 2048 & Pancreatic Cancer & 0.004 & (-0.032, 0.037) &  \\\\\n",
      "gpt2 & 4096 & ICU Admission & 0.011 & (-0.021, 0.044) &  \\\\\n",
      "gpt2 & 4096 & Long LOS & -0.001 & (-0.014, 0.014) &  \\\\\n",
      "gpt2 & 4096 & 30-day Readmission & 0.004 & (-0.009, 0.015) &  \\\\\n",
      "gpt2 & 4096 & Anemia & -0.005 & (-0.006, -0.004) & \\checkmark \\\\\n",
      "gpt2 & 4096 & Hyperkalemia & 0.011 & (0.001, 0.021) & \\checkmark \\\\\n",
      "gpt2 & 4096 & Hypoglycemia & 0.003 & (-0.011, 0.015) &  \\\\\n",
      "gpt2 & 4096 & Hyponatremia & 0.046 & (0.036, 0.055) & \\checkmark \\\\\n",
      "gpt2 & 4096 & Thrombocytopenia & 0.014 & (0.009, 0.018) & \\checkmark \\\\\n",
      "gpt2 & 4096 & Acute MI & 0.006 & (-0.022, 0.033) &  \\\\\n",
      "gpt2 & 4096 & Celiac & 0.149 & (0.041, 0.278) & \\checkmark \\\\\n",
      "gpt2 & 4096 & Hyperlipidemia & 0.012 & (-0.018, 0.043) &  \\\\\n",
      "gpt2 & 4096 & Hypertension & 0.004 & (-0.015, 0.024) &  \\\\\n",
      "gpt2 & 4096 & Lupus & -0.008 & (-0.095, 0.088) &  \\\\\n",
      "gpt2 & 4096 & Pancreatic Cancer & 0.027 & (-0.008, 0.062) &  \\\\\n",
      "hyena & 1024 & ICU Admission & -0.026 & (-0.064, 0.013) &  \\\\\n",
      "hyena & 1024 & Long LOS & -0.006 & (-0.020, 0.011) &  \\\\\n",
      "hyena & 1024 & 30-day Readmission & -0.001 & (-0.012, 0.010) &  \\\\\n",
      "hyena & 1024 & Anemia & -0.002 & (-0.003, -0.001) & \\checkmark \\\\\n",
      "hyena & 1024 & Hyperkalemia & 0.026 & (0.015, 0.036) & \\checkmark \\\\\n",
      "hyena & 1024 & Hypoglycemia & -0.004 & (-0.015, 0.008) &  \\\\\n",
      "hyena & 1024 & Hyponatremia & 0.045 & (0.036, 0.055) & \\checkmark \\\\\n",
      "hyena & 1024 & Thrombocytopenia & 0.019 & (0.014, 0.024) & \\checkmark \\\\\n",
      "hyena & 1024 & Acute MI & 0.011 & (-0.015, 0.038) &  \\\\\n",
      "hyena & 1024 & Celiac & 0.224 & (0.095, 0.367) & \\checkmark \\\\\n",
      "hyena & 1024 & Hyperlipidemia & 0.018 & (-0.000, 0.037) &  \\\\\n",
      "hyena & 1024 & Hypertension & -0.026 & (-0.053, -0.003) & \\checkmark \\\\\n",
      "hyena & 1024 & Lupus & -0.026 & (-0.116, 0.055) &  \\\\\n",
      "hyena & 1024 & Pancreatic Cancer & 0.019 & (-0.022, 0.060) &  \\\\\n",
      "hyena & 4096 & ICU Admission & -0.026 & (-0.058, 0.004) &  \\\\\n",
      "hyena & 4096 & Long LOS & -0.012 & (-0.030, 0.006) &  \\\\\n",
      "hyena & 4096 & 30-day Readmission & 0.002 & (-0.012, 0.013) &  \\\\\n",
      "hyena & 4096 & Anemia & -0.005 & (-0.006, -0.004) & \\checkmark \\\\\n",
      "hyena & 4096 & Hyperkalemia & 0.022 & (0.013, 0.033) & \\checkmark \\\\\n",
      "hyena & 4096 & Hypoglycemia & -0.013 & (-0.027, 0.001) &  \\\\\n",
      "hyena & 4096 & Hyponatremia & 0.066 & (0.056, 0.078) & \\checkmark \\\\\n",
      "hyena & 4096 & Thrombocytopenia & 0.018 & (0.013, 0.023) & \\checkmark \\\\\n",
      "hyena & 4096 & Acute MI & 0.013 & (-0.013, 0.040) &  \\\\\n",
      "hyena & 4096 & Celiac & 0.216 & (0.077, 0.370) & \\checkmark \\\\\n",
      "hyena & 4096 & Hyperlipidemia & 0.023 & (-0.012, 0.057) &  \\\\\n",
      "hyena & 4096 & Hypertension & -0.023 & (-0.050, 0.002) &  \\\\\n",
      "hyena & 4096 & Lupus & -0.019 & (-0.110, 0.056) &  \\\\\n",
      "hyena & 4096 & Pancreatic Cancer & 0.038 & (-0.011, 0.092) &  \\\\\n",
      "hyena & 8192 & ICU Admission & -0.069 & (-0.106, -0.032) & \\checkmark \\\\\n",
      "hyena & 8192 & Long LOS & -0.023 & (-0.041, -0.004) & \\checkmark \\\\\n",
      "hyena & 8192 & 30-day Readmission & -0.017 & (-0.033, -0.002) & \\checkmark \\\\\n",
      "hyena & 8192 & Anemia & -0.016 & (-0.018, -0.014) & \\checkmark \\\\\n",
      "hyena & 8192 & Hyperkalemia & 0.010 & (0.000, 0.022) & \\checkmark \\\\\n",
      "hyena & 8192 & Hypoglycemia & -0.041 & (-0.056, -0.025) & \\checkmark \\\\\n",
      "hyena & 8192 & Hyponatremia & 0.049 & (0.039, 0.059) & \\checkmark \\\\\n",
      "hyena & 8192 & Thrombocytopenia & 0.005 & (-0.001, 0.010) &  \\\\\n",
      "hyena & 8192 & Acute MI & -0.009 & (-0.038, 0.022) &  \\\\\n",
      "hyena & 8192 & Celiac & 0.154 & (-0.013, 0.352) &  \\\\\n",
      "hyena & 8192 & Hyperlipidemia & 0.014 & (-0.026, 0.052) &  \\\\\n",
      "hyena & 8192 & Hypertension & -0.066 & (-0.108, -0.030) & \\checkmark \\\\\n",
      "hyena & 8192 & Lupus & -0.073 & (-0.189, 0.025) &  \\\\\n",
      "hyena & 8192 & Pancreatic Cancer & -0.033 & (-0.088, 0.018) &  \\\\\n",
      "hyena & 16384 & ICU Admission & -0.110 & (-0.147, -0.075) & \\checkmark \\\\\n",
      "hyena & 16384 & Long LOS & -0.048 & (-0.068, -0.029) & \\checkmark \\\\\n",
      "hyena & 16384 & 30-day Readmission & -0.048 & (-0.067, -0.026) & \\checkmark \\\\\n",
      "hyena & 16384 & Anemia & -0.047 & (-0.051, -0.043) & \\checkmark \\\\\n",
      "hyena & 16384 & Hyperkalemia & -0.038 & (-0.054, -0.023) & \\checkmark \\\\\n",
      "hyena & 16384 & Hypoglycemia & -0.093 & (-0.109, -0.075) & \\checkmark \\\\\n",
      "hyena & 16384 & Hyponatremia & 0.010 & (-0.002, 0.021) &  \\\\\n",
      "hyena & 16384 & Thrombocytopenia & 0.003 & (-0.005, 0.011) &  \\\\\n",
      "hyena & 16384 & Acute MI & -0.100 & (-0.145, -0.053) & \\checkmark \\\\\n",
      "hyena & 16384 & Celiac & 0.176 & (0.029, 0.318) & \\checkmark \\\\\n",
      "hyena & 16384 & Hyperlipidemia & -0.016 & (-0.069, 0.034) &  \\\\\n",
      "hyena & 16384 & Hypertension & -0.071 & (-0.125, -0.023) & \\checkmark \\\\\n",
      "hyena & 16384 & Lupus & -0.145 & (-0.268, -0.017) & \\checkmark \\\\\n",
      "hyena & 16384 & Pancreatic Cancer & -0.073 & (-0.148, 0.006) &  \\\\\n",
      "llama & 512 & ICU Admission & -0.018 & (-0.052, 0.015) &  \\\\\n",
      "llama & 512 & Long LOS & 0.002 & (-0.014, 0.017) &  \\\\\n",
      "llama & 512 & 30-day Readmission & 0.012 & (0.000, 0.024) & \\checkmark \\\\\n",
      "llama & 512 & Anemia & -0.004 & (-0.005, -0.003) & \\checkmark \\\\\n",
      "llama & 512 & Hyperkalemia & 0.012 & (0.004, 0.020) & \\checkmark \\\\\n",
      "llama & 512 & Hypoglycemia & -0.011 & (-0.022, 0.001) &  \\\\\n",
      "llama & 512 & Hyponatremia & -0.010 & (-0.016, -0.004) & \\checkmark \\\\\n",
      "llama & 512 & Thrombocytopenia & -0.001 & (-0.006, 0.004) &  \\\\\n",
      "llama & 512 & Acute MI & 0.015 & (-0.006, 0.037) &  \\\\\n",
      "llama & 512 & Celiac & 0.227 & (0.111, 0.356) & \\checkmark \\\\\n",
      "llama & 512 & Hyperlipidemia & 0.001 & (-0.018, 0.020) &  \\\\\n",
      "llama & 512 & Hypertension & -0.035 & (-0.057, -0.012) & \\checkmark \\\\\n",
      "llama & 512 & Lupus & 0.005 & (-0.084, 0.095) &  \\\\\n",
      "llama & 512 & Pancreatic Cancer & 0.001 & (-0.044, 0.046) &  \\\\\n",
      "llama & 1024 & ICU Admission & -0.005 & (-0.042, 0.032) &  \\\\\n",
      "llama & 1024 & Long LOS & -0.013 & (-0.034, 0.005) &  \\\\\n",
      "llama & 1024 & 30-day Readmission & 0.010 & (-0.002, 0.024) &  \\\\\n",
      "llama & 1024 & Anemia & -0.004 & (-0.005, -0.003) & \\checkmark \\\\\n",
      "llama & 1024 & Hyperkalemia & 0.010 & (0.002, 0.019) & \\checkmark \\\\\n",
      "llama & 1024 & Hypoglycemia & -0.003 & (-0.014, 0.008) &  \\\\\n",
      "llama & 1024 & Hyponatremia & -0.004 & (-0.010, 0.001) &  \\\\\n",
      "llama & 1024 & Thrombocytopenia & -0.005 & (-0.009, -0.000) & \\checkmark \\\\\n",
      "llama & 1024 & Acute MI & 0.007 & (-0.014, 0.029) &  \\\\\n",
      "llama & 1024 & Celiac & 0.250 & (0.149, 0.359) & \\checkmark \\\\\n",
      "llama & 1024 & Hyperlipidemia & 0.003 & (-0.016, 0.021) &  \\\\\n",
      "llama & 1024 & Hypertension & -0.014 & (-0.033, 0.003) &  \\\\\n",
      "llama & 1024 & Lupus & -0.014 & (-0.102, 0.079) &  \\\\\n",
      "llama & 1024 & Pancreatic Cancer & -0.007 & (-0.053, 0.037) &  \\\\\n",
      "llama & 2048 & ICU Admission & 0.005 & (-0.023, 0.033) &  \\\\\n",
      "llama & 2048 & Long LOS & 0.014 & (-0.003, 0.029) &  \\\\\n",
      "llama & 2048 & 30-day Readmission & 0.010 & (-0.003, 0.023) &  \\\\\n",
      "llama & 2048 & Anemia & -0.002 & (-0.003, -0.001) & \\checkmark \\\\\n",
      "llama & 2048 & Hyperkalemia & 0.015 & (0.005, 0.025) & \\checkmark \\\\\n",
      "llama & 2048 & Hypoglycemia & 0.011 & (-0.002, 0.023) &  \\\\\n",
      "llama & 2048 & Hyponatremia & 0.013 & (0.005, 0.020) & \\checkmark \\\\\n",
      "llama & 2048 & Thrombocytopenia & -0.000 & (-0.006, 0.004) &  \\\\\n",
      "llama & 2048 & Acute MI & 0.022 & (-0.001, 0.044) &  \\\\\n",
      "llama & 2048 & Celiac & 0.212 & (0.083, 0.343) & \\checkmark \\\\\n",
      "llama & 2048 & Hyperlipidemia & 0.021 & (-0.005, 0.049) &  \\\\\n",
      "llama & 2048 & Hypertension & -0.003 & (-0.025, 0.018) &  \\\\\n",
      "llama & 2048 & Lupus & 0.031 & (-0.049, 0.119) &  \\\\\n",
      "llama & 2048 & Pancreatic Cancer & 0.007 & (-0.042, 0.053) &  \\\\\n",
      "llama & 4096 & ICU Admission & -0.003 & (-0.026, 0.021) &  \\\\\n",
      "llama & 4096 & Long LOS & -0.004 & (-0.018, 0.010) &  \\\\\n",
      "llama & 4096 & 30-day Readmission & 0.013 & (0.002, 0.026) & \\checkmark \\\\\n",
      "llama & 4096 & Anemia & 0.001 & (0.000, 0.002) & \\checkmark \\\\\n",
      "llama & 4096 & Hyperkalemia & 0.024 & (0.016, 0.033) & \\checkmark \\\\\n",
      "llama & 4096 & Hypoglycemia & 0.012 & (-0.000, 0.022) &  \\\\\n",
      "llama & 4096 & Hyponatremia & 0.036 & (0.028, 0.046) & \\checkmark \\\\\n",
      "llama & 4096 & Thrombocytopenia & 0.000 & (-0.004, 0.005) &  \\\\\n",
      "llama & 4096 & Acute MI & 0.015 & (-0.008, 0.038) &  \\\\\n",
      "llama & 4096 & Celiac & 0.226 & (0.097, 0.365) & \\checkmark \\\\\n",
      "llama & 4096 & Hyperlipidemia & 0.016 & (-0.002, 0.036) &  \\\\\n",
      "llama & 4096 & Hypertension & 0.004 & (-0.013, 0.021) &  \\\\\n",
      "llama & 4096 & Lupus & -0.023 & (-0.097, 0.049) &  \\\\\n",
      "llama & 4096 & Pancreatic Cancer & -0.008 & (-0.056, 0.033) &  \\\\\n",
      "mamba & 1024 & ICU Admission & -0.009 & (-0.039, 0.019) &  \\\\\n",
      "mamba & 1024 & Long LOS & -0.003 & (-0.018, 0.010) &  \\\\\n",
      "mamba & 1024 & 30-day Readmission & 0.001 & (-0.010, 0.013) &  \\\\\n",
      "mamba & 1024 & Anemia & 0.000 & (-0.001, 0.001) &  \\\\\n",
      "mamba & 1024 & Hyperkalemia & 0.003 & (-0.006, 0.013) &  \\\\\n",
      "mamba & 1024 & Hypoglycemia & 0.001 & (-0.011, 0.013) &  \\\\\n",
      "mamba & 1024 & Hyponatremia & 0.014 & (0.007, 0.022) & \\checkmark \\\\\n",
      "mamba & 1024 & Thrombocytopenia & -0.005 & (-0.010, -0.001) & \\checkmark \\\\\n",
      "mamba & 1024 & Acute MI & 0.017 & (-0.007, 0.040) &  \\\\\n",
      "mamba & 1024 & Celiac & 0.102 & (-0.076, 0.262) &  \\\\\n",
      "mamba & 1024 & Hyperlipidemia & 0.020 & (-0.010, 0.050) &  \\\\\n",
      "mamba & 1024 & Hypertension & -0.011 & (-0.034, 0.011) &  \\\\\n",
      "mamba & 1024 & Lupus & -0.030 & (-0.115, 0.052) &  \\\\\n",
      "mamba & 1024 & Pancreatic Cancer & 0.032 & (-0.008, 0.071) &  \\\\\n",
      "mamba & 4096 & ICU Admission & 0.004 & (-0.024, 0.029) &  \\\\\n",
      "mamba & 4096 & Long LOS & 0.005 & (-0.010, 0.021) &  \\\\\n",
      "mamba & 4096 & 30-day Readmission & 0.006 & (-0.006, 0.017) &  \\\\\n",
      "mamba & 4096 & Anemia & 0.002 & (0.001, 0.003) & \\checkmark \\\\\n",
      "mamba & 4096 & Hyperkalemia & 0.024 & (0.014, 0.034) & \\checkmark \\\\\n",
      "mamba & 4096 & Hypoglycemia & 0.001 & (-0.012, 0.013) &  \\\\\n",
      "mamba & 4096 & Hyponatremia & 0.066 & (0.057, 0.075) & \\checkmark \\\\\n",
      "mamba & 4096 & Thrombocytopenia & 0.007 & (0.002, 0.011) & \\checkmark \\\\\n",
      "mamba & 4096 & Acute MI & 0.014 & (-0.009, 0.036) &  \\\\\n",
      "mamba & 4096 & Celiac & 0.198 & (0.115, 0.288) & \\checkmark \\\\\n",
      "mamba & 4096 & Hyperlipidemia & 0.015 & (-0.034, 0.057) &  \\\\\n",
      "mamba & 4096 & Hypertension & -0.010 & (-0.033, 0.010) &  \\\\\n",
      "mamba & 4096 & Lupus & -0.003 & (-0.091, 0.086) &  \\\\\n",
      "mamba & 4096 & Pancreatic Cancer & 0.049 & (0.017, 0.081) & \\checkmark \\\\\n",
      "mamba & 8192 & ICU Admission & -0.007 & (-0.033, 0.018) &  \\\\\n",
      "mamba & 8192 & Long LOS & 0.009 & (-0.006, 0.024) &  \\\\\n",
      "mamba & 8192 & 30-day Readmission & 0.003 & (-0.010, 0.016) &  \\\\\n",
      "mamba & 8192 & Anemia & 0.001 & (0.000, 0.002) & \\checkmark \\\\\n",
      "mamba & 8192 & Hyperkalemia & 0.018 & (0.008, 0.029) & \\checkmark \\\\\n",
      "mamba & 8192 & Hypoglycemia & -0.002 & (-0.014, 0.010) &  \\\\\n",
      "mamba & 8192 & Hyponatremia & 0.063 & (0.053, 0.072) & \\checkmark \\\\\n",
      "mamba & 8192 & Thrombocytopenia & 0.004 & (-0.001, 0.008) &  \\\\\n",
      "mamba & 8192 & Acute MI & 0.014 & (-0.008, 0.036) &  \\\\\n",
      "mamba & 8192 & Celiac & 0.173 & (0.083, 0.312) & \\checkmark \\\\\n",
      "mamba & 8192 & Hyperlipidemia & 0.030 & (-0.011, 0.068) &  \\\\\n",
      "mamba & 8192 & Hypertension & -0.016 & (-0.036, 0.003) &  \\\\\n",
      "mamba & 8192 & Lupus & 0.038 & (-0.029, 0.113) &  \\\\\n",
      "mamba & 8192 & Pancreatic Cancer & 0.027 & (-0.010, 0.062) &  \\\\\n",
      "mamba & 16384 & ICU Admission & 0.007 & (-0.028, 0.040) &  \\\\\n",
      "mamba & 16384 & Long LOS & 0.013 & (-0.005, 0.029) &  \\\\\n",
      "mamba & 16384 & 30-day Readmission & 0.005 & (-0.008, 0.017) &  \\\\\n",
      "mamba & 16384 & Anemia & 0.002 & (0.001, 0.003) & \\checkmark \\\\\n",
      "mamba & 16384 & Hyperkalemia & 0.030 & (0.019, 0.042) & \\checkmark \\\\\n",
      "mamba & 16384 & Hypoglycemia & 0.006 & (-0.006, 0.019) &  \\\\\n",
      "mamba & 16384 & Hyponatremia & 0.070 & (0.061, 0.079) & \\checkmark \\\\\n",
      "mamba & 16384 & Thrombocytopenia & 0.008 & (0.004, 0.013) & \\checkmark \\\\\n",
      "mamba & 16384 & Acute MI & 0.016 & (-0.005, 0.036) &  \\\\\n",
      "mamba & 16384 & Celiac & 0.194 & (0.108, 0.333) & \\checkmark \\\\\n",
      "mamba & 16384 & Hyperlipidemia & 0.023 & (-0.013, 0.058) &  \\\\\n",
      "mamba & 16384 & Hypertension & 0.003 & (-0.018, 0.023) &  \\\\\n",
      "mamba & 16384 & Lupus & 0.037 & (-0.056, 0.132) &  \\\\\n",
      "mamba & 16384 & Pancreatic Cancer & 0.053 & (0.024, 0.087) & \\checkmark \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task-level delta CIs for LaTeX\n",
    "latex: str = format_df_deltas_for_latex(df_task_deltas)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>strat</th>\n",
       "      <th>strat_col</th>\n",
       "      <th>tercile</th>\n",
       "      <th>win_rate_true</th>\n",
       "      <th>win_rate_ci_mean</th>\n",
       "      <th>win_rate_ci_025</th>\n",
       "      <th>win_rate_ci_500</th>\n",
       "      <th>win_rate_ci_975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>all</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>all</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>all</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>all</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>all</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>all</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>all</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>1</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.821714</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>all</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>2</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.751571</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>all</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.668786</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>all</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>2</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.699429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model task              strat  \\\n",
       "24                                              clmbr  all  inter_event_times   \n",
       "51                                              clmbr  all       n_gram_count   \n",
       "78                                              clmbr  all   timeline_lengths   \n",
       "25                                              clmbr  all  inter_event_times   \n",
       "52                                              clmbr  all       n_gram_count   \n",
       "..                                                ...  ...                ...   \n",
       "49  mamba-tiny-16384--clmbr_train-tokens-total_non...  all       n_gram_count   \n",
       "76  mamba-tiny-16384--clmbr_train-tokens-total_non...  all   timeline_lengths   \n",
       "23  mamba-tiny-16384--clmbr_train-tokens-total_non...  all  inter_event_times   \n",
       "50  mamba-tiny-16384--clmbr_train-tokens-total_non...  all       n_gram_count   \n",
       "77  mamba-tiny-16384--clmbr_train-tokens-total_non...  all   timeline_lengths   \n",
       "\n",
       "   strat_col  tercile  win_rate_true  win_rate_ci_mean  win_rate_ci_025  \\\n",
       "24       std        0       0.000000          0.000000         0.000000   \n",
       "51      rr_1        0       0.000000          0.000000         0.000000   \n",
       "78  n_events        0       0.000000          0.000000         0.000000   \n",
       "25       std        1       0.000000          0.000000         0.000000   \n",
       "52      rr_1        1       0.000000          0.000000         0.000000   \n",
       "..       ...      ...            ...               ...              ...   \n",
       "49      rr_1        1       0.785714          0.790500         0.642857   \n",
       "76  n_events        1       0.928571          0.821714         0.642857   \n",
       "23       std        2       0.857143          0.751571         0.642857   \n",
       "50      rr_1        2       0.642857          0.668786         0.500000   \n",
       "77  n_events        2       0.642857          0.699429         0.500000   \n",
       "\n",
       "    win_rate_ci_500  win_rate_ci_975  \n",
       "24         0.000000         0.000000  \n",
       "51         0.000000         0.000000  \n",
       "78         0.000000         0.000000  \n",
       "25         0.000000         0.000000  \n",
       "52         0.000000         0.000000  \n",
       "..              ...              ...  \n",
       "49         0.785714         0.928571  \n",
       "76         0.857143         0.928571  \n",
       "23         0.785714         0.857143  \n",
       "50         0.642857         0.857143  \n",
       "77         0.714286         0.857143  \n",
       "\n",
       "[81 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model-level win rates between longest and shortest context lengths for each (model, tercile)\n",
    "df_win_rate_cis = []\n",
    "for strat, strat_cols in strats.items():\n",
    "    for strat_col in strat_cols:\n",
    "        for model in valid_models:\n",
    "\n",
    "            # Only keep shortest and longest context lengths for ease of comparison\n",
    "            if model_to_base(model) in ['mamba', 'hyena' ]:\n",
    "                if model_to_ctx(model) not in [1024, 16384]:\n",
    "                    continue\n",
    "            if model_to_base(model) in ['gpt2', 'llama' ]:\n",
    "                if model_to_ctx(model) not in [512, 4096]:\n",
    "                    continue\n",
    "            baseline_model_name: str = model.replace(str(model_to_ctx(model)), str(1024 if model_to_base(model) in ['mamba', 'hyena' ] else 512))\n",
    "\n",
    "            for tercile in range(3):\n",
    "                win_rates = [] # scores for this tercile, averaged across all tasks\n",
    "                true_win_rate = [] # true scores for this tercile, averaged across all tasks\n",
    "                for task in valid_tasks:\n",
    "                    ## NOTE: Comparison is '>' b/c lower Brier is better\n",
    "                    # True win rate\n",
    "                    true_this_model_score = np.array(true_brier_scores_per_tercile[(model, task, strat, strat_col, tercile)]) # each is 1 x 1\n",
    "                    true_baseline_model_score = np.array(true_brier_scores_per_tercile[(baseline_model_name, task, strat, strat_col, tercile)]) # each is 1 x 1\n",
    "                    assert true_baseline_model_score.shape == true_this_model_score.shape\n",
    "                    true_win_rate.append((true_baseline_model_score > true_this_model_score).astype(bool))\n",
    "                    # Bootstrap win rates\n",
    "                    this_model_scores = np.array(brier_scores_per_tercile[(model, task, strat, strat_col, tercile)]) # each is 1 x 1000\n",
    "                    baseline_model_scores = np.array(brier_scores_per_tercile[(\n",
    "                        baseline_model_name, \n",
    "                        task, strat, strat_col, tercile\n",
    "                    )]) # each is 1 x 1000\n",
    "                    assert baseline_model_scores.shape == this_model_scores.shape\n",
    "                    win_rates.append((baseline_model_scores > this_model_scores).astype(bool))\n",
    "                # Win rates\n",
    "                win_rates = np.vstack(win_rates) # 14 x 1000\n",
    "                assert win_rates.shape == (14, 1000), f\"win_rates.shape={win_rates.shape}\"\n",
    "                win_rates = np.mean(win_rates, axis=0)\n",
    "                assert len(true_win_rate) == 14\n",
    "                df_win_rate_cis.append({\n",
    "                    'model' : model,\n",
    "                    'task' : 'all',\n",
    "                    'strat' : strat,\n",
    "                    'strat_col' : strat_col,\n",
    "                    'tercile' : tercile,\n",
    "                    'win_rate_true' : np.mean(true_win_rate),\n",
    "                    'win_rate_ci_mean' : np.mean(win_rates),\n",
    "                    'win_rate_ci_025' : np.percentile(win_rates, 2.5),\n",
    "                    'win_rate_ci_500' : np.percentile(win_rates, 50),\n",
    "                    'win_rate_ci_975' : np.percentile(win_rates, 97.5),\n",
    "                })\n",
    "df_win_rate_cis = pd.DataFrame(df_win_rate_cis).sort_values(['model', 'tercile'])\n",
    "df_win_rate_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tercile</th>\n",
       "      <th>model</th>\n",
       "      <th>model_base</th>\n",
       "      <th>ctx_length</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-512--clmbr_train-tokens-total_nonPAD...</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-4096--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.4286 (0.1429, 0.5714)</td>\n",
       "      <td>0.5000 (0.2857, 0.6429)</td>\n",
       "      <td>0.5000 (0.2857, 0.7143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hyena-large-1024--clmbr_train-tokens-total_non...</td>\n",
       "      <td>hyena</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-16384--clmbr_train-tokens-total_no...</td>\n",
       "      <td>hyena</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.1429 (0.0714, 0.2143)</td>\n",
       "      <td>0.0714 (0.0000, 0.2857)</td>\n",
       "      <td>0.0714 (0.0000, 0.2857)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.5000 (0.5000, 0.7857)</td>\n",
       "      <td>0.6429 (0.5000, 0.8571)</td>\n",
       "      <td>0.7857 (0.5714, 0.9286)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.8571 (0.6429, 0.9286)</td>\n",
       "      <td>0.7857 (0.6429, 0.9286)</td>\n",
       "      <td>0.6429 (0.5000, 0.8571)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tercile                                              model model_base  \\\n",
       "0                                                    clmbr      clmbr   \n",
       "2        gpt2-base-512--clmbr_train-tokens-total_nonPAD...       gpt2   \n",
       "1        gpt2-base-4096--clmbr_train-tokens-total_nonPA...       gpt2   \n",
       "3        hyena-large-1024--clmbr_train-tokens-total_non...      hyena   \n",
       "4        hyena-large-16384--clmbr_train-tokens-total_no...      hyena   \n",
       "6        llama-base-512--clmbr_train-tokens-total_nonPA...      llama   \n",
       "5        llama-base-4096--clmbr_train-tokens-total_nonP...      llama   \n",
       "7        mamba-tiny-1024--clmbr_train-tokens-total_nonP...      mamba   \n",
       "8        mamba-tiny-16384--clmbr_train-tokens-total_non...      mamba   \n",
       "\n",
       "tercile  ctx_length                        0                        1  \\\n",
       "0                 0  0.0000 (0.0000, 0.0000)  0.0000 (0.0000, 0.0000)   \n",
       "2               512  0.0000 (0.0000, 0.0000)  0.0000 (0.0000, 0.0000)   \n",
       "1              4096  0.4286 (0.1429, 0.5714)  0.5000 (0.2857, 0.6429)   \n",
       "3              1024  0.0000 (0.0000, 0.0000)  0.0000 (0.0000, 0.0000)   \n",
       "4             16384  0.1429 (0.0714, 0.2143)  0.0714 (0.0000, 0.2857)   \n",
       "6               512  0.0000 (0.0000, 0.0000)  0.0000 (0.0000, 0.0000)   \n",
       "5              4096  0.5000 (0.5000, 0.7857)  0.6429 (0.5000, 0.8571)   \n",
       "7              1024  0.0000 (0.0000, 0.0000)  0.0000 (0.0000, 0.0000)   \n",
       "8             16384  0.8571 (0.6429, 0.9286)  0.7857 (0.6429, 0.9286)   \n",
       "\n",
       "tercile                        2  \n",
       "0        0.0000 (0.0000, 0.0000)  \n",
       "2        0.0000 (0.0000, 0.0000)  \n",
       "1        0.5000 (0.2857, 0.7143)  \n",
       "3        0.0000 (0.0000, 0.0000)  \n",
       "4        0.0714 (0.0000, 0.2857)  \n",
       "6        0.0000 (0.0000, 0.0000)  \n",
       "5        0.7857 (0.5714, 0.9286)  \n",
       "7        0.0000 (0.0000, 0.0000)  \n",
       "8        0.6429 (0.5000, 0.8571)  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repetition = clean_df_briers(df_win_rate_cis[df_win_rate_cis['strat_col'] == 'rr_1'])\n",
    "df_repetition = df_repetition.pivot(index=['model', 'model_base', 'ctx_length'], columns='tercile', values='formatted_win_rate').reset_index().sort_values(['model_base', 'ctx_length'])\n",
    "df_repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tercile</th>\n",
       "      <th>model</th>\n",
       "      <th>model_base</th>\n",
       "      <th>ctx_length</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clmbr</td>\n",
       "      <td>clmbr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-base-512--clmbr_train-tokens-total_nonPAD...</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-base-4096--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.5000 (0.2143, 0.7143)</td>\n",
       "      <td>0.5000 (0.3571, 0.7857)</td>\n",
       "      <td>0.4286 (0.2143, 0.5714)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hyena-large-1024--clmbr_train-tokens-total_non...</td>\n",
       "      <td>hyena</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyena-large-16384--clmbr_train-tokens-total_no...</td>\n",
       "      <td>hyena</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.1429 (0.0714, 0.3571)</td>\n",
       "      <td>0.0000 (0.0000, 0.2143)</td>\n",
       "      <td>0.1429 (0.0714, 0.2143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.7143 (0.5000, 0.9286)</td>\n",
       "      <td>0.7857 (0.5000, 0.8571)</td>\n",
       "      <td>0.7143 (0.5000, 0.8571)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "      <td>0.0000 (0.0000, 0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.8571 (0.5714, 0.8571)</td>\n",
       "      <td>0.8571 (0.6429, 0.9286)</td>\n",
       "      <td>0.8571 (0.6429, 0.8571)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tercile                                              model model_base  \\\n",
       "0                                                    clmbr      clmbr   \n",
       "2        gpt2-base-512--clmbr_train-tokens-total_nonPAD...       gpt2   \n",
       "1        gpt2-base-4096--clmbr_train-tokens-total_nonPA...       gpt2   \n",
       "3        hyena-large-1024--clmbr_train-tokens-total_non...      hyena   \n",
       "4        hyena-large-16384--clmbr_train-tokens-total_no...      hyena   \n",
       "6        llama-base-512--clmbr_train-tokens-total_nonPA...      llama   \n",
       "5        llama-base-4096--clmbr_train-tokens-total_nonP...      llama   \n",
       "7        mamba-tiny-1024--clmbr_train-tokens-total_nonP...      mamba   \n",
       "8        mamba-tiny-16384--clmbr_train-tokens-total_non...      mamba   \n",
       "\n",
       "tercile  ctx_length                        0                        1  \\\n",
       "0                 0  0.0000 (0.0000, 0.0000)  0.0000 (0.0000, 0.0000)   \n",
       "2               512  0.0000 (0.0000, 0.0000)  0.0000 (0.0000, 0.0000)   \n",
       "1              4096  0.5000 (0.2143, 0.7143)  0.5000 (0.3571, 0.7857)   \n",
       "3              1024  0.0000 (0.0000, 0.0000)  0.0000 (0.0000, 0.0000)   \n",
       "4             16384  0.1429 (0.0714, 0.3571)  0.0000 (0.0000, 0.2143)   \n",
       "6               512  0.0000 (0.0000, 0.0000)  0.0000 (0.0000, 0.0000)   \n",
       "5              4096  0.7143 (0.5000, 0.9286)  0.7857 (0.5000, 0.8571)   \n",
       "7              1024  0.0000 (0.0000, 0.0000)  0.0000 (0.0000, 0.0000)   \n",
       "8             16384  0.8571 (0.5714, 0.8571)  0.8571 (0.6429, 0.9286)   \n",
       "\n",
       "tercile                        2  \n",
       "0        0.0000 (0.0000, 0.0000)  \n",
       "2        0.0000 (0.0000, 0.0000)  \n",
       "1        0.4286 (0.2143, 0.5714)  \n",
       "3        0.0000 (0.0000, 0.0000)  \n",
       "4        0.1429 (0.0714, 0.2143)  \n",
       "6        0.0000 (0.0000, 0.0000)  \n",
       "5        0.7143 (0.5000, 0.8571)  \n",
       "7        0.0000 (0.0000, 0.0000)  \n",
       "8        0.8571 (0.6429, 0.8571)  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_irregularity = clean_df_briers(df_win_rate_cis[df_win_rate_cis['strat_col'] == 'std'])\n",
    "# Take the 'quartile' column and give each unique value its own column. Use the `formatted_brier` column as the values.\n",
    "df_irregularity = df_irregularity.pivot(index=['model', 'model_base', 'ctx_length'], columns='tercile', values='formatted_win_rate').reset_index().sort_values(['model_base', 'ctx_length'])\n",
    "df_irregularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate t-test scores for each (model, task, tercile)\n",
    "raw_brier_scores_per_tercile = collections.defaultdict(list) # [key] = (model, task, strat, strat_col, tercile), [value] = brier scores across all labels for this task and quaratile\n",
    "if IS_LOAD_FROM_CACHE and os.path.exists('../cache/raw_brier_scores_per_tercile.pkl'):\n",
    "    print(\"Loading raw_brier_scores_per_tercile from cache...\")\n",
    "    raw_brier_scores_per_tercile = pickle.load(open('../cache/raw_brier_scores_per_tercile.pkl', 'rb'))\n",
    "else:\n",
    "    # Add metrics to each (model, task) predictions\n",
    "    for (model, task), df_preds in tqdm(predictions.items(), total=len(predictions)):\n",
    "        # Add (patient ID, label time) to df_preds to align with terciles\n",
    "        df_preds['pid'] = label_data[task]['patient_ids']\n",
    "        df_preds['label_time'] = label_data[task]['times']\n",
    "\n",
    "        # Test split\n",
    "        df_preds = df_preds[df_preds['split'] == 'test']\n",
    "\n",
    "        # Merge patient IDs\n",
    "        for strat, strat_cols in strats.items():\n",
    "            df_metrics = pd.read_parquet(os.path.join(path_to_ehrshot_metrics_dir, f'df__{task}__{strat}__metrics.parquet'))\n",
    "\n",
    "            # If stratifying by inter-event times, need to pivot table since 'time' and 'metric' are separate columns\n",
    "            if strat == 'inter_event_times':\n",
    "                df_metrics = df_metrics.pivot_table(index=['pid', 'pid_idx', 'label_time', 'sub_task'], columns='metric', values='time').reset_index()\n",
    "\n",
    "            # Merge metrics with predictions\n",
    "            df_ = pd.merge(df_preds, df_metrics, on=['pid', 'label_time'])\n",
    "            if df_.shape[0] != df_preds.shape[0]:\n",
    "                print(f'{model} | {task} | {strat} | Number of rows in df does not match number of rows in df_preds: {df_.shape[0]} != {df_preds.shape[0]}')\n",
    "\n",
    "            for strat_col in strat_cols:\n",
    "                if strat_col not in df_metrics.columns:\n",
    "                    raise ValueError(f'col={strat_col} not in df_metrics columns for strat={strat}.')\n",
    "\n",
    "                # Metric values\n",
    "                metric_values = df_[strat_col]\n",
    "\n",
    "                # Calculate t-test values against each model for this task\n",
    "                df_['tercile'] = pd.qcut(df_[strat_col].fillna(0).rank(method='min'), 3, labels=False)\n",
    "                assert set(df_['tercile'].unique()) == {0, 1, 2}, f'terciles not 0, 1, 2: {set(df_[\"tercile\"].unique())}'\n",
    "                for tercile in range(3):\n",
    "                    # Limit to this tercile\n",
    "                    df_tercile = df_[df_['tercile'] == tercile]\n",
    "                    # Labels / Preds\n",
    "                    y = df_tercile['y'].values.astype(int)\n",
    "                    pred_proba = df_tercile['pred_proba'].values\n",
    "                    # Calculate brier score per label\n",
    "                    brier_scores = (y - pred_proba) ** 2\n",
    "                    raw_brier_scores_per_tercile[(model, task, strat, strat_col, tercile)] = brier_scores\n",
    "\n",
    "    # Save results to .pkl file\n",
    "    with open('../cache/raw_brier_scores_per_tercile.pkl', 'wb') as f:\n",
    "        pickle.dump(raw_brier_scores_per_tercile, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2856/2856 [00:14<00:00, 199.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>task</th>\n",
       "      <th>strat</th>\n",
       "      <th>strat_col</th>\n",
       "      <th>quartile</th>\n",
       "      <th>t_test</th>\n",
       "      <th>p_value</th>\n",
       "      <th>model1_name</th>\n",
       "      <th>model1_base</th>\n",
       "      <th>model1_ctx_length</th>\n",
       "      <th>model2_name</th>\n",
       "      <th>model2_base</th>\n",
       "      <th>model2_ctx_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>llama-base-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514707</td>\n",
       "      <td>0.606939</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>llama</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>llama-base-2048--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0</td>\n",
       "      <td>0.772041</td>\n",
       "      <td>0.440381</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>llama</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0</td>\n",
       "      <td>0.631369</td>\n",
       "      <td>0.528029</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>llama-base-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>1</td>\n",
       "      <td>0.516850</td>\n",
       "      <td>0.605443</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>llama-base-1024</td>\n",
       "      <td>llama</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>llama-base-2048--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>1</td>\n",
       "      <td>0.429320</td>\n",
       "      <td>0.667838</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "      <td>llama-base-2048</td>\n",
       "      <td>llama</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8059</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>2</td>\n",
       "      <td>0.049866</td>\n",
       "      <td>0.960239</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>mamba</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8060</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>2</td>\n",
       "      <td>0.077202</td>\n",
       "      <td>0.938478</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>mamba</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8061</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.326128</td>\n",
       "      <td>0.744395</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8062</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.053720</td>\n",
       "      <td>0.957168</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-4096</td>\n",
       "      <td>mamba</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8063</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-8192--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>3</td>\n",
       "      <td>0.061019</td>\n",
       "      <td>0.951356</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-8192</td>\n",
       "      <td>mamba</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8064 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 model1  \\\n",
       "0     llama-base-512--clmbr_train-tokens-total_nonPA...   \n",
       "1     llama-base-512--clmbr_train-tokens-total_nonPA...   \n",
       "2     llama-base-512--clmbr_train-tokens-total_nonPA...   \n",
       "3     llama-base-512--clmbr_train-tokens-total_nonPA...   \n",
       "4     llama-base-512--clmbr_train-tokens-total_nonPA...   \n",
       "...                                                 ...   \n",
       "8059  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "8060  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "8061  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "8062  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "8063  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "\n",
       "                                                 model2              task  \\\n",
       "0     llama-base-1024--clmbr_train-tokens-total_nonP...  new_hypertension   \n",
       "1     llama-base-2048--clmbr_train-tokens-total_nonP...  new_hypertension   \n",
       "2     llama-base-4096--clmbr_train-tokens-total_nonP...  new_hypertension   \n",
       "3     llama-base-1024--clmbr_train-tokens-total_nonP...  new_hypertension   \n",
       "4     llama-base-2048--clmbr_train-tokens-total_nonP...  new_hypertension   \n",
       "...                                                 ...               ...   \n",
       "8059  mamba-tiny-4096--clmbr_train-tokens-total_nonP...           guo_icu   \n",
       "8060  mamba-tiny-8192--clmbr_train-tokens-total_nonP...           guo_icu   \n",
       "8061  mamba-tiny-1024--clmbr_train-tokens-total_nonP...           guo_icu   \n",
       "8062  mamba-tiny-4096--clmbr_train-tokens-total_nonP...           guo_icu   \n",
       "8063  mamba-tiny-8192--clmbr_train-tokens-total_nonP...           guo_icu   \n",
       "\n",
       "                  strat strat_col  quartile    t_test   p_value  \\\n",
       "0     inter_event_times       std         0  0.514707  0.606939   \n",
       "1     inter_event_times       std         0  0.772041  0.440381   \n",
       "2     inter_event_times       std         0  0.631369  0.528029   \n",
       "3     inter_event_times       std         1  0.516850  0.605443   \n",
       "4     inter_event_times       std         1  0.429320  0.667838   \n",
       "...                 ...       ...       ...       ...       ...   \n",
       "8059   timeline_lengths  n_events         2  0.049866  0.960239   \n",
       "8060   timeline_lengths  n_events         2  0.077202  0.938478   \n",
       "8061   timeline_lengths  n_events         3 -0.326128  0.744395   \n",
       "8062   timeline_lengths  n_events         3 -0.053720  0.957168   \n",
       "8063   timeline_lengths  n_events         3  0.061019  0.951356   \n",
       "\n",
       "           model1_name model1_base  model1_ctx_length      model2_name  \\\n",
       "0       llama-base-512       llama                512  llama-base-1024   \n",
       "1       llama-base-512       llama                512  llama-base-2048   \n",
       "2       llama-base-512       llama                512  llama-base-4096   \n",
       "3       llama-base-512       llama                512  llama-base-1024   \n",
       "4       llama-base-512       llama                512  llama-base-2048   \n",
       "...                ...         ...                ...              ...   \n",
       "8059  mamba-tiny-16384       mamba              16384  mamba-tiny-4096   \n",
       "8060  mamba-tiny-16384       mamba              16384  mamba-tiny-8192   \n",
       "8061  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "8062  mamba-tiny-16384       mamba              16384  mamba-tiny-4096   \n",
       "8063  mamba-tiny-16384       mamba              16384  mamba-tiny-8192   \n",
       "\n",
       "     model2_base  model2_ctx_length  \n",
       "0          llama               1024  \n",
       "1          llama               2048  \n",
       "2          llama               4096  \n",
       "3          llama               1024  \n",
       "4          llama               2048  \n",
       "...          ...                ...  \n",
       "8059       mamba               4096  \n",
       "8060       mamba               8192  \n",
       "8061       mamba               1024  \n",
       "8062       mamba               4096  \n",
       "8063       mamba               8192  \n",
       "\n",
       "[8064 rows x 14 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run t-test for each (model, task, strat, strat_col tercile) against every other model for that task and tercile\n",
    "df_t_test_results = []\n",
    "if IS_LOAD_FROM_CACHE and os.path.exists('../cache/df_t_test_results.pkl'):\n",
    "    print(\"Loading df_t_test_results from cache...\")\n",
    "    df_t_test_results = pickle.load(open('../cache/df_t_test_results.pkl', 'rb'))\n",
    "else:\n",
    "    for (model1, task, strat, strat_col, tercile), brier_scores in tqdm(raw_brier_scores_per_tercile.items(), total=len(raw_brier_scores_per_tercile)):\n",
    "        for model2 in valid_models:\n",
    "            if model1 == model2:\n",
    "                continue\n",
    "            if model1[:3] != model2[:3]:\n",
    "                # Only compare models of the same base\n",
    "                continue\n",
    "            brier_scores2 = raw_brier_scores_per_tercile[(model2, task, strat, strat_col, tercile)]\n",
    "            t_test, p_value = scipy.stats.ttest_ind(brier_scores, brier_scores2)\n",
    "            df_t_test_results.append({\n",
    "                'model1': model1,\n",
    "                'model2': model2,\n",
    "                'task': task,\n",
    "                'strat': strat,\n",
    "                'strat_col': strat_col,\n",
    "                'tercile': tercile,\n",
    "                't_test': t_test,\n",
    "                'p_value': p_value,\n",
    "            })\n",
    "df_t_test_results = pd.DataFrame(df_t_test_results)\n",
    "df_t_test_results['model1_name'] = df_t_test_results['model1'].apply(model_to_name)\n",
    "df_t_test_results['model1_base'] = df_t_test_results['model1'].apply(model_to_base)\n",
    "df_t_test_results['model1_ctx_length'] = df_t_test_results['model1'].apply(model_to_ctx).astype(int)\n",
    "df_t_test_results['model2_name'] = df_t_test_results['model2'].apply(model_to_name)\n",
    "df_t_test_results['model2_base'] = df_t_test_results['model2'].apply(model_to_base)\n",
    "df_t_test_results['model2_ctx_length'] = df_t_test_results['model2'].apply(model_to_ctx).astype(int)\n",
    "df_t_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>task</th>\n",
       "      <th>strat</th>\n",
       "      <th>strat_col</th>\n",
       "      <th>quartile</th>\n",
       "      <th>t_test</th>\n",
       "      <th>p_value</th>\n",
       "      <th>model1_name</th>\n",
       "      <th>model1_base</th>\n",
       "      <th>model1_ctx_length</th>\n",
       "      <th>model2_name</th>\n",
       "      <th>model2_base</th>\n",
       "      <th>model2_ctx_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.631369</td>\n",
       "      <td>0.528029</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.457260</td>\n",
       "      <td>0.647643</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.254339</td>\n",
       "      <td>0.799317</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.193799</td>\n",
       "      <td>0.846396</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>new_hypertension</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.416442</td>\n",
       "      <td>0.677229</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>0.898390</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8052</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.446571</td>\n",
       "      <td>0.655280</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8055</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.225352</td>\n",
       "      <td>0.821751</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8058</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.016608</td>\n",
       "      <td>0.986752</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8061</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>guo_icu</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.326128</td>\n",
       "      <td>0.744395</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 model1  \\\n",
       "108   llama-base-4096--clmbr_train-tokens-total_nonP...   \n",
       "111   llama-base-4096--clmbr_train-tokens-total_nonP...   \n",
       "114   llama-base-4096--clmbr_train-tokens-total_nonP...   \n",
       "117   llama-base-4096--clmbr_train-tokens-total_nonP...   \n",
       "120   llama-base-4096--clmbr_train-tokens-total_nonP...   \n",
       "...                                                 ...   \n",
       "8049  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "8052  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "8055  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "8058  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "8061  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "\n",
       "                                                 model2              task  \\\n",
       "108   llama-base-512--clmbr_train-tokens-total_nonPA...  new_hypertension   \n",
       "111   llama-base-512--clmbr_train-tokens-total_nonPA...  new_hypertension   \n",
       "114   llama-base-512--clmbr_train-tokens-total_nonPA...  new_hypertension   \n",
       "117   llama-base-512--clmbr_train-tokens-total_nonPA...  new_hypertension   \n",
       "120   llama-base-512--clmbr_train-tokens-total_nonPA...  new_hypertension   \n",
       "...                                                 ...               ...   \n",
       "8049  mamba-tiny-1024--clmbr_train-tokens-total_nonP...           guo_icu   \n",
       "8052  mamba-tiny-1024--clmbr_train-tokens-total_nonP...           guo_icu   \n",
       "8055  mamba-tiny-1024--clmbr_train-tokens-total_nonP...           guo_icu   \n",
       "8058  mamba-tiny-1024--clmbr_train-tokens-total_nonP...           guo_icu   \n",
       "8061  mamba-tiny-1024--clmbr_train-tokens-total_nonP...           guo_icu   \n",
       "\n",
       "                  strat strat_col  quartile    t_test   p_value  \\\n",
       "108   inter_event_times       std         0 -0.631369  0.528029   \n",
       "111   inter_event_times       std         1 -0.457260  0.647643   \n",
       "114   inter_event_times       std         2 -0.254339  0.799317   \n",
       "117   inter_event_times       std         3 -0.193799  0.846396   \n",
       "120        n_gram_count      rr_1         0 -0.416442  0.677229   \n",
       "...                 ...       ...       ...       ...       ...   \n",
       "8049       n_gram_count      rr_1         3 -0.127728  0.898390   \n",
       "8052   timeline_lengths  n_events         0 -0.446571  0.655280   \n",
       "8055   timeline_lengths  n_events         1 -0.225352  0.821751   \n",
       "8058   timeline_lengths  n_events         2 -0.016608  0.986752   \n",
       "8061   timeline_lengths  n_events         3 -0.326128  0.744395   \n",
       "\n",
       "           model1_name model1_base  model1_ctx_length      model2_name  \\\n",
       "108    llama-base-4096       llama               4096   llama-base-512   \n",
       "111    llama-base-4096       llama               4096   llama-base-512   \n",
       "114    llama-base-4096       llama               4096   llama-base-512   \n",
       "117    llama-base-4096       llama               4096   llama-base-512   \n",
       "120    llama-base-4096       llama               4096   llama-base-512   \n",
       "...                ...         ...                ...              ...   \n",
       "8049  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "8052  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "8055  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "8058  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "8061  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "\n",
       "     model2_base  model2_ctx_length  \n",
       "108        llama                512  \n",
       "111        llama                512  \n",
       "114        llama                512  \n",
       "117        llama                512  \n",
       "120        llama                512  \n",
       "...          ...                ...  \n",
       "8049       mamba               1024  \n",
       "8052       mamba               1024  \n",
       "8055       mamba               1024  \n",
       "8058       mamba               1024  \n",
       "8061       mamba               1024  \n",
       "\n",
       "[672 rows x 14 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t_test_results_high_low = df_t_test_results[\n",
    "    (\n",
    "        (df_t_test_results['model1_base'].isin(['mamba', 'hyena']) & df_t_test_results['model1_ctx_length'].isin([16384]))\n",
    "        | (df_t_test_results['model1_base'].isin(['gpt2', 'llama']) & df_t_test_results['model1_ctx_length'].isin([4096]))\n",
    "    )\n",
    "    & \n",
    "    (\n",
    "        (df_t_test_results['model2_base'].isin(['mamba', 'hyena']) & df_t_test_results['model2_ctx_length'].isin([1024,]))\n",
    "        | (df_t_test_results['model2_base'].isin(['gpt2', 'llama']) & df_t_test_results['model2_ctx_length'].isin([512,]))\n",
    "    )\n",
    "]\n",
    "df_t_test_results_high_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>task</th>\n",
       "      <th>strat</th>\n",
       "      <th>strat_col</th>\n",
       "      <th>quartile</th>\n",
       "      <th>t_test</th>\n",
       "      <th>p_value</th>\n",
       "      <th>model1_name</th>\n",
       "      <th>model1_base</th>\n",
       "      <th>model1_ctx_length</th>\n",
       "      <th>model2_name</th>\n",
       "      <th>model2_base</th>\n",
       "      <th>model2_ctx_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_lupus</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0</td>\n",
       "      <td>2.175077</td>\n",
       "      <td>2.983300e-02</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_lupus</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>3</td>\n",
       "      <td>2.715803</td>\n",
       "      <td>6.713266e-03</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>lab_hyponatremia</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.210958</td>\n",
       "      <td>1.914469e-24</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>lab_hyponatremia</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.703955</td>\n",
       "      <td>1.353396e-14</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>llama-base-4096--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>llama-base-512--clmbr_train-tokens-total_nonPA...</td>\n",
       "      <td>lab_hyponatremia</td>\n",
       "      <td>inter_event_times</td>\n",
       "      <td>std</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.391177</td>\n",
       "      <td>1.487361e-13</td>\n",
       "      <td>llama-base-4096</td>\n",
       "      <td>llama</td>\n",
       "      <td>4096</td>\n",
       "      <td>llama-base-512</td>\n",
       "      <td>llama</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6897</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_celiac</td>\n",
       "      <td>n_gram_count</td>\n",
       "      <td>rr_1</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.369788</td>\n",
       "      <td>1.796855e-02</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6900</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_celiac</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.865597</td>\n",
       "      <td>1.172455e-04</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6903</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_celiac</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.989941</td>\n",
       "      <td>2.851927e-03</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_celiac</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.657793</td>\n",
       "      <td>2.663531e-04</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>mamba-tiny-16384--clmbr_train-tokens-total_non...</td>\n",
       "      <td>mamba-tiny-1024--clmbr_train-tokens-total_nonP...</td>\n",
       "      <td>new_celiac</td>\n",
       "      <td>timeline_lengths</td>\n",
       "      <td>n_events</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.718275</td>\n",
       "      <td>6.664550e-03</td>\n",
       "      <td>mamba-tiny-16384</td>\n",
       "      <td>mamba</td>\n",
       "      <td>16384</td>\n",
       "      <td>mamba-tiny-1024</td>\n",
       "      <td>mamba</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 model1  \\\n",
       "2268  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "2301  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "2412  llama-base-4096--clmbr_train-tokens-total_nonP...   \n",
       "2415  llama-base-4096--clmbr_train-tokens-total_nonP...   \n",
       "2418  llama-base-4096--clmbr_train-tokens-total_nonP...   \n",
       "...                                                 ...   \n",
       "6897  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "6900  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "6903  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "6906  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "6909  mamba-tiny-16384--clmbr_train-tokens-total_non...   \n",
       "\n",
       "                                                 model2              task  \\\n",
       "2268  mamba-tiny-1024--clmbr_train-tokens-total_nonP...         new_lupus   \n",
       "2301  mamba-tiny-1024--clmbr_train-tokens-total_nonP...         new_lupus   \n",
       "2412  llama-base-512--clmbr_train-tokens-total_nonPA...  lab_hyponatremia   \n",
       "2415  llama-base-512--clmbr_train-tokens-total_nonPA...  lab_hyponatremia   \n",
       "2418  llama-base-512--clmbr_train-tokens-total_nonPA...  lab_hyponatremia   \n",
       "...                                                 ...               ...   \n",
       "6897  mamba-tiny-1024--clmbr_train-tokens-total_nonP...        new_celiac   \n",
       "6900  mamba-tiny-1024--clmbr_train-tokens-total_nonP...        new_celiac   \n",
       "6903  mamba-tiny-1024--clmbr_train-tokens-total_nonP...        new_celiac   \n",
       "6906  mamba-tiny-1024--clmbr_train-tokens-total_nonP...        new_celiac   \n",
       "6909  mamba-tiny-1024--clmbr_train-tokens-total_nonP...        new_celiac   \n",
       "\n",
       "                  strat strat_col  quartile     t_test       p_value  \\\n",
       "2268  inter_event_times       std         0   2.175077  2.983300e-02   \n",
       "2301   timeline_lengths  n_events         3   2.715803  6.713266e-03   \n",
       "2412  inter_event_times       std         0 -10.210958  1.914469e-24   \n",
       "2415  inter_event_times       std         1  -7.703955  1.353396e-14   \n",
       "2418  inter_event_times       std         2  -7.391177  1.487361e-13   \n",
       "...                 ...       ...       ...        ...           ...   \n",
       "6897       n_gram_count      rr_1         3  -2.369788  1.796855e-02   \n",
       "6900   timeline_lengths  n_events         0  -3.865597  1.172455e-04   \n",
       "6903   timeline_lengths  n_events         1  -2.989941  2.851927e-03   \n",
       "6906   timeline_lengths  n_events         2  -3.657793  2.663531e-04   \n",
       "6909   timeline_lengths  n_events         3  -2.718275  6.664550e-03   \n",
       "\n",
       "           model1_name model1_base  model1_ctx_length      model2_name  \\\n",
       "2268  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "2301  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "2412   llama-base-4096       llama               4096   llama-base-512   \n",
       "2415   llama-base-4096       llama               4096   llama-base-512   \n",
       "2418   llama-base-4096       llama               4096   llama-base-512   \n",
       "...                ...         ...                ...              ...   \n",
       "6897  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "6900  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "6903  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "6906  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "6909  mamba-tiny-16384       mamba              16384  mamba-tiny-1024   \n",
       "\n",
       "     model2_base  model2_ctx_length  \n",
       "2268       mamba               1024  \n",
       "2301       mamba               1024  \n",
       "2412       llama                512  \n",
       "2415       llama                512  \n",
       "2418       llama                512  \n",
       "...          ...                ...  \n",
       "6897       mamba               1024  \n",
       "6900       mamba               1024  \n",
       "6903       mamba               1024  \n",
       "6906       mamba               1024  \n",
       "6909       mamba               1024  \n",
       "\n",
       "[105 rows x 14 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t_test_results_high_low[df_t_test_results_high_low['p_value'] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
