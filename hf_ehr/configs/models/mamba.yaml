# @package _global_

model:
  # Name of the model for clarity and reference
  name: mamba
  # Path to the model on Hugging Face's Model Hub
  hf_name: state-spaces/mamba-130m-hf
  # Model configuration parameters
  config_kwargs:
    d_model: 96
    hidden_size: 96
    intermediate_size: 96
    n_layer: 48
    num_hidden_layers: 48
    state_size: 16



data:
  # Data loader configuration
  dataloader:
    # Batch size for each training step
    #batch_size: 64
    # Shuffle data each epoch
    shuffle: true
    # Sequence length for time series data
    seq_length: 50
    # Data transformation or normalization parameters, if any
    transform:
      - normalize
      - to_tensor

trainer:
  # Training optimizer
  optimizer:
    # Type of optimizer, e.g., Adam, SGD
    type: Adam
    # Adam specific parameters
    betas: (0.9, 0.999)
    epsilon: 1e-08
    # Learning rate
    lr: 1e-3

  # Learning rate scheduler - https://huggingface.co/xiuyul/mamba-2.8b-zephyr
  scheduler:
    # Type of scheduler, e.g., StepLR, ExponentialLR
    type: linear
    warmup_ratio: 0.1

  # Training epochs
  epochs: 10
  # Device configuration (cuda or cpu)
  device: cuda
