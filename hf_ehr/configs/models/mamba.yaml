# @package _global_

model:
  # Name of the model for clarity and reference
  name: mamba
  # Path to the model on Hugging Face's Model Hub
  hf_name: state-spaces/mamba-130m-hf
  # Model configuration parameters
  config_kwargs:
    d_model: 128
    n_layer: 2
    num_hidden_layers: 2
    state_size: 16



data:
  # Data loader configuration
  dataloader:
    # Batch size for each training step
    batch_size: 2
    # Shuffle data each epoch
    shuffle: true
    # Sequence length for time series data
    seq_length: 1024
    # Data transformation or normalization parameters, if any
    transform:
      - normalize
      - to_tensor

trainer:
  # Training optimizer
  optimizer:
    # Type of optimizer, e.g., Adam, SGD
    type: AdamW
    # Adam specific parameters
    betas: (0.9, 0.95)
    # Learning rate
    lr: 2e-4

  # Training epochs
  epochs: 10
  # Device configuration (cuda or cpu)
  device: cuda
