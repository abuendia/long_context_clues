# @package _global_

data:
  dataloader:
    max_length: 1024
    batch_size: 2
  mlm_prob: 0.15

model:
  # Name of HF model to load
  name: bert
  # Name/path to pass to HF.from_pretrained()
  hf_name: bert-base-uncased
  # Kwargs for ModelConfig
  config_kwargs:
    num_hidden_layers: 12
    num_attention_heads: 12
    hidden_size: 768
    # Max context window
    max_position_embeddings: ${data.dataloader.max_length}

trainer:
  # For models that use MLM training, this is the percent of tokens to randomly mask
  mlm_mask_pct: 0.15
  optimizer:
    # Learning rate
    lr: 2e-4

  scheduler:
    # Number of local steps (i.e. non-global) from `initial_lr` -> `trainer.optimizer.lr`
    num_warmup_steps: 40_000
    # Number of local steps (i.e. non-global) from `trainer.optimizer.lr` -> `final_lr`
    num_decay_steps: 4_000_000
    #initial_lr: 1e-6
    #final_lr: 1e-5
