# @package _global_

data:
  dataloader:
    max_length: 1024
    batch_size: 2
    shuffle: true

model:
  # Name of the model for clarity and reference
  name: mamba
  # Path to the model on Hugging Face's Model Hub
  hf_name: state-spaces/mamba-130m-hf
  # Model configuration parameters
  config_kwargs:
    d_model: 128
    n_layer: 2
    num_hidden_layers: 2
    state_size: 16

trainer:
  # Training optimizer
  optimizer:
    # Type of optimizer, e.g., Adam, SGD
    type: AdamW
    # Adam specific parameters
    betas: (0.9, 0.95)
    # Learning rate
    lr: 2e-4

  # Training epochs
  epochs: 10
  # Device configuration (cuda or cpu)
  device: cuda
